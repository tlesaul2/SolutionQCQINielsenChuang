%!TeX root=../solnQCQI.tex

\chapter{Introduction to quantum mechanics}
\Textbf{2.1} Show that $(1, -1), (1,2)$, and $(2,1)$ are linearly dependent.
\Soln It is enough to express $(0, 0)$ as a linear combination of the specified vectors.
\begin{align*}
	\begin{bmatrix}
		1 \\
		-1
	\end{bmatrix}
	+
	\begin{bmatrix}
		1 \\
		2
	\end{bmatrix}
	-
	\begin{bmatrix}
		2 \\
		1
	\end{bmatrix}
	=
	\begin{bmatrix}
		0 \\
		0
	\end{bmatrix}
\end{align*}

\Textbf{2.2} Suppose $V$ is a vector space with basis vectors $\ket{0}$ and $\ket{1}$, and $A$ is a linear operator
from $V$ to $V$ such that $A\ket{0}=\ket{1}$ and $A\ket{1}=\ket{0}$.  Give a matrix representation for $A$, with
respect to the input bais $\ket{0},\ket{1}$, and the output basis $\ket{0},\ket{1}$.  Find input and output bases which
give rise to a different matrix representation of $A$.
\Soln With specified operations, it is enough to solve for the entries of a 2x2 matrix which
coverts the input vectors expressed as linear combinations of one basis, say $(\ket{a_1},\ket{a_2}),$
into vectors expressed as linear combinations of another basis, say $(\ket{b_1},\ket{b_2})$.
\[
A = \begin{blockarray}{ccc}
\begin{block}{ccc}
& \ket{b_1} & \ket{b_2}\\
    \end{block}
\begin{block}{ c[cc] }
\ket{a_1} & A_{11} & A_{12} \\
\ket{a_2} & A_{21} & A_{22} \\
            \end{block}
        \end{blockarray}
\]
With $(\ket{a_1},\ket{a_2}) = (\ket{0},\ket{1})$ and $(\ket{b_1},\ket{b_2}) = (\ket{0},\ket{1})$, we have
\begin{align*}
	A\ket{0} &:= \ket{1} = 0\ket{0}+1\ket{1}= A_{11}\ket{b_1} + A_{21}\ket{b_2} = A_{11}\ket{0} + A_{21}\ket{1}\Rightarrow A_{11} = 0,\ A_{21} = 1\\
	A\ket{1} &:= \ket{0} = 1\ket{0}+0\ket{1}=A_{12}\ket{b_1} + A_{22}\ket{b_2} = A_{12}\ket{0} + A_{22}\ket{1}\Rightarrow A_{12} = 1,\ A_{22} = 0\\
%
	\therefore A &=
	\begin{bmatrix}
		0 & 1 \\
		1 & 0
	\end{bmatrix}
\end{align*}
If the output basis was $(\ket{b_1},\ket{b_2}) = (\ket{1},\ket{0})$ instead, then $A=I$.  More formally:
\begin{align*}
    A\ket{0} &:= \ket{1} = 1\ket{1}+0\ket{0}= A_{11}\ket{b_1} + A_{21}\ket{b_2} = A_{11}\ket{1} + A_{21}\ket{0}\Rightarrow A_{11} = 1,\ A_{21} = 0\\
    A\ket{1} &:= \ket{0} = 0\ket{1}+1\ket{0}= A_{12}\ket{b_1} + A_{22}\ket{b_2} = A_{12}\ket{1} + A_{22}\ket{0}\Rightarrow A_{12} = 0,\ A_{22} = 1\\
        %
    \therefore A &=
    \begin{bmatrix}
    1 & 0 \\
    0 & 1
    \end{bmatrix}
\end{align*}
With a more interesting orthonormal output basis $(\ket{b_1},\ket{b_2})=(\ket{+},\ket{-})$:
\begin{align*}
        A\ket{0} &:= \ket{1} = \frac{\sqrt{2}}{2}\ket{+}-\frac{\sqrt{2}}{2}\ket{-}= A_{11}\ket{b_1} + A_{21}\ket{b_2} = A_{11}\ket{+} + A_{21}\ket{-}\Rightarrow A_{11} = \frac{\sqrt{2}}{2},\ A_{2} = -\frac{\sqrt{2}}{2}\\
        A\ket{1} &:= \ket{0} = \frac{\sqrt{2}}{2}\ket{+}+\frac{\sqrt{2}}{2}\ket{-}= A_{12}\ket{b_1} + A_{22}\ket{b_2} = A_{12}\ket{+} + A_{22}\ket{-}\Rightarrow A_{12} = \frac{\sqrt{2}}{2},\ A_{22} = \frac{\sqrt{2}}{2}\\
        %
    \therefore A &= \frac{\sqrt{2}}{2}
    \begin{bmatrix}
    1 & -1 \\
    1 & 1
    \end{bmatrix}
\end{align*}
Note: This is similar, but not equal to $\mathbf{H}$.  Had $A$ been the identity transformation when expressed with the
same input and output bases, then the result would have been exactly $\mathbf{H}$.

\Textbf{2.3} Suppose $A$ is a linear operator from vector space $V$  to vector space $W$, and $B$ is a linear operator
from vector space $W$ to vector space $X$.  Let $\ket{v_i}, \ket{w_j},$ and $\ket{x_k}$ be bases for the vector spaces
$V, W,$ and $X$, respectively.  Show that the matrix representation for the linear transformation $BA$ is the matrix
product of the matrix representations for $B$ and $A$ with respect to the appropriate bases.
\Soln Fix $i$.  We'll show that $(B\circ A)_{ki} = (B \cdot A)_{ki}$.
\begin{align*}
(B\circ A) \ket{v_i} = \sum_k (B\circ A)_{ki} \ket{x_k} &= B \left( \sum_{j} A_{ji}\ket{w_j} \right) \tag{Eqn 2.12, composition}\\
	&= \sum_{j} A_{ji} B\ket{w_j}\tag{linearity}\\
	&= \sum_{j,k} A_{ji} B_{kj}\ket{x_k}\tag{Eqn 2.12}\\
	&= \sum_k \left( \sum_j B_{kj} A_{ji}  \right) \ket{x_k}\tag{finiteness, commutativity}\\
    &= \sum_k \left( (B\cdot A)_{ki}  \right) \ket{x_k}\tag{definition}\\\\
	\therefore & (B\circ A)_{ki} = (B\cdot A)_{ki}
\end{align*}



\Textbf{2.4} Show that the identity operator on a vector space $V$ has a matrix representation which is one along the
diagonal and zero everywhere else, if the matrix representation is taken with respect to the same input and output bases.
This matrix is known as the \textit{identity matrix}
\Soln Let $I$ be the matrix in question.
\begin{align*}
I\ket{v_j} := \ket{v_j} &= \sum_i I_{ij} \ket{v_i}, \ \forall j.\\
	&\Rightarrow I_{ij} = \delta_{ij} := \left\{ \begin{array}{cc} 1 & i=j \\ \\ 0 & o/w\end{array}\right.
\end{align*}


\Textbf{2.5} Verify that $(\cdot, \cdot)$ just defined is an inner product on $\mathbb{C}^n$
\Soln Defined inner product on $\mathbb{C}^n$ is
\begin{align*}
	\left(
		(y_1, \cdots, y_n), (z_1, \cdots, z_n)
	\right)
	= \sum_{i} y_i^* z_i .
\end{align*}
Equation (2.13.1), linearity in second argument:
\begin{align*}
	\left((y_1,\cdots,y_n),\sum_i\lambda_i(z_{i1},\cdots,z_{in})\right)
	&= \left((y_1,\cdots,y_n),\left(\sum_i\lambda_iz_{i1},\cdots,\sum_i\lambda_iz_{in}\right)\right) \tag{definition} \\
    &= \sum_jy^*_j\left(\sum_i\lambda_iz_{ij}\right) \tag{definition}\\
    &= \sum_j\left(\sum_iy^*_j\lambda_iz_{ij}\right) \tag{linearity of multiplcation}\\
    &= \sum_j\left(\sum_i\lambda_iy^*_jz_{ij}\right) \tag{associativity/commutativity}\\
    &= \sum_i\left(\sum_j\lambda_iy^*_jz_{ij}\right) \tag{finiteness} \\
    &= \sum_i\lambda_i\left(\sum_jy^*_jz_{ij}\right) \tag{linearity} \\
    &= \sum_i\lambda_i\left((y_1,\cdots,y_n),(z_{i1},\cdots,z_{in})\right) \tag{definition}
\end{align*}
%
%
%    &= \sum_i\lambda_i\left(
%(y_1, \cdots, y_n),(z_{i1}, \cdots, z_{in})
%\right) \tag{definition}
%

% =\sum_i y_i^* \left(
% 										\sum_j \lambda_j z_{ji}
% 			      				   \right)\\
% 	&= \sum_{i,j} y_i^* \lambda_j z_{ji}\\
% 	&= \sum_j \lambda_j \left(\sum_i y_i^* z_{ji}  \right)\\
% 	&= \sum_j \lambda_j \left(
% 													(y_1, \cdots, y_n),  (z_{j1}, \cdots, z_{jn})
% 											  \right)\\
% 	&= \sum_i \lambda_i \left(
% 													(y_1, \cdots, y_n),  (z_{i1}, \cdots, z_{in})
% 												\right).

Equation (2.13.2), conjugate symmetry:
\begin{align}
	\bigl((y_1, \cdots, y_n), (z_1, \cdots, z_n)\bigr)^*
	&= \left(\sum_i y_i^* z_i \right)^* \tag{definition}\\
	&= \left(\sum_i y_i  z_i^* \right) \tag{conjugate symmetry in $\mathbb{C}^1$}\\
	&= \left(\sum_i z_i^* y_i \right) \tag{commutativity in $\mathbb{C}^1$}\\
	&= \bigl((z_1, \cdots, z_n) , (y_1, \cdots, y_n)\bigr) \tag{definition}
\end{align}.


Equation (2.13.3), positive definiteness:
\begin{align*}
	\bigl((y_1, \cdots, y_n), (y_1, \cdots, y_n)\bigr)
	&= \sum_i y_i^* y_i \tag{definition}\\
	&= \sum_i |y_i|^2 \tag{definition} \\
	&\geq 0 \tag{positive definiteness of $|\cdot|^2$ over $\mathbb{C}^1$}
\end{align*}

Now:
\begin{align*}
	\bigl((y_1, \cdots, y_n), (y_1, \cdots, y_n)\bigr) = \sum_i |y_i|^2 &\stackrel{?}{=} 0 \tag{hypothesis}\\ \iff |y_i|^2 &= 0\  \forall i \tag{positivity of $|\cdot|^2$}\\ \iff \ y_i\ \  &= 0\  \forall i \tag{positive definiteness of $|\cdot|^2$ over $\mathbb{C}^1$}\\ \iff (y_1, \cdots, y_n) &=  \textbf{0} \tag{definition}
\end{align*}

%Since $|y_i|^2 \geq 0$ for all $i$. Thus
%$\sum_i |y_i|^2 =
%\left(
%	(y_1, \cdots, y_n), (y_1, \cdots, y_n)
%\right) \geq 0
%$.
%
%From now on,  I will show the following statement,
%\begin{align*}
%	\left(
%		(y_1, \cdots, y_n), (y_1, \cdots, y_n)
%	\right) = 0
%	\text{ iff }  (y_1, \cdots, y_n) = 0.
%\end{align*}
%($\Leftarrow$) This is obvious.\\
%($\Rightarrow$)
%Suppose $\left( (y_1, \cdots, y_n), (y_1, \cdots, y_n) \right) = 0$. Then $\sum_i |y_i|^2 = 0$.
%
%Since $|y_i|^2 \geq 0$ for all $i$, if $\sum_i |y_i|^2 = 0$, then $|y_i|^2 = 0$ for all $i$.
%Therefore $|y_i|^2 = 0 \Leftrightarrow y_i = 0$  for all $i$.
%Thus,
%\begin{align*}
%	(y_1, \cdots, y_n) = 0.
%\end{align*}

\Textbf{2.6}
\begin{align*}
	\left(\sum_i \lambda_i \ket{w_i},\ \ket{v}\right) &=
	\left(\ket{v},\ \sum_i \lambda_i \ket{w_i}\right)^*\\
	&= \left[\sum_i \lambda_i \left(\ket{v},\ \ket{w_i}  \right) \right]^* (\because \text{linearlity in the 2nd arg.})\\
	&= \sum_i \lambda_i^* \left(\ket{v},\ \ket{w_i} \right)^*\\
	&= \sum_i \lambda_i^* (\ket{w_i},\ \ket{v})
\end{align*}


\Textbf{2.7}

\begin{align*}
	\braket{w | v} &= \begin{bmatrix}
		1 & 1
	\end{bmatrix}
	\begin{bmatrix}
	1 \\
	-1
	\end{bmatrix}
	= 1 - 1 = 0\\
%
	\frac{\ket{w}}{\norm{\ket{w}}} &=
	\frac{\ket{w}}{\sqrt{\braket{w|w}}} = \frac{1}{\sqrt{2}} \begin{bmatrix}
	1 \\
	1
	\end{bmatrix}\\
%
	\frac{\ket{v}}{\norm{\ket{v}}} &=
	\frac{\ket{v}}{\sqrt{\braket{v|v}}} = \frac{1}{\sqrt{2}} \begin{bmatrix}
	1 \\
	-1
	\end{bmatrix}
\end{align*}



\Textbf{2.8}

If $k = 1$,
\begin{align*}
	\ket{v_2} &= \frac{\ket{w_2} - \braket{v_1 | w_2}\ket{v_1}}{\norm{\ket{w_2} - \braket{v_1 | w_2}\ket{v_1}}}\\
	\braket{v_1 | v_2} &= \bra{v_1} \left(\frac{\ket{w_2} - \braket{v_1 | w_2}\ket{v_1}}{\norm{\ket{w_2} - \braket{v_1 | w_2}\ket{v_1}}}\right)\\
		&= \frac{\braket{v_1 | w_2} - \braket{v_1 | w_2}\braket{v_1 | v_1}}{\norm{\ket{w_2} - \braket{v_1 | w_2}\ket{v_1}}}\\
		&= 0.
\end{align*}

Suppose $\left\{v_1, \cdots v_n \right\}$ $(n \leq d-1)$ is a orthonormal basis. Then
\begin{align*}
	\braket{v_j | v_{n+1}} &= \bra{v_j} \left(\frac{\ket{w_{n+1}} - \sum_{i=1}^{n}\braket{v_i | w_{n+1}}\ket{v_i}}{\norm{\ket{w_{n+1}} - \sum_{i=1}^{n}\braket{v_i | w_{n+1}}\ket{v_i}}}\right)~~(j \leq n)\\
	&= \frac{\braket{v_j | w_{n+1}} - \sum_{i=1}^{n}\braket{v_i | w_{n+1}}\braket{v_j | v_i}}{\norm{\ket{w_{n+1}} - \sum_{i=1}^{n}\braket{v_i | w_{n+1}}\ket{v_i}}}\\
	&= \frac{\braket{v_j | w_{n+1}} - \sum_{i=1}^{n}\braket{v_i | w_{n+1}}\delta_{ij}}{\norm{\ket{w_{n+1}} - \sum_{i=1}^{n}\braket{v_i | w_{n+1}}\ket{v_i}}}\\
	&= \frac{\braket{v_j | w_{n+1}} - \braket{v_j | w_{n+1}}}{\norm{\ket{w_{n+1}} - \sum_{i=1}^{n}\braket{v_i | w_{n+1}}\ket{v_i}}}\\
	&= 0.
\end{align*}
Thus Gram-Schmidt procedure produces an orthonormal basis.


\Textbf{2.9}
\begin{align*}
	\sigma_0 &= I = \ket{0}\bra{0} + \ket{1}\bra{1}\\
	\sigma_1 &= X = \ket{0}\bra{1} + \ket{1}\bra{0}\\
	\sigma_2 &= Y = -i\ket{0}\bra{1} + i\ket{1}\bra{0}\\
	\sigma_3 &= Z = \ket{0}\bra{0} - \ket{1}\bra{1}
\end{align*}


\Textbf{2.10}
\begin{align*}
	\ket{v_j}\bra{v_k} &= I_V \ket{v_j} \bra{v_k} I_V\\
	&= \left(\sum_p \ket{v_p}\bra{v_p} \right) \ket{v_j}\bra{v_k} \left(\sum_q \ket{v_q}\bra{v_q} \right)\\
	&= \sum_{p,q} \ket{v_p} \braket{v_p|v_j}
	\braket{v_k | v_q} \bra{v_q}\\
	&= \sum_{p,q} \delta_{pj} \delta_{kq} \ket{v_p} \bra{v_q}
\end{align*}
Thus
\begin{align*}
	\left( \ket{v_j}\bra{v_k} \right)_{pq} = \delta_{pj} \delta_{kq}
\end{align*}



\Textbf{2.11}
\begin{align*}
	X = \begin{bmatrix}
	0 & 1 \\
	1 & 0
	\end{bmatrix},\ \det(X-\lambda I) =
	\det \left(\begin{bmatrix}
	-\lambda & 1 \\
	1 & -\lambda
	\end{bmatrix} \right) = 0 \Rightarrow \lambda = \pm 1
\end{align*}

If $\lambda = -1$,
\begin{align*}
	\begin{bmatrix}
		1 & 1 \\
		1 & 1
	\end{bmatrix}
	\begin{bmatrix}
		c_1 \\
		c_2
	\end{bmatrix} =
	\begin{bmatrix}
		0 \\
		0
	\end{bmatrix}
\end{align*}
Thus
\begin{align*}
	\ket{\lambda = -1} = \begin{bmatrix}
	c_1 \\
	c_2
	\end{bmatrix} = \frac{1}{\sqrt{2}}
	\begin{bmatrix}
	-1 \\
	1
	\end{bmatrix}
\end{align*}

If $\lambda = 1$
\begin{align*}
	\ket{\lambda = 1} = \frac{1}{\sqrt{2}}
	\begin{bmatrix}
	1 \\
	1
	\end{bmatrix}
\end{align*}

\begin{align*}
	X = \begin{bmatrix}
	-1 & 0 \\
	0 & 1
	\end{bmatrix}
	\text{ w.r.t. } \left\{ \ket{\lambda = -1},\ \ket{\lambda = 1}\right\}
\end{align*}



\Textbf{2.12}
\begin{align*}
	\det \left(\begin{bmatrix}
	1 & 0 \\
	1 & 1
	\end{bmatrix} - \lambda I \right) = (1 - \lambda)^2 = 0 \Rightarrow \lambda = 1
\end{align*}
Therefore the eigenvector associated with eigenvalue $\lambda = 1$ is
\begin{align*}
	\ket{\lambda = 1} = \begin{bmatrix}
	0 \\
	1
	\end{bmatrix}
\end{align*}

Because $\ket{\lambda = 1}\bra{\lambda = 1} = \begin{bmatrix}
0 & 0 \\
0 & 1
\end{bmatrix}$,
\begin{align*}
	\begin{bmatrix}
	1 & 0 \\
	1 & 1
	\end{bmatrix} \neq c\ket{\lambda = 1}\bra{\lambda = 1} = \begin{bmatrix}
	0 & 0 \\
	0 & c
	\end{bmatrix}
\end{align*}



\Textbf{2.13}

Suppose $\ket{\psi},\ \ket{\phi}$ are arbitrary vectors in $V$.
\begin{align*}
	\left(\ket{\psi},\ (\ket{w}\bra{v}) \ket{\phi}\right)^* &=
	\left((\ket{w}\bra{v})^\dagger \ket{\psi},\  \ket{\phi}\right)^*\\
	&= \left(\ket{\phi},\ (\ket{w}\bra{v})^\dagger \ket{\psi} \right)\\
	&= \bra{\phi} (\ket{w}\bra{v})^\dagger \ket{\psi}.
\end{align*}

On the other hand,
\begin{align*}
	\left(\ket{\psi},\ (\ket{w}\bra{v}) \ket{\phi}\right)^*
	&= (\braket{\psi | w} \braket{v | \phi})^*\\
	&= \braket{\phi | v} \braket{w | \psi}.
\end{align*}

Thus
\begin{align*}
	\bra{\phi} (\ket{w}\bra{v})^\dagger \ket{\psi} = \braket{\phi | v} \braket{w | \psi} \text{ for arbitrary vectors } \ket{\psi},\ \ket{\phi}\\
	\therefore (\ket{w}\bra{v})^\dagger = \ket{v}\bra{w}
\end{align*}


\Textbf{2.14}
\begin{align*}
	( (a_i A_i)^\dagger \ket{\phi},\ \ket{\psi} )
	&= (\ket{\phi},\ a_i A_i \ket{\psi})\\
	&= a_i (\ket{\phi},\ A_i \ket{\psi})\\
	&= a_i (A_i^\dagger \ket{\phi},\ \ket{\psi})\\
	&= (a_i^* A_i^\dagger \ket{\phi},\ \ket{\psi})\\
%
	\therefore (a_i A_i)^\dagger = a_i^* A_i^\dagger
\end{align*}




\Textbf{2.15}
\begin{align*}
	((A^\dagger)^\dagger\ket{\psi},\ \ket{\phi} )
	&= (\ket{\psi},\ A^\dagger \ket{\phi})\\
	&= (A^\dagger \ket{\phi},\ \ket{\psi})^*\\
	&= (\ket{\phi},\ A\ket{\psi})^*\\
	&= (A\ket{\psi},\ \ket{\phi})\\
	\therefore (A^\dagger)^\dagger = A
\end{align*}


\Textbf{2.16}
\begin{align*}
	P &= \sum_i \ket{i}\bra{i}.\\
	P^2 &= \left(\sum_i \ket{i}\bra{i}\right) \left(\sum_j \ket{j}\bra{j}\right)\\
	&= \sum_{i,j} \ket{i}\braket{i | j}\bra{j}\\
	&= \sum_{i,j} \ket{i}\bra{j} \delta_{ij}\\
	&= \sum_i \ket{i}\bra{i}\\
	&= P
\end{align*}


\Textbf{2.17}
\begin{proof}
	($\Rightarrow$) Suppose $A$ is Hermitian. Then $A=A^\dagger$.
	Let $\ket{\lambda}$ be eigenvectors of $A$ with eigenvalues $\lambda$, that is,
	\begin{align*}
		A \ket{\label{text}} = \lambda \ket{\lambda}.
	\end{align*}
	Therefore
	\begin{align*}
		\braket{\lambda | A | \lambda} = \lambda \braket{\lambda | \lambda} = \lambda.
	\end{align*}
	On the other hand,
	\begin{align*}
		\lambda^*  = \braket{\lambda | A | \lambda}^*
								= \braket{\lambda | A^\dagger |  \lambda}
								= \braket{\lambda | A | \lambda}
								= \lambda \braket{\lambda | \lambda} = \lambda.
	\end{align*}
	Hence eigenvalues of Hermitian matrix are real.

	($\Leftarrow$) Suppose eigenvalues of $A$ are real. From spectral theorem, normal matrix $A$ can be written by
	\begin{align}
		A= \sum_i \lambda_i \kb{\lambda_i}
	\end{align}
	where $\lambda_i$ are  real eigenvalues with eigenvectors $\ket{\lambda_i}$.
	By taking adjoint, we get
	\begin{align*}
		A^\dagger &= \sum_i \lambda_i^* \kb{\lambda_i}\\
								&= \sum_i \lambda_i \kb{\lambda_i}~~~(\because \lambda_i \text{ are real})\\
								&= A
	\end{align*}
	Thus $A$ is Hermitian.
\end{proof}

\Textbf{2.18}

Suppose $\ket{v}$ is a eigenvector with corresponding eigenvalue $\lambda$.
\begin{align*}
	U \ket{v} &= \lambda \ket{v}.\\
	1 &= \braket{v | v}\\
	&= \bra{v} I \ket{v}\\
	&= \bra{v} U^\dagger U \ket{v}\\
	&= \lambda \lambda^* \braket{v | v}\\
	&= \norm{\lambda}^2\\
	\therefore \lambda &= e^{i \theta}
\end{align*}




\Textbf{2.19}
\begin{align*}
	X^2 = \begin{bmatrix}
		0 & 1 \\
		1 & 0
	\end{bmatrix}
	\begin{bmatrix}
		0 & 1 \\
		1 & 0
	\end{bmatrix}
	= \begin{bmatrix}
		1 & 0 \\
		0 & 1
	\end{bmatrix} = I
\end{align*}



\Textbf{2.20}
\begin{align*}
	U &\equiv \sum_i \ket{w_i}\bra{v_i}\\
	A_{ij}^{'} &= \braket{v_i | A | v_j}\\
	&= \braket{v_i | UU^\dagger A UU^\dagger | v_j}\\
	&= \sum_{p,q,r,s} \braket{v_i | w_p} \braket{v_p | v_q} \braket{w_q | A | w_r} \braket{v_r | v_s} \braket{w_s | v_j}\\
	&= \sum_{p,q,r,s} \braket{v_i | w_p} \delta_{pq} A_{qr}^{''} \delta_{rs}  \braket{w_s | v_j}\\
	&= \sum_{p,r}  \braket{v_i | w_p}  \braket{w_r | v_j} A_{pr}^{''}
\end{align*}


\Textbf{2.21}

Suppose $M$ be Hermitian. Then $M = M^\dagger$.
\begin{align*}
	M &= IMI\\
		&= (P+Q) M (P+Q)\\
		&= PMP + QMP + PMQ + QMQ\\
\end{align*}
Now $PMP = \lambda P$, $QMP = 0$, $PMQ = PM^\dagger Q = (QMP)^* = 0$.
Thus $M = PMP + QMQ$.
Next prove $QMQ$ is normal.
\begin{align*}
	QMQ (QMQ)^\dagger &= QMQ QM^\dagger Q\\
		&= QM^\dagger Q QMQ ~~~ (M = M^\dagger)\\
		&= (QM^\dagger Q) QMQ
\end{align*}
Therefore $QMQ$ is normal.
By induction, $QMQ$ is diagonal ... (following is same as Box 2.2)

\Textbf{2.22}

Suppose $A$ is a Hermitian operator and $\ket{v_i}$ are eigenvectors of $A$ with eigenvalues $\lambda_i$.
Then
\begin{align*}
	\braket{v_i | A | v_j} = \lambda_j \braket{v_i | v_j}.
\end{align*}

On the other hand,
\begin{align*}
	\braket{v_i | A | v_j} = \braket{v_i | A^\dagger | v_j}
	= \braket{v_j | A | v_i}^*
	= \lambda_i^* \braket{v_j | v_i}^*
	=  \lambda_i^* \braket{v_i | v_j}
	=  \lambda_i \braket{v_i | v_j}
\end{align*}

Thus
\begin{align*}
	(\lambda_i - \lambda_j) \braket{v_i | v_j}  = 0.
\end{align*}
If $\lambda_i \neq \lambda_j$, then $\braket{v_i | v_j}  = 0$.


\Textbf{2.23}

Suppose $P$ is projector and $\ket{\lambda}$  are eigenvectors of $P$ with eigenvalues $\lambda$.
Then $P^2 = P$.
\begin{align*}
	P \ket{\lambda} = \lambda \ket{\lambda} \text{ and }	P \ket{\lambda} = P^2 \ket{\lambda} = \lambda  P \ket{\lambda} = \lambda^2 \ket{\lambda}.
\end{align*}

Therefore
\begin{align*}
	\lambda = \lambda^2\\
	\lambda (\lambda - 1) = 0\\
	\lambda = 0 \text{ or } 1.
\end{align*}


\Textbf{2.24}

Def of positive $\braket{v | A | v} \geq 0$ for all $\ket{v}$.

Suppose $A$ is a positive operator. $A$ can be decomposed as follows.
\begin{align*}
	A &= \frac{A + A^\dagger}{2} + i \frac{A - A^\dagger}{2i}\\
		&= B + i C  ~~~\text{where } B =\frac{A + A^\dagger}{2}, ~~  C = \frac{A - A^\dagger}{2i}.
\end{align*}
Now operators $B$ and $C$ are Hermitian.

\begin{align*}
	\braket{v | A | v}  &= \braket{v | B + iC | v}\\
		&= \braket{v | B | v}  + i \braket{v | C | v}\\
		&= \alpha + i \beta ~~ \text{where } \alpha = \braket{v | B | v}, ~\beta = \braket{v | C | v}.
\end{align*}

Since $B$ and $C$ are Hermitian, $\alpha,~ \beta \in \mathds{R}$.
From def of positive operator, $\beta$ should be vanished because $\braket{v | A | v}$ is real.
Hence $\beta = \braket{v | C | v} =  0$ for all $\ket{v}$, i.e. $C = 0$.

Therefore $A = A^\dagger$.

\begin{screen}
	Reference: MIT 8.05 Lecture note  by Prof. Barton Zwiebach.\\
	\url{https://ocw.mit.edu/courses/physics/8-05-quantum-physics-ii-fall-2013/lecture-notes/MIT8_05F13_Chap_03.pdf}

	\begin{prop} \label{prop: zeroop}
		Let $T$ be a linear operator in a complex vector space $V$.

		If $(u, Tv) = 0$ for all $u, v \in V$, then $T = 0$.
	\end{prop}

	\begin{proof}
		Suppose $u = Tv$. Then $(Tv, Tv) = 0$ for all $v$ implies that $Tv = 0$ for all $v$. Therefore $T = 0$.
	\end{proof}

	\begin{thm}
		If $(v, Av) = 0$ for all $v \in V$, then $A = 0$. \label{thm:zerooperator}
	\end{thm}

	\begin{proof}
		First, we show that $(u, Tv) = 0$ if $(v, Av) = 0$. Then apply proposition \ref{prop: zeroop}\\
		Suppose $u, v \in V$. Then $(u, Tv)$ is decomposed as
		\begin{align*}
			(u, Tv) = \frac{1}{4} \left[ (u+v, T(u+v)) - (u-v, T(u-v)) + \frac{1}{i} (u+iv, T(u+iv)) \right.\\
			\left. \hspace{4cm} - \frac{1}{i} (u-iv, T(u-iv))  \right].
		\end{align*}
		If $(v, Tv) = 0$ for all $v \in V$, the right hand side of above eqn vanishes. Thus $(u, Tv) = 0$ for all $u, v \in V$.
		Then $T = 0$.
	\end{proof}


\end{screen}


\Textbf{2.25}
\begin{align*}
	\braket{\psi | A^\dagger A | \psi} = \norm{A \ket{\psi}}^2 \geq 0 \text{ for all } \ket{\psi}.
\end{align*}
Thus $A^\dagger A$ is positive.


\Textbf{2.26}
\begin{align*}
	\ket{\psi}^{\otimes 2} &= \frac{1}{\sqrt{2}} (\ket{0} + \ket{1}) \otimes \frac{1}{\sqrt{2}} (\ket{0} + \ket{1}\\
		&= \frac{1}{2} (\ket{00}  + \ket{01} + \ket{10} + \ket{11}  )\\
		&= \frac{1}{2} \begin{bmatrix}
			1 \\
			1 \\
			1 \\
			1
		\end{bmatrix}
\end{align*}

\begin{align*}
	\ket{\psi}^{\otimes 3} &= \frac{1}{\sqrt{2}} (\ket{0} + \ket{1}) \otimes \frac{1}{\sqrt{2}} (\ket{0} + \ket{1}  \otimes \frac{1}{\sqrt{2}} (\ket{0} + \ket{1}\\
		&= \frac{1}{2\sqrt{2}} (\ket{000}  + \ket{001} + \ket{010} + \ket{011} +  \ket{100}  + \ket{101} + \ket{110} + \ket{111})\\
		&= \frac{1}{2\sqrt{2}} \begin{bmatrix}
			1 \\
			1 \\
			1 \\
			1 \\
			1 \\
			1 \\
			1 \\
			1
		\end{bmatrix}
\end{align*}


\Textbf{2.27}
\begin{align*}
	X \otimes Z &= \begin{bmatrix}
		0 & 1 \\
		1 & 0
	\end{bmatrix}
	\otimes
	\begin{bmatrix}
		1 & 0 \\
		0 & -1
	\end{bmatrix} \\
	&= \begin{bmatrix}
		0 & 0 & 1 & 0 \\
		0 & 0 & 0 & -1 \\
		1 & 0 & 0 & 0 \\
		0 & -1 & 0 & 0
	\end{bmatrix}
\end{align*}

\begin{align*}
	I \otimes X &= \begin{bmatrix}
		1 & 0 \\
		0 & 1
	\end{bmatrix}
	\otimes
	\begin{bmatrix}
		0 & 1 \\
		1 & 0
	\end{bmatrix}\\
	&=
	\begin{bmatrix}
	0 & 1 & 0 & 0 \\
	1 & 0 & 0 & 0 \\
	0 & 0 & 0 & 1 \\
	0 & 0 & 1 & 0
	\end{bmatrix}
\end{align*}

\begin{align*}
	X \otimes I &= \begin{bmatrix}
		0 & 1 \\
		1 & 0
	\end{bmatrix}
	\otimes
	\begin{bmatrix}
		1 & 0 \\
		0 & 1
	\end{bmatrix}\\
	&= \begin{bmatrix}
	0 & 0 & 1 & 0 \\
	0 & 0 & 0 & 1 \\
	1 & 0 & 0 & 0 \\
	0 & 1 & 0 & 0
	\end{bmatrix}
\end{align*}

In general, tensor product is not commutable.



\Textbf{2.28}
\begin{align*}
	(A \otimes B)^*
	&=
	\begin{bmatrix}
		A_{11} B & \cdots & A_{1n} B \\
		\vdots & \ddots  & \vdots \\
		A_{m1}B & \cdots & A_{mn} B
	\end{bmatrix}^* \\
	&=
	\begin{bmatrix}
		A_{11}^* B^* & \cdots & A_{1n}^* B^* \\
		\vdots & \ddots  & \vdots \\
		A_{m1}^* B^* & \cdots & A_{mn}^* B^*
	\end{bmatrix} \\
	&= A^* \otimes B^*.
\end{align*}


\begin{align*}
	(A\otimes B)^T &=
	\begin{bmatrix}
		A_{11} B & \cdots & A_{1n} B \\
		\vdots & \ddots  & \vdots \\
		A_{m1}B & \cdots & A_{mn} B
	\end{bmatrix}^T \\
	&=
	\begin{bmatrix}
		A_{11} B^T & \cdots & A_{m1} B^T \\
		\vdots & \ddots  & \vdots \\
		A_{1n} B^T & \cdots & A_{mn} B^T
	\end{bmatrix} \\
	&=
	\begin{bmatrix}
		A_{11} B^T & \cdots & A_{1m} B^T \\
		\vdots & \ddots  & \vdots \\
		A_{n1} B^T & \cdots & A_{nm} B^T
	\end{bmatrix} \\
	&= A^T \otimes B^T.
\end{align*}


\begin{align*}
	(A\otimes B)^\dagger&=((A \otimes B)^*)^T	\\
		&= (A^* \otimes B^*)^T\\
		&= (A^*)^T \otimes (B^*)^T\\
		&= A^\dagger \otimes B^\dagger.
\end{align*}

\Textbf{2.29}
Suppose $U_1$ and $U_2$ are unitary operators. Then
\begin{align*}
	(U_1 \otimes U_2) (U_1 \otimes U_2)^\dagger &=U_1 U_1^\dagger \otimes U_2 U_2^\dagger\\
		&= I \otimes I.
\end{align*}

Similarly,
\begin{align*}
	(U_1 \otimes U_2)^\dagger (U_1 \otimes U_2)  = I \otimes I.
\end{align*}

\Textbf{2.30}
Suppose $A$ and $B$ are Hermitian operators. Then
\begin{align}
(A \otimes B)^\dagger = A^\dagger \otimes B^\dagger = A \otimes B.
\end{align}
Thus $A \otimes B$ is Hermitian.



\Textbf{2.31}

Suppose $A$ and $B$ are positive operators. Then
\begin{align*}
	\bra{\psi} \otimes \bra{\phi} (A \otimes B) \ket{\psi} \otimes \ket{\phi} &= \braket{\psi |A| \psi} \braket{\phi | B | \phi}.
\end{align*}

Since $A$ and $B$ are positive operators,
$\braket{\psi |A| \psi} \geq 0$ and $\braket{\phi | B | \phi} \geq 0$ for all $\ket{\psi}, \ket{\phi} $.
Then $\braket{\psi |A| \psi} \braket{\phi | B | \phi} \geq 0$.
Thus $A \otimes B$ is positive if $A$ and $B$ are positive.


\Textbf{2.32}

Suppose $P_1$ and $P_2$  are projectors. Then
\begin{align*}
	(P_1 \otimes P_2) ^2 &= P_1^2 \otimes P_2^2\\
		&= P_1 \otimes P_2.
\end{align*}
Thus $ P_1 \otimes P_2.$ is also projector.


\Textbf{2.33}
\begin{align}
	H  = \frac{1}{\sqrt{2}} \begin{bmatrix}
		1 & 1 \\
		1 & -1
	\end{bmatrix}
\end{align}

\begin{align*}
	H^{\otimes 2}
	=
	\frac{1}{\sqrt{2}} \begin{bmatrix}
	1 & 1 \\
	1 & -1
	\end{bmatrix}
	\otimes
	\frac{1}{\sqrt{2}} \begin{bmatrix}
	1 & 1 \\
	1 & -1
	\end{bmatrix}
	=
	\frac{1}{2} \begin{bmatrix}
		1 & 1 & 1 & 1 \\
		1 & -1 & 1 & -1 \\
		1 & 1 & -1 & -1 \\
		1 & -1 & -1 & 1
	\end{bmatrix}
\end{align*}



\Textbf{2.34}

Suppose $A = \begin{bmatrix}
4 & 3 \\
3 & 4
\end{bmatrix} $.

\begin{align*}
	\det (A - \lambda I ) &= (4-\lambda)^2 - 3^2\\
		&= \lambda^2 -8\lambda + 7\\
		&= (\lambda - 1)(\lambda - 7)
\end{align*}

Eigenvalues of $A$ are $\lambda = 1, ~ 7$.
Corresponding eigenvectors are
$
	\ket{\lambda = 1} = \frac{1}{\sqrt{2}} \begin{bmatrix}
	1 \\
	-1
	\end{bmatrix}
$,
$
	\ket{\lambda = 7} = \frac{1}{\sqrt{2}} \begin{bmatrix}
1 \\
1
\end{bmatrix}
$.

\vspace{5mm}
Thus
\begin{align*}
	A = \kb{\lambda = 1} + 7 \kb{\lambda = 7}.
\end{align*}

\begin{align*}
	\sqrt{A} &= \kb{\lambda = 1} + \sqrt{7} \kb{\lambda = 7}\\
		&= \frac{1}{2} \begin{bmatrix}
		1 & -1 \\
		-1 & 1
		\end{bmatrix}
		+
		\frac{\sqrt{7}}{2} \begin{bmatrix}
		1 & 1 \\
		1 & 1
		\end{bmatrix}\\
		&=
		\frac{1}{2}
		 \begin{bmatrix}
			1+\sqrt{7} & -1+\sqrt{7} \\
			-1 + \sqrt{7} & 1+\sqrt{7}
		\end{bmatrix}
\end{align*}

 \begin{align*}
 	\log (A) &=  \log (1) \kb{\lambda = 1} + \log (7) \kb{\lambda = 7}\\
 		&= \frac{\log (7)}{2} \begin{bmatrix}
	 		1 & 1 \\
	 		1 & 1
 		\end{bmatrix}
 \end{align*}



\Textbf{2.35}
\begin{align*}
	\vec{v} \cdot \vec{\sigma} &= \sum_{i=1}^3 v_i \sigma_i\\
		&= v_1 \begin{bmatrix}
		0 & 1 \\
		1 & 0
		\end{bmatrix}
		+ v_2 \begin{bmatrix}
		0 & -i \\
		i & 0
		\end{bmatrix}
		+ v_3 \begin{bmatrix}
		1 & 0 \\
		0 & -1
		\end{bmatrix} \\
		&= \begin{bmatrix}
		v_3 & v_1 - i v_2 \\
		v_1 + iv_2 & -v_3
		\end{bmatrix}
\end{align*}

\begin{align*}
	\det (\vec{v} \cdot \vec{\sigma}  - \lambda I) &= (v_3 - \lambda) (-v_3 - \lambda) - (v_1 - iv_2) (v_1 + iv_2)\\
			&= \lambda^2 - (v_1^2 + v_2^2  + v_3^2)\\
			&= \lambda^2 - 1 ~~~ (\because |\vec{v}| = 1)
\end{align*}
Eigenvalues are $\lambda = \pm 1$.
Let $\ket{\lambda_{ \pm 1 } }$ be eigenvectors with eigenvalues $\pm  1$.

Since $\vec{v} \cdot \vec{\sigma}$ is Hermitian,  $\vec{v} \cdot \vec{\sigma}$ is diagonalizable.
Then
\begin{align*}
	\vec{v} \cdot \vec{\sigma} = \kb{\lambda_1} - \kb{\lambda_{-1}}
\end{align*}

Thus
\begin{align*}
	\exp \left(i \theta \vec{v} \cdot \vec{\sigma} \right) &=
	e^{i \theta} \kb{\lambda_1}  + e^{-i \theta} \kb{\lambda_{-1}}\\
	&= (\cos \theta + i \sin \theta) \kb{\lambda_1} + (\cos \theta - i \sin \theta) \kb{\lambda_{-1}}\\
	&= \cos \theta (\kb{\lambda_1} + \kb{\lambda_{-1}}) + i \sin \theta (\kb{\lambda_1} - \kb{\lambda_{-1}}) \\
	&= \cos( \theta) I + i \sin (\theta) \vec{v} \cdot \vec{\sigma}.
\end{align*}

$\because$ Since $\vec{v} \cdot \vec{\sigma}$ is Hermitian, $\ket{\lambda_1}$ and $\ket{\lambda_{-1}}$ are orthogonal.
Thus
\begin{align*}
	\kb{\lambda_1} + \kb{\lambda_{-1}} = I.
\end{align*}


\Textbf{2.36}
\begin{align*}
	\Tr (\sigma_1) &= \Tr \left(
		\begin{bmatrix}
		0 & 1 \\
		1 & 0
		\end{bmatrix}
	\right) = 0\\
%
	\Tr (\sigma_2) &= \Tr \left(
		\begin{bmatrix}
		0 & -i \\
		i & 0
		\end{bmatrix}
	\right) = 0\\
%
	\Tr (\sigma_3) &= \Tr \left(
	\begin{bmatrix}
		1 & 0 \\
		0 & -1
	\end{bmatrix}
	\right) = 1 -1 = 0\\
\end{align*}




\Textbf{2.37}
\begin{align*}
	\Tr (AB) &= \sum_i \braket{i | AB | i}\\
		&=\sum_i \braket{i | A I B | i}\\
		&= \sum_{i,j} \braket{i | A | j}\braket{j | B | i}\\
		&= \sum_{i,j} \braket{j | B | i} \braket{i | A | j}\\
		&= \sum_j \braket{j | BA | j}\\
		&= \Tr (BA)
\end{align*}



\Textbf{2.38}
\begin{align*}
	\Tr (A + B) &= \sum_i \braket{i | A+B | i}\\
		&= \sum_i (\braket{i|A|i}  + \braket{i | B | i}  )\\
		&= \sum_i \braket{i|A|i} + \sum_i \braket{i|B|i}\\
		&= \Tr (A) + \Tr (B).
\end{align*}

\begin{align*}
	\Tr (z A) &=  \sum_i \braket{i | z A | i}\\
		&= \sum_i z \braket{i | A | i}\\
		&= z \sum_i \braket{i | A | i}\\
		&= z \Tr (A).
\end{align*}




\Textbf{2.39}

(1) $(A, B) \equiv \Tr (A^\dagger B)$.

\vspace{5mm}
(i)
\begin{align*}
	\left(A, \sum_i \lambda_i B_i \right) &= \Tr \left[ A^\dagger \left(\sum_i \lambda_i B_i  \right) \right]\\
		&= \Tr (A^\dagger \lambda_1 B_1) + \cdots +  \Tr (A^\dagger \lambda_n B_n) ~~~ (\because \text{Execise 2.38}) \\
		&= \lambda_1 \Tr (A^\dagger B_1)  + \cdots  + \lambda_n \Tr (A^\dagger B_n) \\
		&= \sum_i \lambda_i \Tr (A^\dagger B_i)
\end{align*}


(ii)
\begin{align*}
	(A, B)^* &= \left( \Tr (A^\dagger B) \right)^*\\
		&= \left(\sum_{i,j} \braket{ i | A^\dagger | j} \braket{j | B | i}  \right)^*\\
		&= \sum_{i,j} \braket{ i | A^\dagger | j}^* \braket{j | B | i}^*\\
		&= \sum_{i,j}  \braket{j | B | i}^* \braket{ i | A^\dagger | j}^*\\
		&=  \sum_{i,j}  \braket{i | B^\dagger | j} \braket{ j | A | i}\\
		&= \sum_i \braket{i | B^\dagger A | i} \\
		&= \Tr (B^\dagger A)\\
		&= (B, A).
\end{align*}


(iii)
\begin{align*}
	(A, A) &= \Tr (A^\dagger A)\\
		&= \sum_i \braket{ i | A^\dagger A | i }
\end{align*}
Since $A^\dagger A$ is positive, $\braket{ i | A^\dagger A | i } \geq 0$ for all $\ket{ i }$.


Let $a_i$ be i-th column of $A$.
If $\braket{ i | A^\dagger A | i } = 0$, then
\begin{align*}
	\braket{ i | A^\dagger A | i } = a^\dagger_i a_i = \norm{a_i}^2 = 0 \text{ iff }a_i = \mathbf{0}.
\end{align*}

Therefore $(A, A) = 0$ iff $A = \mathbf{0}$.

\vspace{5mm}
(2)

(3)


%\begin{bmatrix}
%	0 & 1 \\
%	1 & 0
%\end{bmatrix}
%
%\begin{bmatrix}
%	0 & -i \\
%	i & 0
%\end{bmatrix}
%
%\begin{bmatrix}
%	1 & 0 \\
%	0 & -1
%\end{bmatrix}

\Textbf{2.40}
\begin{align*}
	\left[X, Y \right] &=XY - YX\\
		&= \begin{bmatrix}
		0 & 1 \\
		1 & 0
		\end{bmatrix}
		\begin{bmatrix}
		0 & -i \\
		i & 0
		\end{bmatrix}
		-
		\begin{bmatrix}
		0 & -i \\
		i & 0
		\end{bmatrix}
		\begin{bmatrix}
		0 & 1 \\
		1 & 0
		\end{bmatrix} \\
%
		&=
%
		\begin{bmatrix}
			i & 0 \\
			0 & -i
		\end{bmatrix}
		-
		\begin{bmatrix}
			-i & 0 \\
			0 & i
		\end{bmatrix}\\
%
		&=
%
		\begin{bmatrix}
			2i & 0 \\
			0 & -2i
		\end{bmatrix} \\
%
		&=	2i Z
\end{align*}



\begin{align*}
	\left[Y, Z \right] &= \begin{bmatrix}
		0 & -i \\
		i & 0
	\end{bmatrix}
	\begin{bmatrix}
		1 & 0 \\
		0 & -1
	\end{bmatrix}
	-
	\begin{bmatrix}
		1 & 0 \\
		0 & -1
	\end{bmatrix}
	\begin{bmatrix}
		0 & -i \\
		i & 0
	\end{bmatrix}\\
	&=
	\begin{bmatrix}
		0 & 2i \\
		2i & 0
	\end{bmatrix}\\
	&= 2iX
\end{align*}



\begin{align*}
	\left[Z, X\right] &= \begin{bmatrix}
	1 & 0 \\
	0 & -1
	\end{bmatrix}
	\begin{bmatrix}
	0 & 1 \\
	1 & 0
	\end{bmatrix}
	-
	\begin{bmatrix}
	0 & 1 \\
	1 & 0
	\end{bmatrix}
	\begin{bmatrix}
	1 & 0 \\
	0 & -1
	\end{bmatrix}\\
	&=
	2i \begin{bmatrix}
	0 & -i \\
	i & 0
	\end{bmatrix}\\
	&= 2iY
\end{align*}



\Textbf{2.41}
\begin{align*}
\left\{\sigma_1, \sigma_2 \right\} &=\sigma_1 \sigma_2 + \sigma_2 \sigma_1\\
&= \begin{bmatrix}
0 & 1 \\
1 & 0
\end{bmatrix}
\begin{bmatrix}
0 & -i \\
i & 0
\end{bmatrix}
+
\begin{bmatrix}
0 & -i \\
i & 0
\end{bmatrix}
\begin{bmatrix}
0 & 1 \\
1 & 0
\end{bmatrix} \\
%
&=
%
\begin{bmatrix}
i & 0 \\
0 & -i
\end{bmatrix}
+
\begin{bmatrix}
-i & 0 \\
0 & i
\end{bmatrix}\\
%
&= 0
\end{align*}



\begin{align*}
\left\{\sigma_2, \sigma_3 \right\} &= \begin{bmatrix}
0 & -i \\
i & 0
\end{bmatrix}
\begin{bmatrix}
1 & 0 \\
0 & -1
\end{bmatrix}
+
\begin{bmatrix}
1 & 0 \\
0 & -1
\end{bmatrix}
\begin{bmatrix}
0 & -i \\
i & 0
\end{bmatrix}\\
&=0
\end{align*}



\begin{align*}
\left\{\sigma_3, \sigma_1 \right\} &= \begin{bmatrix}
1 & 0 \\
0 & -1
\end{bmatrix}
\begin{bmatrix}
0 & 1 \\
1 & 0
\end{bmatrix}
+
\begin{bmatrix}
0 & 1 \\
1 & 0
\end{bmatrix}
\begin{bmatrix}
1 & 0 \\
0 & -1
\end{bmatrix}\\
&=0
\end{align*}

\begin{align*}
	\sigma_0^2 &= I^2 = I\\
%
	\sigma_1^2 &= \begin{bmatrix}
	0 & 1 \\
	1 & 0
	\end{bmatrix} ^2 = I\\
%
	\sigma_2^2 &= \begin{bmatrix}
	0 & -i \\
	i & 0
	\end{bmatrix} ^2 = I\\
%
	\sigma_3^2 &= \begin{bmatrix}
	1 & 0 \\
	0 & -1
	\end{bmatrix} ^2 = I
\end{align*}



\Textbf{2.42}
\begin{align*}
	\frac{\left[A, B \right] + \left\{A, B\right\}}{2} = \frac{AB - BA + AB + BA}{2} = AB
\end{align*}



\Textbf{2.43}

From eq (2.75) and eq (2.76), $\left\{\sigma_j,  \sigma_k \right\} = 2 \delta_{jk} I$.
From eq (2.77),
\begin{align*}
	\sigma_j \sigma_k &= \frac{\left[\sigma_j, \sigma_k  \right] + \left\{\sigma_j, \sigma_k \right\}}{2}\\
		&= \frac{2i \sum_{l=1}^{3} \epsilon_{jkl}\sigma_l +  2 \delta_{jk} I}{2}\\
		&= \delta_{jk} I + i \sum_{l=1}^{3} \epsilon_{jkl}\sigma_l
\end{align*}


\Textbf{2.44}

By assumption, $\left[A, B\right] = 0$ and $\left\{A, B\right\} = 0$, then $AB = 0$.
Since $A$ is invertible, multiply by $A^{-1}$ from left, then
\begin{align*}
	A^{-1} AB = 0\\
	IB = 0\\
	B=0.
\end{align*}


\Textbf{2.45}
\begin{align*}
	\left[A, B\right]^\dagger &= (AB -BA)^\dagger\\
		&= B^\dagger A^\dagger - A^\dagger B^\dagger\\
		&= \left[B^\dagger, A^\dagger \right]
\end{align*}



\Textbf{2.46}
\begin{align*}
	\left[A, B\right] &= AB - BA\\
		&= - (BA - AB)\\
		&= -\left[B, A\right]
\end{align*}



\Textbf{2.47}
\begin{align*}
	\left(i \left[A, B\right] \right)^\dagger &= -i \left[A, B\right]^\dagger\\
		&= -i \left[B^\dagger, A^\dagger \right]\\
		&= -i \left[B, A \right]\\
		&= i \left[A, B\right]
\end{align*}



\Textbf{2.48}

(Positive )

 Since $P$ is positive, it is diagonalizable. Then $P = \sum_i \lambda_i \kb{i}$, $(\lambda_i \geq 0)$.
\begin{align*}
	J = \sqrt{P^\dagger P} = \sqrt{P P} = \sqrt{P^2} = \sum_i \sqrt{\lambda_i^2} \kb{i} = \sum_i \lambda_i \kb{i} = P.
\end{align*}
 Therefore polar decomposition of $P$ is $P = UP$ for all $P$.
 Thus $U = I$, then $P = P$.


\vspace{5mm}
(Unitary)

Suppose unitary $U$ is decomposed by $U = WJ$ where $W$ is unitary and $J$ is positive, $J = \sqrt{U^\dagger U}$.
\begin{align*}
	J = \sqrt{U^\dagger U} = \sqrt{I} = I
\end{align*}
Since unitary operators are invertible, $W = UJ^{-1} = UI^{-1} = UI = U$.
Thus polar decomposition of $U$ is $U = U$.


\vspace{5mm}
(Hermitian)

Suppose $H = UJ$.
\begin{align*}
	J = \sqrt{H^\dagger H} = \sqrt{HH} = \sqrt{H^2}.
\end{align*}
Thus $H = U\sqrt{H^2}$.

\begin{screen}
	In general, $H \neq \sqrt{H^2}$.

	From spectral decomposition, $H = \sum_i \lambda_i \kb{i}$, $\lambda_i \in \mathds{R}$.
	\begin{align*}
		 \sqrt{H^2} = \sqrt{ \sum_i \lambda_i^2 \kb{i} }
		 =
 		\sum_i
 			\sqrt{
 				\lambda_i^2
			} \kb{i}
		= \sum_i | \lambda_i | \kb{i} \neq H
	\end{align*}
\end{screen}


\Textbf{2.49}

Normal matrix is diagonalizable, $A = \sum_i \lambda_i \kb{i}$.
\begin{align*}
	J &= \sqrt{A^\dagger A} = \sum_i | \lambda_i | \kb{i}.\\
	U &= \sum_i \kbt{e_i}{i}\\
	A &= UJ = \sum_i |\lambda_i| \kbt{e_i}{i}.
\end{align*}



\Textbf{2.50}

Define
$A = \begin{bmatrix}
1 & 0 \\
1 & 1
\end{bmatrix}$.
%
$A^\dagger A = \begin{bmatrix}
2 & 1 \\
1 & 1
\end{bmatrix}$.

 Characteristic equation of $A^\dagger A$ is $\det(A^\dagger A - \lambda I) = \lambda^2 - 3 \lambda + 1 = 0$.
 Eigenvalues of $A^\dagger A$ are $\lambda_\pm = \frac{3 \pm \sqrt{5}}{2}$
 and associated eigenvectors are $\ket{\lambda_\pm} = \frac{1}{\sqrt{10 \mp 2 \sqrt{5}}} \begin{bmatrix}
 2 \\
 -1 \pm \sqrt{5}
 \end{bmatrix} $.

\begin{align*}
	A^\dagger A = \lambda_+ \kb{\lambda_+} + \lambda_- \kb{\lambda_-}.
\end{align*}

 \begin{align*}
 	J = \sqrt{A^\dagger A} &= \sqrt{\lambda_+} \kb{\lambda_+} + \sqrt{\lambda_-} \kb{\lambda_-}\\
 		&= \sqrt{\frac{3 + \sqrt{5}}{2}} \cdot \frac{5 - \sqrt{5}}{40} \begin{bmatrix}
 		4 & 2\sqrt{5} -2 \\
 		2 \sqrt{5} - 2 & 6 -2\sqrt{5}
 		\end{bmatrix}
 		+
 		\sqrt{\frac{3 - \sqrt{5}}{2}} \cdot \frac{5 + \sqrt{5}}{40} \begin{bmatrix}
 		4 & -2\sqrt{5} -2 \\
 		-2 \sqrt{5} - 2 & 6 +2\sqrt{5}
 		\end{bmatrix}
 \end{align*}


\begin{align*}
	J^{-1} = \frac{1}{ \sqrt{\lambda_+} } \kb{\lambda_+} + \frac{1}{ \sqrt{\lambda_-} } \kb{\lambda_-}.
\end{align*}


\begin{align*}
	U = AJ^{-1}
\end{align*}

I'm tired.




\Textbf{2.51}

\begin{align*}
	H^\dagger H = \left(\frac{1}{\sqrt{2}} \begin{bmatrix}
	1 & 1 \\
	1 & -1
	\end{bmatrix}\right)^\dagger
	\frac{1}{\sqrt{2}} \begin{bmatrix}
	1 & 1 \\
	1 & -1
	\end{bmatrix}
	=
	\frac{1}{\sqrt{2}} \begin{bmatrix}
	1 & 1 \\
	1 & -1
	\end{bmatrix}
	\frac{1}{\sqrt{2}} \begin{bmatrix}
	1 & 1 \\
	1 & -1
	\end{bmatrix}
	=
	\frac{1}{2} \begin{bmatrix}
	2 & 0 \\
	0 & 2
	\end{bmatrix}
	=
	I.
\end{align*}




\Textbf{2.52}

\begin{align*}
	H^\dagger = \left(\frac{1}{\sqrt{2}} \begin{bmatrix}
	1 & 1 \\
	1 & -1
	\end{bmatrix}\right)^\dagger
	=
	\frac{1}{\sqrt{2}} \begin{bmatrix}
	1 & 1 \\
	1 & -1
	\end{bmatrix}
	=
	H.
\end{align*}

Thus
\begin{align*}
	H^2 = I.
\end{align*}



\Textbf{2.53}

\begin{align*}
	\det \left(H - \lambda I\right) &= \left(\frac{1}{\sqrt{2}} - \lambda \right) \left(- \frac{1}{\sqrt{2}} - \lambda \right) - \frac{1}{2}\\
		&= \lambda^2 - \frac{1}{2} - \frac{1}{2}\\
		&= \lambda^2 - 1
\end{align*}

Eigenvalues are $\lambda_\pm = \pm 1$ and associated eigenvectors are $\ket{\lambda_\pm} = \frac{1}{\sqrt{4 \mp 2 \sqrt{2}}} \begin{bmatrix}
1 \\
-1 \pm \sqrt{2}
\end{bmatrix} $.




\Textbf{2.54}

Since $[A, B] = 0$, $A$ and $B$ are simultaneously diagonalize, $A = \sum_i a_i \kb{i}$, $B = \sum_i b_i \kb{i}$.
\begin{align*}
	\exp (A) \exp (B) &= \left(\sum_i \exp (a_i) \kb{i}\right) \left(\sum_i \exp (b_i) \kb{i}\right) \\
		&= \sum_{i,j} \exp (a_i + b_j) \ket{i} \braket{i | j} \bra{j}\\
		&= \sum_{i,j} \exp (a_i + b_j) \kbt{i}{j} \delta_{i, j}\\
		&= \sum_i \exp (a_i +  b_i) \kb{i}\\
		&= \exp (A+B)
\end{align*}


\Textbf{2.55}
\begin{align*}
	H = \sum_E E \kb{E}
\end{align*}

\begin{align*}
	U(t_2 - t_1) U^\dagger (t_2 - t_1) &= \exp \left( - \frac{iH(t_2 - t_1)}{\hbar} \right)  \exp \left(  \frac{iH(t_2 - t_1)}{\hbar} \right)\\
		&= \sum_{E, E'} \left(\exp \left(- \frac{iE(t_2 - t_1)}{\hbar} \right) \kb{E} \right)
										\left(\exp \left(- \frac{iE'(t_2 - t_1)}{\hbar} \right) \kb{E'} \right)\\
		&= \sum_{E, E'} \left(\exp \left(- \frac{i(E-E')(t_2 - t_1)}{\hbar} \right) \kbt{E}{E'} \delta_{E,E'} \right) \\
		&= \sum_E \exp(0) \kb{E}\\
		&= \sum_E \kb{E}\\
		&= I
\end{align*}

Similarly, $U^\dagger (t_2 - t_1) U (t_2 - t_1) = I$.



\Textbf{2.56}

$U = \sum_i \lambda_i \kb{\lambda_i}$~~~ ($|\lambda_i| = 1$).
\begin{align*}
	\log (U) &= \sum_j \log (\lambda_j) \kb{\lambda_j} = \sum_j i \theta_j  \kb{\lambda_j} \text{ where } \theta_j = \arg (\lambda_j)\\
	K &= - i \log(U) = \sum_j \theta_j \kb{\lambda_j}.
\end{align*}

\begin{align*}
	K^\dagger = (-i \log U)^\dagger = \left(\sum_j \theta_j \kb{\lambda_j}\right)^\dagger
	= \sum_j \theta_j^* \kb{\lambda_j} = \sum_j \theta_j \kb{\lambda_j} = K
\end{align*}



\Textbf{2.57}
\begin{align*}
	&\ket{\phi} \equiv \frac{L_l \ket{\psi}}{\sqrt{\braket{\psi |L_l^\dagger L_l | \psi}}}\\
	&\braket{\phi | M_m^\dagger M_m | \phi} = \frac{\braket{\psi | L_l^\dagger M_m^\dagger M_m L_l | \psi}}{\braket{\psi | L_l^\dagger L_l | \psi}} \\
%
	&\frac{M_m \ket{\phi}}{\sqrt{\braket{\phi | M_m^\dagger M_m | \phi}}} =
		\frac{M_m L_l \ket{\psi}}{\sqrt{\braket{\psi |L_l^\dagger L_l | \psi}}}
		\cdot
		\frac{ \sqrt{\braket{\psi | L_l^\dagger L_l | \psi}} }{ \sqrt{\braket{\psi | L_l^\dagger M_m^\dagger M_m L_l | \psi}} }
		=
		\frac{M_m L_l \ket{\psi}}{ \sqrt{\braket{\psi | L_l^\dagger M_m^\dagger M_m L_l | \psi}} }
		=
		\frac{N_{lm} \ket{\psi}}{\sqrt{\braket{\psi |N^\dagger_{lm}  N_{lm}  | \psi}}}
\end{align*}



\Textbf{2.58}
\begin{align*}
	\braket{M} &= \braket{ \psi | M | \psi} = \braket{\psi | m | \psi} = m \braket{\psi | \psi} = m\\
	\braket{M^2} &= \braket{ \psi | M^2 | \psi} = \braket{\psi | m^2 | \psi} = m^2 \braket{\psi | \psi} = m^2\\
	\text{deviation} &= \braket{M^2} - \braket{M}^2 = m^2 - m^2 = 0.
\end{align*}


\Textbf{2.59}
\begin{align*}
	\braket{X} &= \braket{0 | X | 0} = \braket{0 | 1} = 0\\
	\braket{X^2} &= \braket{0 | X^2 | 0} = \braket{0 | X | 1} =\braket{0 | 0} = 1\\
	\text{standard deviation} &= \sqrt{ \braket{X^2} - \braket{X}^2 } = 1
\end{align*}


\Textbf{2.60}
\begin{align*}
    \vec{v} \cdot \vec{\sigma} &= \sum_{i=1}^3 v_i \sigma_i\\
    &= v_1 \begin{bmatrix}
        0 & 1 \\
        1 & 0
    \end{bmatrix}
    + v_2 \begin{bmatrix}
        0 & -i \\
        i & 0
    \end{bmatrix}
    + v_3 \begin{bmatrix}
        1 & 0 \\
        0 & -1
    \end{bmatrix} \\
    &= \begin{bmatrix}
        v_3 & v_1 - i v_2 \\
        v_1 + iv_2 & -v_3
    \end{bmatrix}
\end{align*}

\begin{align*}
    \det (\vec{v} \cdot \vec{\sigma}  - \lambda I) &= (v_3 - \lambda) (-v_3 - \lambda) - (v_1 - iv_2) (v_1 + iv_2)\\
    &= \lambda^2 - (v_1^2 + v_2^2  + v_3^2)\\
    &= \lambda^2 - 1 ~~~ (\because |\vec{v}| = 1)
\end{align*}
Eigenvalues are $\lambda = \pm 1$.


(i) if $\lambda = 1$
\begin{align*}
	\vec{v} \cdot \vec{\sigma}  - \lambda I &= \vec{v} \cdot \vec{\sigma}  - I\\
		&= \begin{bmatrix}
    		v_3 - 1 & v_1 - i v_2 \\
    		v_1 + i v_2 & - v_3 - 1
		\end{bmatrix}
\end{align*}

Normalized eigenvector is $\ket{\lambda_1} = \sqrt{ \frac{1+v_3}{2 }} \begin{bmatrix}
1 \\
\frac{1-v_3}{v_1 - iv_2}
\end{bmatrix} $.

\begin{align*}
	\kb{\lambda_1} &= \frac{1+v_3}{2 } \begin{bmatrix}
		1 \\
		\frac{1-v_3}{v_1 - iv_2}
	\end{bmatrix}
	\begin{bmatrix}
   		1 &
   		\frac{1-v_3}{v_1 + iv_2}
	\end{bmatrix}\\
%
	&=
	 \frac{1+v_3}{2 } \begin{bmatrix}
    	 1 & \frac{v_1 - iv_2}{1 + v_3} \\
    	 \frac{v_1 + iv_2}{1 + v_3} & \frac{1-v_3}{1+v_3}
	 \end{bmatrix} \\
	 &=
	 \frac{1}{2} \begin{bmatrix}
    	 1+v_3 & v_1 - iv_2 \\
    	 v_1 + iv_2 & 1 - v_3
	 \end{bmatrix} \\
	 &=
	  \frac{1}{2} \left( I + \begin{bmatrix}
    	 v_3 & v_1 - iv_2 \\
    	 v_1 + iv_2 & - v_3
	 \end{bmatrix} \right) \\
	 &=
	 \frac{1}{2} (I + \vec{v} \cdot \vec{\sigma} )
\end{align*}



(ii) If $\lambda = -1$.
\begin{align*}
	\vec{v} \cdot \vec{\sigma}  - \lambda I &= \vec{v} \cdot \vec{\sigma}  + I\\
	&= \begin{bmatrix}
		v_3 + 1 & v_1 - i v_2 \\
		v_1 + i v_2 & - v_3 + 1
	\end{bmatrix}
\end{align*}

Normalized eigenvalue is $\ket{\lambda_{-1}} = \sqrt{ \frac{1-v_3}{2 }} \begin{bmatrix}
    1 \\
    - \frac{1+v_3}{v_1 - iv_2}
\end{bmatrix} $.


\begin{align*}
	\kb{\lambda_{-1}} &= \frac{1 - v_3}{2} \begin{bmatrix}
	1 \\
	- \frac{1+v_3}{v_1 - iv_2}
	\end{bmatrix}
	\begin{bmatrix}
		1 & - \frac{1+v_3}{v_1 + iv_2}
	\end{bmatrix}\\
	&=
	\frac{1 - v_3}{2} \begin{bmatrix}
		1 & - \frac{v_1 - iv_2}{1 - v_3} \\
		- \frac{v_1 + iv_2}{1 - v_3} & \frac{1+v_3}{1 - v_3}
	\end{bmatrix} \\
	&=
	\frac{1}{2} \begin{bmatrix}
		1 - v_3 & -(v_1 - iv_2) \\
		- (v_1 + iv_2) & 1 + v_3
	\end{bmatrix} \\
	&=
	\frac{1}{2} \left( I - \begin{bmatrix}
		v_3 & v_1 - iv_2 \\
		(v_1 + iv_2 & - v_3
	\end{bmatrix} \right)\\
	&= \frac{1}{2} (I - \vec{v} \cdot \vec{\sigma} ).
\end{align*}


	While I review my proof, I notice that my proof has a defect.
	The case $(v_1,v_2,v_3) = (0,0,1)$, second component of eigenstate, $\frac{1-v_3}{v_1 - iv_2}$, diverges.
	So I implicitly assume $v_1 - iv_2 \neq 0$. Hence my proof is incomplete.

	Since the exercise doesn't require explicit form of projector, we should prove the problem more abstractly.
	In order to prove, we use the following properties of $\vec{v} \cdot \vec{\sigma}$
	\begin{itemize}
		\item $\vec{v} \cdot \vec{\sigma}$ is Hermitian
		\item $(\vec{v} \cdot \vec{\sigma})^2 = I$ where $\vec{v}$ is a real unit vector.
	\end{itemize}

	We can easily check above conditions.
	\begin{align*}
	(\vec{v} \cdot \vec{\sigma})^\dagger &= (v_1 \sigma_1 + v_2 \sigma_2 + v_3 \sigma_3)^\dagger\\
	&= v_1 \sigma_1^\dagger + v_2 \sigma_2^\dagger + v_3 \sigma_3^\dagger\\
	&= v_1 \sigma_1 + v_2 \sigma_2 + v_3 \sigma_3~~~(\because \text{Pauli matrices are Hermitian.})\\
	&= \vec{v} \cdot \vec{\sigma}
	\end{align*}

	\begin{align*}
	(\vec{v} \cdot \vec{\sigma})^2 &= \sum_{j,k=1}^3 (v_j \sigma_j)  (v_k \sigma_k)\\
	&= \sum_{j,k=1}^3 v_j v_k \sigma_j \sigma_k\\
	&= \sum_{j,k=1}^3 v_j v_k \left(\delta_{jk}I + i \sum_{l=1}^3 \epsilon_{jkl}\sigma_l \right) ~~~(\because \text{eqn}(2.78)~ \text{page} 78)\\
	&= \sum_{j,k=1}^3 v_j v_k \delta_{jk}I  + i \sum_{j,k,l=1}^3 \epsilon_{jkl} v_j v_k \sigma_l\\
	&= \sum_{j=1}^3 v_j^2 I\\
	&= I ~~~\left(\because \sum_j v_j^2 = 1 \right)
	\end{align*}


	\begin{proof}
		Suppose $\ket{\lambda}$ is an eigenstate of $\vec{v} \cdot \vec{\sigma}$ with eigenvalue $\lambda$. Then
		\begin{align*}
		\vec{v} \cdot \vec{\sigma} \ket{\lambda} = \lambda \ket{\lambda}\\
		(\vec{v} \cdot \vec{\sigma})^2 \ket{\lambda} = \lambda^2 \ket{\lambda}
		\end{align*}
		On the other hand $(\vec{v} \cdot \vec{\sigma})^2 = I$,
		\begin{align*}
		(\vec{v} \cdot \vec{\sigma})^2 \ket{\lambda} = I \ket{\lambda} = \ket{\lambda}\\
		\therefore \lambda^2\ket{\lambda} = \ket{\lambda}.
		\end{align*}
		Thus $\lambda^2 = 1 \Rightarrow \lambda = \pm 1$. Therefore $\vec{v} \cdot \vec{\sigma}$ has eigenvalues $\pm 1$.

		Let $\ket{\lambda_1}$ and $\ket{\lambda_{-1}}$ are eigenvectors with eigenvalues $1$ and $-1$, respectively.
		I will prove that $P_\pm = \kb{\lambda_{\pm 1}}$.

		In order to prove above equation, all we have to do is prove following condition. (see Theorem \ref{thm:zerooperator})
		\begin{screen}
			\begin{align}
				\braket{\psi | (P_\pm - \kb{\lambda_{\pm 1}})| \psi} = 0 \text{ for all } \ket{\psi} \in \mathds{C}^2.
			\end{align}
		\end{screen}

		Since $\vec{v} \cdot \vec{\sigma}$ is Hermitian, $\ket{\lambda_1}$ and $\ket{\lambda_{-1}}$ are orthonormal vector ($\because $ Exercise 2.22).
		Let $\ket{\psi} \in \mathds{C}^2$ be an arbitrary state. $\ket{\psi}$ can be written as
		\begin{align*}
		\ket{\psi} = \alpha \ket{\lambda_1} + \beta \ket{\lambda_{\pm 1}} ~~(|\alpha|^2 + |\beta|^2 = 1, \alpha, \beta \in \mathds{C}).
		\end{align*}

		\begin{align*}
		\braket{\psi | (P_{\pm} - \kb{\lambda_\pm})| \psi}
		%		&= \braket{\psi | \left(\frac{1}{2} (I \pm \vec{v} \cdot \vec{\sigma}) - \kb{\lambda_\pm}\right)| \psi}\\
		&= \braket{\psi | P_\pm | \psi} - \braket{\psi | \lambda_\pm} \braket{\lambda_\pm | \psi}.\\
		\braket{\psi | P_\pm | \psi} &= \braket{\psi | \frac{1}{2}(I \pm \vec{v} \cdot \vec{\sigma}) | \psi}\\
		&= \frac{1}{2} \pm \frac{1}{2} \braket{\psi | \vec{v} \cdot \vec{\sigma})| \psi}\\
		&= \frac{1}{2} \pm \frac{1}{2} (|\alpha|^2 - |\beta|^2)\\
		&= \frac{1}{2} \pm \frac{1}{2} (2|\alpha|^2 - 1) ~~~(\because |\alpha|^2 + |\beta|^2 = 1)\\
		\braket{\psi | \lambda_1} \braket{\lambda_1 | \psi} &= |\alpha|^2\\
		\braket{\psi | \lambda_{-1}} \braket{\lambda_{-1}| \psi} &= |\beta|^2 = 1  - |\alpha|^2
		\end{align*}

		Therefore $\braket{\psi | (P_\pm - \kb{\lambda_{\pm 1}})| \psi} = 0$ for all $\ket{\psi} \in \mathds{C}^2$.
		Thus $P_\pm = \kb{\lambda_{\pm 1}}$.
	\end{proof}

\Textbf{2.61}
\begin{align*}
	\braket{\lambda_1 | 0} \braket{0 | \lambda_1} &= \braket{0 | \lambda_1} \braket{\lambda_1 | 0}\\
		&= \braket{0 | \frac{1}{2} ( I + \vec{v} \cdot \vec{\sigma} ) | 0}\\
		&= \frac{1}{2} (1 + v_3)
\end{align*}

Post-measurement state is
\begin{align*}
	\frac{\ket{\lambda_1} \braket{\lambda_1 | 0}}{ \sqrt{\braket{0 | \lambda_1} \braket{\lambda_1 | 0}} } &= \frac{1}{\sqrt{\frac{1}{2} (1 + v_3)}}
	\cdot \frac{1}{2}
	\begin{bmatrix}
		1 + v_3 \\
		v_1 + iv_2
	\end{bmatrix} \\
		&= \sqrt{ \frac{1}{2}  (1 + v_3) } \begin{bmatrix}
		1 \\
		\frac{v_1 + iv_2}{1+v_3}
		\end{bmatrix} \\
		&=  \sqrt{ \frac{1 + v_3}{2} } \begin{bmatrix}
		1 \\
		\frac{1 - v_3}{v_1 - iv_2}
		\end{bmatrix} \\
		&= \ket{\lambda_1}.
\end{align*}



\Textbf{2.62}

Suppose $M_m$ is a measurement operator.
From the assumption, $E_m = M_m^\dagger M_m = M_m$.

Then
\begin{align*}
    \braket{\psi | E_m | \psi} = \braket{\psi | M_m | \psi} \geq 0.
\end{align*}
for all $\ket{\psi}$.

Since $M_m$ is positive operator, $M_m$ is Hermitian.
Therefore,
\begin{align*}
    E_m = M_m^\dagger M_m = M_m M_m = M_m^2 = M_m.
\end{align*}

Thus the measurement is a projective measurement.



\Textbf{2.63}
\begin{align*}
    M_m^\dagger M_m &= \sqrt{E_m} U_m^\dagger U_m \sqrt{E_m}\\
        &= \sqrt{E_m} I \sqrt{E_m}\\
        &= E_m.
\end{align*}

Since $E_m$ is POVM,  for arbitrary  unitary $U$, $M_m^\dagger M_m$ is POVM.



\Textbf{2.64}
Read following paper:
\begin{itemize}
    \item Lu-Ming Duan, Guang-Can Guo.  Probabilistic cloning and identification of linearly independent quantum states. Phys. Rev. Lett.,80:4999-5002, 1998. arXiv:quant-ph/9804064\\
    \url{https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.80.4999}\\
    \url{https://arxiv.org/abs/quant-ph/9804064}
%
    \item Stephen M. Barnett, Sarah Croke, Quantum state discrimination, arXiv:0810.1970 [quant-ph]\\
    \url{https://arxiv.org/abs/0810.1970}\\
    \url{https://www.osapublishing.org/DirectPDFAccess/67EF4200-CBD2-8E68-1979E37886263936_176580/aop-1-2-238.pdf}
\end{itemize}


\Textbf{2.65}
\begin{align*}
    \ket{+} \equiv \frac{\ket{0} + \ket{1}}{\sqrt{2}}, ~~~ \ket{-} \equiv \frac{\ket{0} - \ket{1}}{\sqrt{2}}
\end{align*}


\Textbf{2.66}
\begin{align*}
     X_1 Z_2 \left(\frac{\ket{00} + \ket{11}}{\sqrt{2}} \right) = \frac{\ket{10} - \ket{01}}{\sqrt{2}}
\end{align*}


\begin{align*}
     \braket{X_1 Z_2} = \left(\frac{\bra{00} + \bra{11}}{\sqrt{2}} \right) X_1 Z_2 \left(\frac{\ket{00} + \ket{11}}{\sqrt{2}} \right)
    = \frac{\bra{00} + \bra{11}}{\sqrt{2}}  \cdot \frac{\ket{10} - \ket{01}}{\sqrt{2}}
    = 0
\end{align*}



\Textbf{2.67}

Suppose $W^\perp$ is the orthogonal complement of $W$. Then $V = W \oplus W^\perp$.
Let $\ket{w_i},\ket{w_j'},\ket{u_j'}$ be orthonormal bases for $W$, $W^\perp$, $\left( \mathrm{image}(U) \right)^\perp$, respectively.

% Define $U'$ as $U'|_W\ket{w_i} = U\ket{w_i} (= \ket{u_i})$ and $U'|_{W^\perp}\ket{w_j'} = \ket{u_j'}$
Define $U': V \rightarrow V$ as $U' = \sum_i \kbt{u_i}{w_i} + \sum_j \kbt{u_j'}{w_j'}$,
where $\ket{u_i} = U \ket{w_i}$.


Now
\begin{align*}
    (U')^\dagger U' &= \left( \sum_{i = 1}^{\dim W} \kbt{w_i}{u_i} + \sum_{j = 1}^{\dim W^\perp} \kbt{w_j'}{u_j'} \right)  \left( \sum_i \kbt{u_i}{w_i} + \sum_j \kbt{u_j'}{w_j'} \right)\\
                    &= \sum_i \kb{w_i} + \sum_j \kb{w_j'} = I
\end{align*}
%
and
%
\begin{align*}
    U' (U')^\dagger &= \left( \sum_i \kbt{u_i}{w_i} + \sum_j \kbt{u_j'}{w_j'} \right) \left( \sum_i \kbt{w_i}{u_i} + \sum_j \kbt{w_j'}{u_j'} \right)\\
                    &= \sum_i \kb{u_i} + \sum_j \kb{u_j'} = I.
\end{align*}
%
Thus $U'$ is an unitary operator.
Moreover, for all $\ket{w} \in W$,
\begin{align*}
    U' \ket{w} &= \left( \sum_i \kbt{u_i}{w_i} + \sum_j \kbt{u_j'}{w_j'} \right) \ket{w}\\
               &= \sum_i \ket{u_i} \braket{w_i | w} + \sum_j \ket{u_j'} \braket{w_j' | w}\\
			   &= \sum_i \ket{u_i} \braket{w_i | w}  ~~~(\because \ket{w_j'} \perp \ket{w})\\
			   &= \sum_i U \ket{w_i} \braket{w_i | w}\\
			   &= U \ket{w}.
\end{align*}
%
Therefore $U'$ is an extension of $U$.


\Textbf{2.68}

$\ket{\psi} = \frac{\ket{00} + \ket{11}}{\sqrt{2}}$.

Suppose $\ket{a} = a_0 \ket{0}  + a_1\ket{1}$ and $\ket{b} = b_0 \ket{0}  + b_1\ket{1}$.
%
\begin{align*}
    \ket{a} \ket{b} = a_0 b_0 \ket{00} + a_0 b_1 \ket{01} + a_1 b_0 \ket{10} + a_1 b_1 \ket{11}.
\end{align*}

If $\ket{\psi} = \ket{a} \ket{b}$, then $a_0 b_0 = 1,~ a_0 b_1=0,~ a_1 b_0 = 0,~ a_1 b_1 = 1$ since $\{\ket{ij}\}$ is an orthonormal basis.

If $a_0 b_1 = 0$, then $a_0 = 0$ or $b_1 = 0$.

When $a_0 = 0$ , this is contradiction to $a_0 b_0 = 1$.
When $b_1 = 0$ , this is contradiction to $a_1 b_1 = 1$.

Thus $\ket{\psi} \neq \ket{a} \ket{b}$.


\Textbf{2.69}
Define Bell states as follows.
\begin{align*}
    \ket{\psi_1} &\equiv \frac{\ket{00} + \ket{11}}{\sqrt{2}} = \frac{1}{\sqrt{2}} \begin{bmatrix}
    1 \\
    0 \\
    0 \\
    1
    \end{bmatrix} \\
    \ket{\psi_2} &\equiv \frac{\ket{00} - \ket{11}}{\sqrt{2}} = \frac{1}{\sqrt{2}} \begin{bmatrix}
    1 \\
    0 \\
    0 \\
    -1
    \end{bmatrix} \\
    \ket{\psi_3} &\equiv \frac{\ket{01} + \ket{10}}{\sqrt{2}} = \frac{1}{\sqrt{2}} \begin{bmatrix}
    0 \\
    1 \\
    1 \\
    0
    \end{bmatrix} \\
    \ket{\psi_4} &\equiv \frac{\ket{01} - \ket{10}}{\sqrt{2}} = \frac{1}{\sqrt{2}} \begin{bmatrix}
    0 \\
    1 \\
    -1 \\
    0
    \end{bmatrix} \\
\end{align*}

First, we prove $\{\ket{\psi_i} \}$ is a linearly independent basis.
\begin{align*}
    &a_1 \ket{\psi_1} + a_2 \ket{\psi_2} + a_3 \ket{\psi_3} + a_4 \ket{\psi_4} = 0\\
    &\therefore \frac{1}{\sqrt{2}} \begin{bmatrix}
        a_1 + a_2 \\
        a_3 + a_4 \\
        a_3 - a_4 \\
        a_1 - a_2
    \end{bmatrix} = 0
\end{align*}
\begin{subnumcases}
 \therefore {}
a_1 + a_2 = 0& \nonumber \\
a_3+ a_4 = 0& \nonumber \\
a_3 - a_4 = 0& \nonumber \\
a_1 - a_2 = 0& \nonumber
\end{subnumcases}
\begin{align*}
    \therefore a_1 = a_2 = a_3 = a_4 = 0
\end{align*}
Thus $\{\ket{\psi_i}\}$ is a linearly independent basis.

Moreover $\norm{\ket{\psi_i}} = 1$ and $\braket{\psi_i | \psi_j} = \delta_{ij}$ for $i,j = 1, 2, 3, 4$.
Therefore $\{\ket{\psi_i}\}$ forms an orthonormal basis.




\Textbf{2.70}

For any Bell states we get $\braket{\psi_i | E \otimes I | \psi_i} = \frac{1}{2} (\braket{0|E|0} + \braket{1|E|1})$.

Suppose Eve measures the qubit Alice sent by measurement operators $M_m$.
The probability that Eve gets result $m$ is $p_i(m) = \braket{\psi_i | M_m^\dagger M_m \otimes I | \psi_i}$.
Since $M ^\dagger_m M_m$ is positive, $p_i(m)$ are same values for all $\ket{\psi_i}$.
Thus Eve can't distinguish Bell states.




\Textbf{2.71}

From spectral decomposition,
\begin{align*}
    \rho &= \sum_i p_i \kb{\psi_i}, ~~ p_i \geq 0, ~~ \sum_i p_i = 1.\\
    \rho^2 &= \sum_{i,j} p_i p_j \ket{i}\braket{i|j}\bra{j}\\
        &= \sum_{i,j} p_i p_j \kbt{i}{j}\delta_{ij}\\
        &= \sum_i p_i^2 \kb{i}
\end{align*}

\begin{align*}
    \Tr (\rho^2) &= \Tr \left(\sum_i p_i^2 \kb{i}\right)
        = \sum_i p_i^2 \Tr(\kb{i})
        = \sum_i p_i^2 \braket{i|i}
        = \sum_i p_i^2
        \leq \sum_i p_i = 1~~~ (\because p_i^2 \leq p_i)
\end{align*}

Suppose $\Tr (\rho^2) = 1$. Then $\sum_i p_i^2 = 1$.
Since $p_i^2 < p_i$ for $0 < p_i < 1$,
only single $p_i$ should be 1 and otherwise have to  vanish.
Therefore $\rho = \kb{\psi_i}$. It is a pure state.

Conversely if $\rho$ is pure, then $\rho = \kb{\psi}$.
\begin{align*}
    \Tr (\rho^2) = \Tr (\ket{\psi}\braket{\psi | \psi} \bra{\psi}) = \Tr (\kb{\psi}) = \braket{\psi | \psi} = 1.
\end{align*}



\Textbf{2.72}

(1) Since density matrix is Hermitian, matrix representation is
$\rho = \begin{bmatrix}
    a & b \\ b^* & d
\end{bmatrix}$,
$a, d \in \mathds{R}$ and $b \in \mathds{C}$ w.r.t. standard basis.
Because $\rho$ is density matrix, $\Tr (\rho) = a+d = 1$.

Define $a = (1+r_3)/2$, $d = (1-r_3)/2$ and $b = (r_1 - ir_2)/2$, $(r_i \in \mathds{R})$.

In this case,
\begin{align*}
    \rho = \begin{bmatrix}
        a & b \\ b^* & d
    \end{bmatrix}
    =
    \frac{1}{2} \begin{bmatrix}
        1+r_3 & r_1 - ir_2 \\
        r_1 + ir_2 & 1 - r_3
    \end{bmatrix}
    =
    \frac{1}{2} (I + \vec{r} \cdot \vec{\sigma}).
\end{align*}
Thus for arbitrary density matrix $\rho$ can be written as $\rho = \frac{1}{2} (I + \vec{r} \cdot \vec{\sigma})$.

Next, we derive the condition that $\rho$ is positive.

If $\rho$ is positive, all eigenvalues of $\rho$ should be non-negative.
\begin{align*}
    \det (\rho - \lambda I) &= (a-  \lambda) (b - \lambda) - |b|^2 = \lambda^2 - (a+d)\lambda + ad - |b^2| = 0\\
    \lambda &= \frac{(a+d) \pm \sqrt{(a+d)^2 - 4 (ad - |b|^2)}}{2}\\
        &= \frac{1 \pm \sqrt{1 - 4 \left(\frac{1 - r_3^2}{4} - \frac{r_1^2 + r_2^2}{4} \right)}}{2}\\
        &= \frac{1 \pm \sqrt{1 - (1 - r_1^2 - r_2^2 - r_3^2)}}{2}\\
        &= \frac{1 \pm \sqrt{|\vec{r}|^2}}{2}\\
        &= \frac{1 \pm |\vec{r}|}{2}
\end{align*}

Since $\rho$ is positive, $\frac{1 - |\vec{r}|}{2} \geq 0 \rightarrow |\vec{r}| \leq 1$.

Therefore an arbitrary density matrix for a mixed state qubit is written as $\rho = \frac{1}{2} (I + \vec{r} \cdot \vec{\sigma})$.

\vspace{5mm}
(2)

$\rho = I / 2 \rightarrow \vec{r}  = 0$. Thus  $\rho = I / 2$ corresponds to the origin of Bloch sphere.

\vspace{5mm}
(3)

\begin{align*}
    \rho^2 &= \frac{1}{2} (I + \vec{r} \cdot \vec{\sigma})~ \frac{1}{2} (I + \vec{r} \cdot \vec{\sigma})\\
        &= \frac{1}{4} \left[ I + 2 \vec{r}\cdot \vec{\sigma} + \sum_{j,k}r_j r_k \left(\delta_{jk} I + i \sum_{l=1}^3 \epsilon_{jkl}\sigma_l \right)  \right]\\
        &= \frac{1}{4} \left(I + 2 \vec{r}\cdot \vec{\sigma} + |\vec{r}|^2 I \right)\\
    \Tr (\rho^2) &= \frac{1}{4} (2 + 2|\vec{r}|^2)
\end{align*}

If $\rho$ is pure, then $\Tr (\rho^2) = 1$.
\begin{align*}
   1 =  \Tr (\rho^2) = \frac{1}{4} (2 + 2|\vec{r}|^2)\\
   \therefore |\vec{r}| = 1.
\end{align*}

Conversely, if $|\vec{r}| = 1$, then $\Tr (\rho^2) = \frac{1}{4} (2 + 2|\vec{r}|^2) = 1$. Therefore $\rho$ is pure.




\Textbf{2.73}
\begin{screen}
    \Textbf{Theorem 2.6}
%
    \begin{align*}
        \rho = \sum_i p_i \kb{\psi_i}
            = \sum_i \kb{\tilde{\psi_i}}
            = \sum_j \kb{\tilde{\varphi}_j}
            = \sum_j q_j \kb{\varphi_j}
                ~~ \Leftrightarrow ~~
            \ket{\tilde{\psi}_i} = \sum_j u_{ij} \ket{\tilde{\varphi}_j}
    \end{align*}
    where $u$ is unitary.

	The-transformation in theorem 2.6, $\ket{\tilde{\psi}_i} = \sum_j u_{ij} \ket{\tilde{\varphi}_j}$, corresponds to
	\begin{align*}
	    \left[ \ket{\tilde{\psi}_1} \cdots \ket{\tilde{\psi}_k} \right] = \Big[ \ket{\tilde{\varphi}_1} \cdots \ket{\tilde{\varphi}_k} \Big] U^T
	\end{align*}
	where $k = \mathrm{rank} (\mathcal{\rho})$.
    \begin{align}
        \sum_i \kb{\tilde{\psi_i}} &= \left[ \ket{\tilde{\psi}_1} \cdots \ket{\tilde{\psi}_k} \right]
            \begin{bmatrix}
                \bra{\tilde{\psi_1}}\\
                \vdots\\
                \bra{\tilde{\psi_k}}
            \end{bmatrix}\\
        &= \Big[ \ket{\tilde{\varphi}_1} \cdots \ket{\tilde{\varphi}_k} \Big] U^T
            U^* \begin{bmatrix}
                    \bra{\tilde{\varphi}_1}\\
                    \vdots\\
                    \bra{\tilde{\varphi}_k}
            \end{bmatrix}\\
        &= \Big[ \ket{\tilde{\varphi}_1} \cdots \ket{\tilde{\varphi}_k} \Big]
             \begin{bmatrix}
                \bra{\tilde{\varphi}_1}\\
                \vdots\\
                \bra{\tilde{\varphi}_k}
            \end{bmatrix}\\
        &= \sum_j \kb{\tilde{\varphi}_j}.
    \end{align}
\end{screen}

From spectral theorem, density matrix $\rho$ is decomposed as $\rho = \sum_{k=1}^{d} \lambda_k \kb{k}$ where $d = \dim \mathcal{H}$.
Without loss of generality, we can assume $p_k > 0$ for $k = 1 \cdots , l$ where $l = \mathrm{rank} (\rho)$ and $p_k = 0$ for $k = l+1, \cdots, d$.
Thus $\rho = \sum_{k=1}^{l} p_k \kb{k} = \sum_{k=1}^{l} \kb{\tilde{k}}$, where $\ket{\tilde{k}} = \sqrt{\lambda_k} \ket{k}$.

Suppose $\ket{\psi_i}$ is a state in support $\rho$. Then
\begin{align*}
	\ket{\psi_i} = \sum_{k=1}^l c_{ik} \ket{k}, ~~ \sum_k |c_{ik}|^2 = 1.
\end{align*}

Define $\displaystyle p_i = \frac{1}{\sum_k \frac{|c_{ik}|^2}{\lambda_k} }$ and $\displaystyle u_{ik} = \frac{\sqrt{p_i} c_{ik}}{\sqrt{\lambda_k}}$.

Now
\begin{align*}
	\sum_k |u_{ik}|^2 = \sum_k \frac{p_i | c_{ik} |^2 }{\lambda_k} = p_i \sum_k \frac{| c_{ik} |^2 }{\lambda_k} = 1.
\end{align*}

Next prepare an unitary operator
\footnote{By Gram-Schmidt procedure construct an orthonormal basis $\{\boldsymbol{u}_j\}$ (row vector) with $\boldsymbol{u}_i = [u_{i1} \cdots u_{ik} \cdots u_{il}]$. Then define unitary $U = \begin{bmatrix}
    \boldsymbol{u}_1 \\
    \vdots \\
    \boldsymbol{u}_i \\
    \vdots \\
    \boldsymbol{u}_l
    \end{bmatrix}$.}
such that $i$th row of $U$ is $[u_{i1} \cdots u_{ik} \cdots u_{il}]$.
Then we can define another ensemble such that
\begin{align*}
	\Big[  \ket{\tilde{\psi}_1} \cdots  \ket{\tilde{\psi}_i} \cdots \ket{\tilde{\psi}_l}\Big] = \Big[ \ket{\tilde{k}_1} \cdots \ket{\tilde{k}_l} \Big] U^T
\end{align*}
where $\ket{\tilde{\psi_i}} = \sqrt{p_i} \ket{\psi_i}$.
From theorem 2.6,
\begin{align*}
	\rho = \sum_k \kb{\tilde{k}} = \sum_k \kb{\tilde{\psi}_k}.
\end{align*}

Therefore we can obtain a minimal ensemble for $\rho$ that contains $\ket{\psi_i}$.

Moreover since $\rho^{-1} = \sum_k \frac{1}{\lambda_k} \kb{k}$,
\begin{align*}
	\braket{\psi_i | \rho^{-1} | \psi_i} = \sum_k \frac{1}{\lambda_k} \braket{\psi_i | k} \hspace{-1mm} \braket{k | \psi_i} = \sum_k \frac{|c_{ik}|^2}{\lambda_k} = \frac{1}{p_i}.
\end{align*}

Hence, $ \frac{1}{\braket{\psi_i | \rho^{-1} | \psi_i}} = p_i $.


\Textbf{2.74}
\begin{align*}
	\rho_{AB} &= \kb{a}_A \otimes \kb{b}_B\\
	\rho_A &= \Tr_{B} \rho_{AB} = \kb{a} \Tr (\kb{b}) = \kb{a}\\
	\Tr (\rho_A^2) &= 1
\end{align*}
Thus $\rho_A$ is pure.


\Textbf{2.75}
Define $\ket{\Phi_\pm} = \frac{1}{\sqrt{2}} (\ket{00} \pm \ket{11})$ and $\ket{\Psi_\pm} = \frac{1}{\sqrt{2}} (\ket{01} \pm \ket{10})$.
\begin{align*}
	\kb{\Phi_\pm}_{AB} &= \frac{1}{2} (\kb{00} \pm \kbt{00}{11} \pm \kbt{11}{00} + \kb{11})\\
	\Tr_B (\kb{\Phi_\pm}_{AB}) &= \frac{1}{2} (\kb{0} + \kb{1}) = \frac{I}{2}\\
%
	\kb{\Psi_\pm} &= \frac{1}{2} (\kb{01} \pm \kbt{01}{10} \pm \kbt{10}{01} + \kb{10})\\
	\Tr_B (\kb{\Psi_\pm}) &= \frac{1}{2} (\kb{0} + \kb{1}) = \frac{I}{2}
\end{align*}


\Textbf{2.76}

Unsolved. \sout{I think the polar decomposition can only apply to square matrix $A$, not arbitrary linear operators.
Suppose $A$ is $m \times n$ matrix. Then size of $A^\dagger A$ is $n \times n$. Thus the size of $U$ should be $m \times n$.
Maybe $U$ is isometry, but I think it is not unitary.}

I misunderstand linear operator.
\begin{quote}
	Quoted from "Advanced Liner Algebra" by Steven Roman, ISBN 0387247661.

	A linear transformation $\tau : V \rightarrow V$ is called a \textbf{linear operator} on $V$.\footnote{According to Roman, some authors use the term linear operator for any linear transformation from $V$ to $W$.}
\end{quote}
Thus coordinate matrices of linear operator are square matrices. And Nielsen and Chaung say at Theorem 2.3, "Let $A$ be a linear operator on a vector space $V$." Therefore $A$ is a linear transformation such that $A : V \rightarrow V$.

\Textbf{2.77}
\begin{align*}
	\ket{\psi}  &=  \ket{0}  \ket{\Phi_+}\\
		&= \ket{0} \left[\frac{1}{\sqrt{2}}(\ket{00} + \ket{11})\right]\\
		&= (\alpha \ket{\phi_0} + \beta \ket{\phi_1})  \left[\frac{1}{\sqrt{2}}(\ket{\phi_0 \phi_0} + \ket{\phi_1 \phi_1})\right]\\
\end{align*}
where $\ket{\phi_i}$ are arbitrary orthonormal states and $\alpha, \beta \in \mathds{C}$.
We cannot vanish cross term. Therefore $\ket{\psi}$ cannot be written as $\ket{\psi} = \sum_i \lambda_i \ket{i}_A \ket{i}_B \ket{i}_C$.


\Textbf{2.78}
\begin{proof}
	Former part.

	If $\ket{\psi}$ is product, then there exist a state $\ket{\phi_A}$ for system $A$, and a state $\ket{\phi_B}$ for system $B$ such that
	$\ket{\psi} = \ket{\phi_A} \ket{\phi_B}$.

	Obviously, this Schmidt number is  1.

	Conversely, if Schmidt number is 1, the state is written as $\ket{\psi} = \ket{\phi_A} \ket{\phi_B}$.
	Hence this is a product state.
\end{proof}


\begin{proof}
	Later part.

	($\Rightarrow$) Proved by exercise 2.74.

	($\Leftarrow$) Let a pure state be  $\ket{\psi} = \sum_i \lambda_i \ket{i_A} \ket{i_B}$. Then $\rho_A = \Tr_B (\kb{\psi}) = \sum_i \lambda_i^2 \kb{i}$.
	If $\rho_A$ is a pure state, then $\lambda_j = 1$ and otherwise 0 for some $j$.
	It follows that  $\ket{\psi_j} = \ket{j_A} \ket{j_B}$. Thus $\ket{\psi}$ is a product state.
\end{proof}


\Textbf{2.79}
\begin{screen}
	Procedure of Schmidt decomposition.

	Goal: $\ket{\psi} = \sum_{i} \sqrt{\lambda_i} \ket{i_A} \ket{i_B}$

	\begin{itemize}
		\item Diagonalize reduced density matrix $\rho_A = \sum_i \lambda_i \kb{i_A}$.
		\item Derive $\ket{i_B}$, $\displaystyle  \ket{i_B} = \frac{(I \otimes \bra{i_A}) \ket{\psi}}{\sqrt{\lambda_i}}$
		\item Construct $\ket{\psi}$.
	\end{itemize}

\end{screen}


(i)
\begin{align*}
	\frac{1}{\sqrt{2}} (\ket{00} + \ket{11}) \text{ This is already decomposed.}
\end{align*}


(ii)
\begin{align*}
	\frac{\ket{00}+ \ket{01} + \ket{10} + \ket{11}}{2} = \left( \frac{\ket{0} + \ket{1}}{\sqrt{2}}  \right) \otimes \left( \frac{\ket{0} + \ket{1}}{\sqrt{2}}  \right) = \ket{\psi} \ket{\psi} \text{ where } \ket{\psi} = \frac{\ket{0} + \ket{1}}{\sqrt{2}}
\end{align*}



(iii)
\begin{align*}
	\ket{\psi}_{AB} &= \frac{1}{\sqrt{3}} (\ket{00} + \ket{01} + \ket{10})\\
	\rho_{AB} &= \kb{\psi}_{AB}
\end{align*}
%
%
%
\begin{align*}
	\rho_A &= \Tr_B (\rho_{AB}) = \frac{1}{3} \left( 2\kb{0} + \kbt{0}{1} + \kbt{1}{0} + \kb{1} \right)\\
	\det (\rho_A - \lambda I) &= \left( \frac{2}{3} - \lambda \right) \left( \frac{1}{3} - \lambda \right) - \frac{1}{9} = 0\\
	\lambda^2 &- \lambda + \frac{1}{9} = 0\\
	\lambda &= \frac{1 \pm \sqrt{5} / 3}{2} = \frac{3 \pm \sqrt{5}}{6}
\end{align*}



Eigenvector with eigenvalue $\displaystyle \lambda_0 \equiv \frac{3 + \sqrt{5}}{6}$ is $\displaystyle \ket{\lambda_0} \equiv \frac{1}{\sqrt{\frac{5 + \sqrt{5}}{2}}} \begin{bmatrix}
    \frac{1 + \sqrt{5}}{2} \\
    1
\end{bmatrix}$ .

Eigenvector with eigenvalue $\displaystyle \lambda_1 \equiv \frac{3 - \sqrt{5}}{6}$ is $\displaystyle \ket{\lambda_1} \equiv \frac{1}{\sqrt{\frac{5 - \sqrt{5}}{2}}} \begin{bmatrix}
    \frac{1 - \sqrt{5}}{2} \\
    1
\end{bmatrix} $.

\begin{align*}
	\rho_A = \lambda_0 \kb{\lambda_0} + \lambda_1 \kb{\lambda_1}.
\end{align*}


\begin{align*}
	\ket{a_0} \equiv \frac{(I \otimes \bra{\lambda_0}) \ket{\psi} }{\sqrt{\lambda_0}}\\
	\ket{a_1} \equiv \frac{(I \otimes \bra{\lambda_1}) \ket{\psi} }{\sqrt{\lambda_1}}
\end{align*}

Then
\begin{align*}
	\ket{\psi} = \sum_{i=0}^1 \sqrt{\lambda_i} \ket{a_i} \ket{\lambda_i}.
\end{align*}


(It's too tiresome to calculate $\ket{a_i}$)



\Textbf{2.80}

Let $\ket{\psi} = \sum_i \lambda_i \ket{\psi_i}_A \ket{\psi_i}_B$ and $\ket{\varphi} = \sum_i \lambda_i \ket{\varphi_i}_A \ket{\varphi_i}_B$.

Define $U = \sum_i \kbt{\psi_j}{\varphi_j}_A$ and $V = \sum_j \kbt{\psi_j}{\varphi_j}_B$.

Then
\begin{align*}
	(U \otimes V) \ket{\varphi} &= \sum_i \lambda_i U \ket{\varphi_i}_A  V \ket{\varphi_i}_B\\
		&= \sum_i \lambda_i \ket{\psi_i}_A \ket{\psi_i}_B\\
		&= \ket{\psi}.
\end{align*}


\Textbf{2.81}

Let the Schmidt decomposition of $\ket{AR_1}$ be $\ket{AR_1} = \sum_i \sqrt{p_i} \ket{\psi_i^A} \ket{\psi_i^R}$ and
let $\ket{AR_2} = \sum_i \sqrt{q_i} \ket{\phi_i^A} \ket{\phi_i^R}$.

Suppose $\rho^A$ has orthonormal decomposition $\rho^A = \sum_i p_i \kb{i}$.

Since $\ket{AR_1}$ and $\ket{AR_2}$ are purifications of the $\rho^A$, we have
% \begin{align*}
%     \ket{AR_1} &= \sum_i \sqrt{p_i} \ket{i} \ket{\psi_i}\\
%     \ket{AR_2} &= \sum_i \sqrt{p_i} \ket{i} \ket{\phi_i}
% \end{align*}
% where $\ket{\psi_i}$ and $\ket{\phi_i}$ are orthonormal bases on system $R$.
%
\begin{align*}
    \Tr_R (\kb{AR_1}) = \Tr_R (\kb{AR_2}) = \rho^A\\
    \therefore \sum_i p_i \kb{\psi_i^A} = \sum_i q_i \kb{\phi_i^A} = \sum_i \lambda_i \kb{i}.
\end{align*}

The $\ket{i}$, $\ket{\psi_i^A}$, and $\ket{\psi_i^A}$ are orthonormal bases and they are eigenvectors of $\rho^A$.
Hence without loss of generality, we can consider
\begin{align*}
    \lambda_i = p_i = q_i \text{ and } \ket{i} = \ket{\psi_i^A} = \ket{\phi_i^A}.
\end{align*}
%
Then
\begin{align*}
    \ket{AR_1} = \sum_i \lambda_i \ket{i} \ket{\psi_i^R}\\
    \ket{AR_2} = \sum_i \lambda_i \ket{i} \ket{\phi_i^R}
\end{align*}
Since $\ket{AR_1}$ and $\ket{AR_2}$ have same Schmidt numbers, there are two unitary operators $U$ and $V$ such that
$\ket{AR_1} = (U \otimes V) \ket{AR_2}$ from exercise 2.80.

Suppose $U = I$ and $V = \sum_i \kbt{\psi_i^R}{\phi_i^R}$.
Then
\begin{align*}
    \left(I \otimes \sum_j \kbt{\psi_j^R}{\phi_j^R} \right) \ket{AR_2} &= \sum_i \lambda_i \ket{i} \left( \sum_j \ket{\psi_j^R} \braket{\phi_j^R | \phi_i^R} \right)\\
                                                           &= \sum_i \lambda_i \ket{i} \ket{\psi_i^R}\\
                                                           &= \ket{AR_1}.
\end{align*}
%
Therefore there exists a unitary transformation $U_R$ acting on system $R$ such that $\ket{AR_1} = (I \otimes U_R) \ket{AR_2}$.


\Textbf{2.82}

(1)

Let $\ket{\psi} = \sum_i \sqrt{p_i} \ket{\psi_i} \ket{i}$.
\begin{align*}
    \Tr_R (\kb{\psi})
        &= \sum_{i,j} \sqrt{p_i} \sqrt{p_j} \kbt{\psi_i}{\psi_j} \Tr_R (\kbt{i}{j})\\
        &= \sum_{i,j} \sqrt{p_i} \sqrt{p_j} \kbt{\psi_i}{\psi_j} \delta_{ij}\\
        &= \sum_i p_i \kb{\psi_i} = \rho.
\end{align*}
Thus $\ket{\psi}$ is a purification of $\rho$.

\vspace{5mm}
(2)

Define the projector $P$ by $P = I \otimes \kb{i}$.
The probability we get the result $i$ is
\begin{align*}
    \Tr \left[ P \kb{\psi}\right] = \braket{\psi | P | \psi} = \braket{\psi | (I \otimes \kb{i}) | \psi} = p_i \braket{\psi_i | \psi_i} = p_i.
\end{align*}

The post-measurement state is
\begin{align*}
    \frac{P \ket{\psi}}{\sqrt{p_i}}
    = \frac{(I \otimes \kb{i}) \ket{\psi}}{\sqrt{p_i}}
        = \frac{\sqrt{p_i} \ket{\psi_i}\ket{i}}{\sqrt{p_i}} = \ket{\psi_i}\ket{i}.
\end{align*}

If we only focus on the state on system $A$,
\begin{align*}
    \Tr_R (\ket{\psi_i} \ket{i}) = \ket{\psi_i}.
\end{align*}

\vspace{5mm}
(3)

($\{ \ket{\psi_i} \}$ is not necessary an orthonormal basis.)


Suppose $\ket{AR}$ is a purification of $\rho$ and its Schmidt decomposition is $\ket{AR} = \sum_i \sqrt{\lambda_i} \ket{\phi_i^A} \ket{\phi_i^R}$.

From assumption
\begin{align*}
    \Tr_R \left( \kb{AR} \right) = \sum_i \lambda_i \kb{\phi_i^A} = \sum_i p_i \kb{\psi_i}.
\end{align*}

By theorem 2.6, there exits an unitary matrix $u_{ij}$ such that $\sqrt{\lambda_i}\ket{\phi_i^A} = \sum_j u_{ij} \sqrt{p_j} \ket{\psi_j}$.
Then
\begin{align*}
    \ket{AR} &= \sum_i \left( \sum_j u_{ij} \sqrt{p_j} \ket{\psi_j} \right) \ket{\phi_i^R}\\
             &= \sum_j \sqrt{p_j} \ket{\psi_j} \otimes\left( \sum_i u_{ij} \ket{\phi_i^R} \right)\\
             &= \sum_j \sqrt{p_j} \ket{\psi_j} \ket{j}\\
             &= \sum_i \sqrt{p_i} \ket{\psi_i} \ket{i}
\end{align*}
where $\ket{i} = \sum_k u_{ki} \ket{\phi_k^R}$.

About $\ket{i}$,
\begin{align*}
    \braket{k | l} &= \sum_{m,n} u_{mk}^* u_{nl} \braket{\phi_m^R | \phi_n^R }\\
        &= \sum_{m,n} u_{mk}^* u_{nl} \delta_{mn}\\
        &= \sum_m u_{mk}^* u_{ml}\\
        &= \delta_{kl}, ~~~(\because u_{ij} \text{ is unitary.})
\end{align*}
which implies $\ket{j}$ is an orthonormal basis for system $R$.

Therefore if we measure system $R$ w.r.t $\ket{j}$, we obtain $j$ with probability $p_j$ and post-measurement state for $A$ is $\ket{\psi_j}$ from (2).
Thus for any purification $\ket{AR}$, there exists an orthonormal basis $\ket{i}$ which satisfies the assertion.


\Textbf{Problem 2.1}

From Exercise 2.35, $\vec{n} \cdot \vec{\sigma}$ is decomposed as
\begin{align*}
	\vec{n} \cdot \vec{\sigma} &= \kb{\lambda_1} - \kb{\lambda_{-1}}
\end{align*}
where $\ket{\lambda_{\pm 1}}$ are eigenvector of $\vec{n} \cdot \vec{\sigma}$ with eigenvalues $\pm 1$.

Thus
\begin{align*}
	f(\theta \vec{n} \cdot \vec{\sigma}) &= f(\theta) \kb{\lambda_1} + f(- \theta) \kb{\lambda_{-1}}\\
		&= \left( \frac{f(\theta) + f(-\theta)}{2} + \frac{f(\theta) - f(-\theta)}{2}  \right) \kb{\lambda_1} + \left( \frac{f(\theta) + f(-\theta)}{2} - \frac{f(\theta) - f(-\theta)}{2}  \right)\kb{\lambda_{-1}}\\
		&= \frac{f(\theta) + f(-\theta)}{2} \left( \kb{\lambda_1} + \kb{\lambda_{-1}} \right) +  \frac{f(\theta) - f(-\theta)}{2} \left(\kb{\lambda_1} - \kb{\lambda_{-1}} \right)\\
		&= \frac{f(\theta) + f(-\theta)}{2} I + \frac{f(\theta) - f(-\theta)}{2} \vec{n} \cdot \vec{\sigma}
\end{align*}

\Textbf{Problem 2.2}
Unsolved

\Textbf{Problem 2.3}
Unsolved
