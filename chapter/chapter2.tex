%!TeX root=../solnQCQI.tex

\chapter{Introduction to quantum mechanics}
\Textbf{2.1} Show that $(1, -1), (1,2)$, and $(2,1)$ are linearly dependent.
\Soln It is enough to express $(0, 0)$ as a linear combination of the specified vectors.
\begin{align*}
	\begin{bmatrix}
		1 \\
		-1
	\end{bmatrix}
	+
	\begin{bmatrix}
		1 \\
		2
	\end{bmatrix}
	-
	\begin{bmatrix}
		2 \\
		1
	\end{bmatrix}
	=
	\begin{bmatrix}
		0 \\
		0
	\end{bmatrix}
\end{align*}

\Textbf{2.2} Suppose $V$ is a vector space with basis vectors $\ket{0}$ and $\ket{1}$, and $A$ is a linear operator
from $V$ to $V$ such that $A\ket{0}=\ket{1}$ and $A\ket{1}=\ket{0}$.  Give a matrix representation for $A$, with
respect to the input basis $\ket{0},\ket{1}$, and the output basis $\ket{0},\ket{1}$.  Find input and output bases which
give rise to a different matrix representation of $A$.
\Soln With specified operations, it is enough to solve for the entries of a 2x2 matrix which
coverts the input vectors expressed as linear combinations of one basis, say $(\ket{a_1},\ket{a_2}),$
into vectors expressed as linear combinations of another basis, say $(\ket{b_1},\ket{b_2})$.
\[
A = \begin{blockarray}{ccc}
\begin{block}{ccc}
& \ket{b_1} & \ket{b_2}\\
    \end{block}
\begin{block}{ c[cc] }
\ket{a_1} & A_{11} & A_{12} \\
\ket{a_2} & A_{21} & A_{22} \\
            \end{block}
        \end{blockarray}
\]
With $(\ket{a_1},\ket{a_2}) = (\ket{0},\ket{1})$ and $(\ket{b_1},\ket{b_2}) = (\ket{0},\ket{1})$, we have
\begin{align*}
	A\ket{0} &:= \ket{1} = 0\ket{0}+1\ket{1}= A_{11}\ket{b_1} + A_{21}\ket{b_2} = A_{11}\ket{0} + A_{21}\ket{1}\Rightarrow A_{11} = 0,\ A_{21} = 1\\
	A\ket{1} &:= \ket{0} = 1\ket{0}+0\ket{1}=A_{12}\ket{b_1} + A_{22}\ket{b_2} = A_{12}\ket{0} + A_{22}\ket{1}\Rightarrow A_{12} = 1,\ A_{22} = 0\\
%
	\therefore A &=
	\begin{bmatrix}
		0 & 1 \\
		1 & 0
	\end{bmatrix}
\end{align*}
If the output basis was $(\ket{b_1},\ket{b_2}) = (\ket{1},\ket{0})$ instead, then $A=I$.  More formally:
\begin{align*}
    A\ket{0} &:= \ket{1} = 1\ket{1}+0\ket{0}= A_{11}\ket{b_1} + A_{21}\ket{b_2} = A_{11}\ket{1} + A_{21}\ket{0}\Rightarrow A_{11} = 1,\ A_{21} = 0\\
    A\ket{1} &:= \ket{0} = 0\ket{1}+1\ket{0}= A_{12}\ket{b_1} + A_{22}\ket{b_2} = A_{12}\ket{1} + A_{22}\ket{0}\Rightarrow A_{12} = 0,\ A_{22} = 1\\
        %
    \therefore A &=
    \begin{bmatrix}
    1 & 0 \\
    0 & 1
    \end{bmatrix}
\end{align*}
With a more interesting orthonormal output basis $(\ket{b_1},\ket{b_2})=(\ket{+},\ket{-})$:
\begin{align*}
        A\ket{0} &:= \ket{1} = \frac{\sqrt{2}}{2}\ket{+}-\frac{\sqrt{2}}{2}\ket{-}= A_{11}\ket{b_1} + A_{21}\ket{b_2} = A_{11}\ket{+} + A_{21}\ket{-}\Rightarrow A_{11} = \frac{\sqrt{2}}{2},\ A_{2} = -\frac{\sqrt{2}}{2}\\
        A\ket{1} &:= \ket{0} = \frac{\sqrt{2}}{2}\ket{+}+\frac{\sqrt{2}}{2}\ket{-}= A_{12}\ket{b_1} + A_{22}\ket{b_2} = A_{12}\ket{+} + A_{22}\ket{-}\Rightarrow A_{12} = \frac{\sqrt{2}}{2},\ A_{22} = \frac{\sqrt{2}}{2}\\
        %
    \therefore A &= \frac{\sqrt{2}}{2}
    \begin{bmatrix}
    1 & -1 \\
    1 & 1
    \end{bmatrix}
\end{align*}
Note: This is similar, but not equal to $\mathbf{H}$.  Had $A$ been the identity transformation when expressed with the
same input and output bases, then the result would have been exactly $\mathbf{H}$.

\Textbf{2.3} Suppose $A$ is a linear operator from vector space $V$  to vector space $W$, and $B$ is a linear operator
from vector space $W$ to vector space $X$.  Let $\ket{v_i}, \ket{w_j},$ and $\ket{x_k}$ be bases for the vector spaces
$V, W,$ and $X$, respectively.  Show that the matrix representation for the linear transformation $BA$ is the matrix
product of the matrix representations for $B$ and $A$ with respect to the appropriate bases.
\Soln Fix $i$.  We'll show that $(B\circ A)_{ki} = (B \cdot A)_{ki}$.
\begin{align*}
(B\circ A) \ket{v_i} = \sum_k (B\circ A)_{ki} \ket{x_k} &= B \left( \sum_{j} A_{ji}\ket{w_j} \right) \tag{Eqn 2.12, composition}\\
	&= \sum_{j} A_{ji} B\ket{w_j}\tag{linearity}\\
	&= \sum_{j,k} A_{ji} B_{kj}\ket{x_k}\tag{Eqn 2.12}\\
	&= \sum_k \left( \sum_j B_{kj} A_{ji}  \right) \ket{x_k}\tag{finiteness, commutativity}\\
    &= \sum_k \left( (B\cdot A)_{ki}  \right) \ket{x_k}\tag{definition}\\\\
	\therefore & (B\circ A)_{ki} = (B\cdot A)_{ki}
\end{align*}

\Textbf{2.4} Show that the identity operator on a vector space $V$ has a matrix representation which is one along the
diagonal and zero everywhere else, if the matrix representation is taken with respect to the same input and output bases.
This matrix is known as the \textit{identity matrix}
\Soln Let $I$ be the matrix in question.
\begin{align*}
I\ket{v_j} := \ket{v_j} &= \sum_i I_{ij} \ket{v_i}, \ \forall j.\\
	&\Rightarrow I_{ij} = \delta_{ij} := \left\{ \begin{array}{cc} 1 & i=j \\ \\ 0 & o/w\end{array}\right.
\end{align*}

\Textbf{2.5} Verify that $(\cdot, \cdot)$ just defined is an inner product on $\mathbb{C}^n$
\Soln Defined inner product on $\mathbb{C}^n$ is
\begin{align*}
	\left(
		(y_1, \cdots, y_n), (z_1, \cdots, z_n)
	\right)
	= \sum_{i} y_i^* z_i .
\end{align*}
Equation (2.13.1), linearity in second argument:
\begin{align*}
	\left((y_1,\cdots,y_n),\sum_i\lambda_i(z_{i1},\cdots,z_{in})\right)
	&= \left((y_1,\cdots,y_n),\left(\sum_i\lambda_iz_{i1},\cdots,\sum_i\lambda_iz_{in}\right)\right) \tag{definition} \\
    &= \sum_jy^*_j\left(\sum_i\lambda_iz_{ij}\right) \tag{definition}\\
    &= \sum_j\left(\sum_iy^*_j\lambda_iz_{ij}\right) \tag{linearity of multiplication}\\
    &= \sum_j\left(\sum_i\lambda_iy^*_jz_{ij}\right) \tag{associativity/commutativity}\\
    &= \sum_i\left(\sum_j\lambda_iy^*_jz_{ij}\right) \tag{finiteness} \\
    &= \sum_i\lambda_i\left(\sum_jy^*_jz_{ij}\right) \tag{linearity} \\
    &= \sum_i\lambda_i\left((y_1,\cdots,y_n),(z_{i1},\cdots,z_{in})\right) \tag{definition}
\end{align*}
%
%
%    &= \sum_i\lambda_i\left(
%(y_1, \cdots, y_n),(z_{i1}, \cdots, z_{in})
%\right) \tag{definition}
%

% =\sum_i y_i^* \left(
% 										\sum_j \lambda_j z_{ji}
% 			      				   \right)\\
% 	&= \sum_{i,j} y_i^* \lambda_j z_{ji}\\
% 	&= \sum_j \lambda_j \left(\sum_i y_i^* z_{ji}  \right)\\
% 	&= \sum_j \lambda_j \left(
% 													(y_1, \cdots, y_n),  (z_{j1}, \cdots, z_{jn})
% 											  \right)\\
% 	&= \sum_i \lambda_i \left(
% 													(y_1, \cdots, y_n),  (z_{i1}, \cdots, z_{in})
% 												\right).
Equation (2.13.2), conjugate symmetry:
\begin{align}
	\bigl((y_1, \cdots, y_n), (z_1, \cdots, z_n)\bigr)^*
	&= \left(\sum_i y_i^* z_i \right)^* \tag{definition}\\
	&= \left(\sum_i y_i  z_i^* \right) \tag{conjugate symmetry in $\mathbb{C}^1$}\\
	&= \left(\sum_i z_i^* y_i \right) \tag{commutativity in $\mathbb{C}^1$}\\
	&= \bigl((z_1, \cdots, z_n) , (y_1, \cdots, y_n)\bigr) \tag{definition}
\end{align}
Equation (2.13.3), positive definiteness:
\begin{align*}
	\bigl((y_1, \cdots, y_n), (y_1, \cdots, y_n)\bigr)
	&= \sum_i y_i^* y_i \tag{definition}\\
	&= \sum_i |y_i|^2 \tag{definition} \\
	&\geq 0 \tag{positive definiteness of $|\cdot|^2$ over $\mathbb{C}^1$}
\end{align*}

Now:
\begin{align*}
	\bigl((y_1, \cdots, y_n), (y_1, \cdots, y_n)\bigr) = \sum_i |y_i|^2 &\stackrel{?}{=} 0 \tag{hypothesis}\\ \iff |y_i|^2 &= 0\  \forall i \tag{positivity of $|\cdot|^2$}\\ \iff \ y_i\ \  &= 0\  \forall i \tag{positive definiteness of $|\cdot|^2$ over $\mathbb{C}^1$}\\ \iff (y_1, \cdots, y_n) &=  \textbf{0} \tag{definition}
\end{align*}
%Since $|y_i|^2 \geq 0$ for all $i$. Thus
%$\sum_i |y_i|^2 =
%\left(
%	(y_1, \cdots, y_n), (y_1, \cdots, y_n)
%\right) \geq 0
%$.
%
%From now on,  I will show the following statement,
%\begin{align*}
%	\left(
%		(y_1, \cdots, y_n), (y_1, \cdots, y_n)
%	\right) = 0
%	\text{ iff }  (y_1, \cdots, y_n) = 0.
%\end{align*}
%($\Leftarrow$) This is obvious.\\
%($\Rightarrow$)
%Suppose $\left( (y_1, \cdots, y_n), (y_1, \cdots, y_n) \right) = 0$. Then $\sum_i |y_i|^2 = 0$.
%
%Since $|y_i|^2 \geq 0$ for all $i$, if $\sum_i |y_i|^2 = 0$, then $|y_i|^2 = 0$ for all $i$.
%Therefore $|y_i|^2 = 0 \Leftrightarrow y_i = 0$  for all $i$.
%Thus,
%\begin{align*}
%	(y_1, \cdots, y_n) = 0.
%\end{align*}

\Textbf{2.6} Show that any inner product $(\cdot,\cdot)$ is conjugate-linear in the first argument,
\[
\left(\sum_i \lambda_i\ket{w_i}, \ket{v}\right) = \sum_i \lambda_i^{*}(\ket{w_i}, \ket{v}).
\]
\Soln
\begin{align*}
	\left(\sum_i \lambda_i \ket{w_i},\ket{v}\right) &=
	\bigl(\ket{v},\sum_i \lambda_i \ket{w_i}\bigr)^* \tag{conjugate symmetry}\\
	&= \left(\sum_i \lambda_i \bigl(\ket{v},\ket{w_i}  \bigr) \right)^*\tag{linearity in the 2nd arg.}\\
	&= \sum_i \lambda_i^* \bigl(\ket{v},\ket{w_i} \bigr)^* \tag{distributivity of complex conjugate}\\
	&= \sum_i \lambda_i^* (\ket{w_i},\ket{v}) \tag{conjugate symmetry}
\end{align*}

\Textbf{2.7} Verify that $\ket{w} = (1,1)$ and $\ket{v} = (1,-1)$ are orthogonal.  What are the normalized forms of these vectors?
\Soln
\begin{align*}
	 (\ket{w}, \ket{v}) &= 	\braket{w | v} \tag{notation} \\
	 &= \begin{bmatrix}
		1^* & 1^*
		\end{bmatrix}
		\begin{bmatrix}
		1 \\
		-1
		\end{bmatrix} \tag{definition}\\
	&= 1^*\cdot1 + 1^*\cdot(-1) \tag{matrix multiplication} \\
	&= 1\cdot1 - 1\cdot1 = 0 \tag{arithmetic}\\
%
	\frac{\ket{w}}{\norm{\ket{w}}} &=
	\frac{\ket{w}}{\sqrt{\braket{w|w}}} = \frac{1}{\sqrt{2}} \begin{bmatrix}
	1 \\
	1
	\end{bmatrix}
	= \ket{+} \\
%
	\frac{\ket{v}}{\norm{\ket{v}}} &=
	\frac{\ket{v}}{\sqrt{\braket{v|v}}} = \frac{1}{\sqrt{2}} \begin{bmatrix}
	1 \\
	-1
	\end{bmatrix}
	=\ket{-}
\end{align*}

\Textbf{2.8} Prove that the Gram-Schmidt procedure produces an orthonormal basis.
\Soln
We prove inductively.  For $d = 1$, the only requirement is that the procedure normalize $\ket{w_d}$, which it does by definition for all $d$.   For $d = 2$, suppose $\ket{v_1}, \cdots,\ket{v_{d-1}}$ is a orthonormal basis for the subspace spanned by  $\ket{w_1},\cdots,\ket{w_{d-1}}$.  Being a basis, the subspace spanned by $\ket{v_1},\cdots,\ket{v_{d-1}}$ is the same.  Linear independence of $\ket{w_1},\cdots,\ket{w_d}$ implies that $\ket{w_d}$ is not in this subspace, so $\ket{v_1},\cdots,\ket{v_{d-1}},\ket{w_d}$ is easily seen to be linearly independent as well.  It remains to be shown that $\ket{v_d}$ is linearly independent of $\ket{v_1},\cdots,\ket{v_{d-1}}$, and is orthogonal to all such vectors.  For independence, note that any dependence relation between $\ket{v_1},\cdots,\ket{v_d}$ immediately induces one between $\ket{v_1},\cdots,\ket{v_{d-1}},\ket{w_d}$, violating their independence.  For orthogonality, let $1\leq j\leq d-1$.  We show $\braket{v_j|v_d} = 0$, completing the proof.
\begin{comment}
\begin{align*}
	\ket{v_2} &= \frac{\ket{w_2} - \braket{v_1 | w_2}\ket{v_1}}{\norm{\ket{w_2} - \braket{v_1 | w_2}\ket{v_1}}}\\
	\braket{v_1 | v_2} &= \bra{v_1} \left(\frac{\ket{w_2} - \braket{v_1 | w_2}\ket{v_1}}{\norm{\ket{w_2} - \braket{v_1 | w_2}\ket{v_1}}}\right)\\
		&= \frac{\braket{v_1 | w_2} - \braket{v_1 | w_2}\braket{v_1 | v_1}}{\norm{\ket{w_2} - \braket{v_1 | w_2}\ket{v_1}}}\\
		&= 0.
\end{align*}
\end{comment}
\begin{align*}
	\braket{v_j | v_d} &= \bra{v_j} \left(\frac{\ket{w_d} - \sum_{i=1}^{d-1}\braket{v_i | w_d}\ket{v_i}}{\norm{\ket{w_d} - \sum_{i=1}^{d-1}\braket{v_i | w_d}\ket{v_i}}}\right) \tag{definition}\\
	&= \frac{\braket{v_j | w_d} - \sum_{i=1}^{d-1}\braket{v_i | w_d}\braket{v_j | v_i}}{\norm{\ket{w_d} - \sum_{i=1}^{d-1}\braket{v_i | w_d}\ket{v_i}}} \tag{linearity in the 2nd argument}\\
	&= \frac{\braket{v_j | w_d} - \sum_{i=1}^{d-1}\braket{v_i | w_d}\delta_{ij}}{\norm{\ket{w_d} - \sum_{i=1}^{d-1}\braket{v_i | w_d}\ket{v_i}}} \tag{orthonormality of $\ket{v_1},\cdots,\ket{v_{d-1}}$}\\
	&= \frac{\braket{v_j | w_d} - \braket{v_j | w_d}}{\norm{\ket{w_d} - \sum_{i=1}^{d-1}\braket{v_i | w_d}\ket{v_i}}} \tag{definition of $\delta_{ij}$}\\
	&= 0 \tag{arithmetic}.
\end{align*}

\Textbf{2.9} \textbf{(Pauli operators and the outer product)} The Pauli matrices can be considered as operators with respect to an orthonormal basis $\ket{0}, \ket{1}$ for a two-dimensional Hilbert space.  Express each of the Pauli operators in the outer product notation.
\begin{align*}
	\sigma_0 &= I \hspace{4.5pt}= \ket{0}\bra{0} + \ket{1}\bra{1}\\
	\sigma_x = \sigma_1 &= X = \ket{1}\bra{0} + \ket{0}\bra{1}\\
	\sigma_y = \sigma_2 &= Y \hspace{1.5pt}=  i\ket{1}\bra{0} - i\ket{0}\bra{1}\\
	\sigma_z = \sigma_3 &= Z \hspace{1.5pt}= \ket{0}\bra{0} - \ket{1}\bra{1}
\end{align*}

\Textbf{2.10} Suppose $\ket{v_i}$ is an orthonormal basis for an inner product space $V$.  What is the matrix representation for the operator $\ket{v_j}\bra{v_k}$, with respect to the $\ket{v_i}$ basis?
\Soln \begin{comment} Let $\ket{v}=\sum_i a_i\ket{v_i}$ be a vector in $V$. 
\begin{align*}
	(\ket{v_j}\bra{v_k})(\ket{v}) = (\ket{v_j}\bra{v_k})\left(\sum_i a_i\ket{v_i}\right) &= \sum_i\braket{v_k|v_i}\ket{v_j} \tag{definition}\\
	&= \sum_i\delta_{ki}\ket{v_j}
\end{align*}
\end{comment}
\begin{align*}
	\ket{v_j}\bra{v_k} &= I_V \ket{v_j} \bra{v_k} I_V \tag{multiply by identity}\\
	&= \left(\sum_p \ket{v_p}\bra{v_p} \right) \ket{v_j}\bra{v_k} \left(\sum_q \ket{v_q}\bra{v_q} \right) \tag{completeness}\\
	&= \sum_{p,q} \ket{v_p} \braket{v_p|v_j}\braket{v_k | v_q} \bra{v_q} \tag{linearity and outer product definition}\\
	&= \sum_{p,q} \delta_{pj} \delta_{kq} \ket{v_p} \bra{v_q} \tag{orthonormality}
\end{align*}
Thus
\begin{align*}
	\left( \ket{v_j}\bra{v_k} \right)_{pq} = \delta_{pj} \delta_{kq} = \left\{\begin{array}{ll}1 & p=j, k=q \\ 0 & o/w\end{array}.\right. 
\end{align*}
That is, $\ket{v_j}\bra{v_k}$ is a square matrix with a 1 in row $j$,  column $k$, and 0s everywhere else.

\Textbf{(Cauchy-Schwartz inequality} A brief expansion from a mathematician: in equation (2.26), other $\ket{i}$-basis vectors appear, but since $\braket{i|v}=\braket{v|i}^*$, $a\cdot a^* = \norm{a} \geq 0$ for all $a\in\mathbb{C}$, and $\braket{\cdot | \cdot}$ is positive definite, all terms but the first constructed in terms of $\ket{w}$ are non-negative and can be removed, leaving the inequality.

\Textbf{2.11} Eigendecomposition of the Pauli matrices: Find the eigenvectors, eigenvalues, and diagonal representations of the Pauli matrices $X, Y$, and $Z$.
\Soln
\begin{align*}
	X = \begin{bmatrix}
	0 & 1 \\
	1 & 0
	\end{bmatrix},\ \det(X-\lambda I) =
	\det \left(\begin{bmatrix}
	-\lambda & 1 \\
	1 & -\lambda
	\end{bmatrix} \right) = \lambda^2-1 = 0 \Rightarrow \lambda = \pm 1
\end{align*}
If $\lambda = 1$,
\begin{align*}
	\begin{bmatrix}
		0 & 1 \\
		1 & 0
	\end{bmatrix}
	\begin{bmatrix}
		c_1 \\
		c_2
	\end{bmatrix} =
	\begin{bmatrix}
	      c_2 \\
	      c_1
	\end{bmatrix} =
	 1
	\begin{bmatrix}
		c_1 \\
		c_2
	\end{bmatrix}
	\Rightarrow c_2 = c_1
\end{align*}
The eigenspace corresponding to $\lambda = 1$ is the set of vectors $\begin{bmatrix}c \\ c\end{bmatrix}$.  The vector $\frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ 1\end{bmatrix} = \ket{+}$ is such a unit (normalized) vector. 
If $\lambda = -1$,
\begin{align*}
	\begin{bmatrix}
		0 & 1 \\
		1 & 0
	\end{bmatrix}
	\begin{bmatrix}
		c_1 \\
		c_2
	\end{bmatrix} =
	\begin{bmatrix}
	      c_2 \\
	      c_1
	\end{bmatrix} =
	 -1
	\begin{bmatrix}
		c_1 \\
		c_2
	\end{bmatrix}
	\Rightarrow c_2 = -c_1
\end{align*}
The eigenspace corresponding to $\lambda = -1$ is the set of vectors $\begin{bmatrix}c \\ -c\end{bmatrix}$.  The vector $\frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ -1\end{bmatrix} = \ket{-}$ is such a unit (normalized) vector.  So, a diagonal representation of $X$ (when expressed in terms of the computational basis) is $\bigl(\ket{+}\bra{+}\bigr)-\bigl(\ket{-}\bra{-}\bigr) = \begin{bmatrix}\frac{1}{2} & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{2}\end{bmatrix} - \begin{bmatrix} \frac{1}{2} & \frac{-1}{2} \\ \frac{-1}{2} & \frac{1}{2}\end{bmatrix} \bigl( = X\bigr)$.
\begin{align*}
	Y = \begin{bmatrix}
	0 & -i \\
	i & 0
	\end{bmatrix},\ \det(Y-\lambda I) =
	\det \left(\begin{bmatrix}
	-\lambda & -i \\
	i & -\lambda
	\end{bmatrix} \right) = \lambda^2-(i)(-i)=\lambda^2 - 1 = 0 \Rightarrow \lambda = \pm 1
\end{align*}
If $\lambda = 1$,
\begin{align*}
	\begin{bmatrix}
		0 & -i \\
		i & 0
	\end{bmatrix}
	\begin{bmatrix}
		c_1 \\
		c_2
	\end{bmatrix} =
	\begin{bmatrix}
	      -i\cdot c_2 \\
	      i\cdot c_1
	\end{bmatrix} =
	 1
	\begin{bmatrix}
		c_1 \\
		c_2
	\end{bmatrix}
	\Rightarrow c_2 = i\cdot c_1
\end{align*}
The eigenspace corresponding to $\lambda = 1$ is the set of vectors $\begin{bmatrix}c \\ i\cdot c\end{bmatrix}$.  The vector $\frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ i\end{bmatrix}\equiv\ket{\psi_{y^+}}$ is such a unit (normalized) vector. 
If $\lambda = -1$,
\begin{align*}
	\begin{bmatrix}
		0 & -i \\
		i & 0
	\end{bmatrix}
	\begin{bmatrix}
		c_1 \\
		c_2
	\end{bmatrix} =
	\begin{bmatrix}
	      -i\cdot c_2 \\
	      i\cdot c_1
	\end{bmatrix} =
	 -1
	\begin{bmatrix}
		c_1 \\
		c_2
	\end{bmatrix}
	\Rightarrow c_2 = -i\cdot c_1
\end{align*}
The eigenspace corresponding to $\lambda = -1$ is the set of vectors $\begin{bmatrix}c \\ -i\cdot c\end{bmatrix}$.  The vector $\frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ -i\end{bmatrix}\equiv\ket{\psi_{y^-}}$ is such a unit (normalized) vector.  So, a diagonal representation of $Y$ (when expressed in terms of the computational basis) is $\bigl(\ket{\psi_{y^+}}\bra{\psi_{y^+}}\bigr) - \bigl(\ket{\psi_{y^-}}\bra{\psi_{y^-}}\bigr) = \begin{bmatrix}\frac{1}{2} & \frac{-i}{2} \\ \frac{i}{2} & \frac{1}{2}\end{bmatrix} - \begin{bmatrix}\frac{1}{2} & \frac{i}{2} \\ \frac{-i}{2} & \frac{1}{2}\end{bmatrix} \bigl( = Y\bigr)$.
\begin{align*}
	Z = \begin{bmatrix}
	1 & 0 \\
	0 & -1
	\end{bmatrix},\ \det(Z-\lambda I) =
	\det \left(\begin{bmatrix}
	1-\lambda & 0 \\
	0 & -1-\lambda
	\end{bmatrix} \right) = (\lambda+1)(\lambda- 1) = 0 \Rightarrow \lambda = \pm 1
\end{align*}
If $\lambda = 1$,
\begin{align*}
	\begin{bmatrix}
		1 & 0 \\
		0 & -1
	\end{bmatrix}
	\begin{bmatrix}
		c_1 \\
		c_2
	\end{bmatrix} =
	\begin{bmatrix}
	      c_1 \\
	      -c_2
	\end{bmatrix} =
	 1
	\begin{bmatrix}
		c_1 \\
		c_2
	\end{bmatrix}
	\Rightarrow c_2 = -c_2 \Rightarrow c_2 = 0
\end{align*}
The eigenspace corresponding to $\lambda = 1$ is the set of vectors $\begin{bmatrix}c \\ 0\end{bmatrix}$.  The vector $\begin{bmatrix} 1 \\ 0\end{bmatrix}=\ket{0}$ is such a unit (normalized) vector. 
If $\lambda = -1$,
\begin{align*}
	\begin{bmatrix}
		1 & 0 \\
		0 & -1
	\end{bmatrix}
	\begin{bmatrix}
		c_1 \\
		c_2
	\end{bmatrix} =
	\begin{bmatrix}
	      c_1 \\
	      -c_2
	\end{bmatrix} =
	 -1
	\begin{bmatrix}
		c_1 \\
		c_2
	\end{bmatrix}
	\Rightarrow c_1 = -c_1
\end{align*}
The eigenspace corresponding to $\lambda = -1$ is the set of vectors $\begin{bmatrix}0 \\ c\end{bmatrix}$.  The vector $\begin{bmatrix} 0 \\ 1\end{bmatrix}=\ket{1}$ is such a unit (normalized) vector.  So, the computation basis \textit{is} the eigenbasis for $Z$, and a diagonal representation of $Z$ is $\bigl(\ket{0}\bra{0}\bigr) - \bigl(\ket{1}\bra{1}\bigr) = \begin{bmatrix}1 & 0 \\ 0 & 0\end{bmatrix} - \begin{bmatrix}0 & 0 \\ 0 & -1\end{bmatrix} \bigl( = Z\bigr)$.


\Textbf{2.12} Prove that the matrix $\begin{bmatrix}1 & 0 \\ 1 & 1\end{bmatrix} (\equiv A)$ is not diagonalizable.
\Soln
\begin{align*}\det(A-\lambda I) = 
	\det \left(\begin{bmatrix}
	1 & 0 \\
	1 & 1
	\end{bmatrix} - \lambda I \right) = \det \left(\begin{bmatrix}1-\lambda & 0 \\ 1 & 1-\lambda\end{bmatrix}\right)= (1 - \lambda)^2 = 0 \Rightarrow \lambda = 1\ \mathrm{(with\ multiplicity\ 2)}
\end{align*}
All eigenvectors $\begin{bmatrix}c_1\\c_2\end{bmatrix}$ satisfy:
\begin{align*}
	\begin{bmatrix}
		1 & 0 \\
		1 & 1
	\end{bmatrix}
	\begin{bmatrix}
		c_1 \\
		c_2
	\end{bmatrix} =
	\begin{bmatrix}
	      c_1 \\
	      c_1+c_2
	\end{bmatrix} =
	 1
	\begin{bmatrix}
		c_1 \\
		c_2
	\end{bmatrix}
	\Rightarrow c_1 = 0
\end{align*}
So, the eigenspace corresponding to eigenvalue 1 of $A$ is 1-dimensional, with a single unit (normalized) vector $\begin{bmatrix}0 \\ 1\end{bmatrix}=\ket{1}$. The only possible diagonal representation of $A$ would then be $A = \ket{1}\bra{1}$, but this equality does not hold.  We conclude that $A$ has no diagonal representation and is not diagonalizable.

\Textbf{2.13} If $\ket{w}$ and $\ket{v}$ are any two vectors, show that $\bigl(\ket{w}\bra{v}\bigr)^\dagger = \ket{v}\bra{w}$.
\Soln We show that $\ket{v}\bra{w}$ has the defining property of $\bigl(\ket{w}\bra{v}\bigr)^\dagger$, \textit{i.e.} if $\ket{\psi},\ \ket{\phi}$ are arbitrary vectors in $V$, then $\Bigl(\ket{\psi}, \bigl(\ket{w}\bra{v}\bigr)\ket{\phi}\Bigr) = \Bigl(\bigl(\ket{v}\bra{w}\bigr)\ket{\psi}, \ket{\phi}\Bigr)$.  We do so by expanding $\Bigl(\ket{\psi}, \bigl(\ket{w}\bra{v}\bigr)\ket{\phi}\Bigr)^*$ in two different ways. 
\begin{align*}
	\Bigl(\ket{\psi},\ \bigl(\ket{w}\bra{v}\bigr) \ket{\phi}\Bigr)^* &=
	\Bigl(\bigl(\ket{w}\bra{v}\bigr)^\dagger \ket{\psi},\  \ket{\phi}\Bigr)^* \tag{definition of $^\dagger$}\\
	&= \Bigl(\ket{\phi},\bigl(\ket{w}\bra{v}\bigr)^\dagger \ket{\psi} \Bigr) \tag{conjugate symmetry}
\end{align*}
On the other hand,
\begin{align*}
	\Bigl(\ket{\psi},\bigl(\ket{w}\bra{v}\bigr) \ket{\phi}\Bigr)^*
	&= \bigl(\braket{\psi | w}, \braket{v | \phi}\bigr)^* \tag{associativity of $\bra{\cdot},\ket{\cdot}, \braket{\cdot|\cdot}$, and $\ket{\cdot}\bra{\cdot}$}\\
	&= \bigl(\braket{\phi | v}, \braket{w | \psi}\bigr)^* \tag{conjugate symmetry}\\
	&= \Bigl(\bra{\phi}, \bigl(\ket{v}\bra{w}\bigr)\ket{\psi}\Bigr). \tag{notation}
\end{align*}
Thus
\begin{align*}
	\Bigl(\ket{\phi},\bigl(\ket{w}\bra{v}\bigr)^\dagger \ket{\psi} \Bigr) = \Bigl(\bra{\phi}, \bigl(\ket{v}\bra{w}\bigr)\ket{\psi}\Bigr) \text{ for arbitrary vectors } \ket{\psi},\ \ket{\phi}
\end{align*}
We conclude that $\bigl(\ket{w}\bra{v}\bigr)^\dagger$ and $\ket{v}\bra{w}$ are \underline{the same operator}, so $\bigl(\ket{w}\bra{v}\bigr)^\dagger = \ket{v}\bra{w}$.

\Textbf{2.14} Anti-linearity of the adjoint: Show that the adjoint operation is anti-linear,
\begin{align*}\left(\sum_ia_iA_i\right)^\dagger = \sum_ia_i^*A_i^\dagger\end{align*}
\Soln It is tempting to assume that $\left(\sum_ia_iA_i\right)^\dagger = \sum_i(a_iA_i)^\dagger$, \textit{i.e.} that the $^\dagger$ transformation is additive, but we don't yet know this. It will follow from the fact that $A^\dagger \equiv (A^*)^T$ given after problem 2.15, and that both $^*$ and $^T$ are linear. This itself is not hard to prove by observing that $(A^*)^T$ has the defining property of $A^\dagger$, making use of the matrix formulation of the inner product.  Without the assumption though, we must be careful to carry around the full sums until additivity (and in-fact full linearity) is known.

\begin{align*}
	\left(\left(\sum_ia_i A_i\right)^\dagger \ket{\phi},\ \ket{\psi} \right)
	&= \left(\ket{\phi},\left(\sum_ia_i A_i\right) \ket{\psi}\right) \tag{definition of $\dagger$}\\
	&= \left(\ket{\phi}, \sum_ia_iA_i\ket{\psi}\right) \tag{distributivity of matrix multiplication}\\
	&= \sum_ia_i \bigl(\ket{\phi},\ A_i \ket{\psi}\bigr) \tag{linearity in the second argument}\\
	&= \sum_ia_i \left(A_i^\dagger \ket{\phi},\ \ket{\psi}\right) \tag{definition of $\dagger$}\\
	&= \sum_i\left(a_i^* A_i^\dagger \ket{\phi},\ \ket{\psi}\right) \tag{conjugate-linearity in the first argument}\\
	&= \left(\left(\sum_ia_i^* A_i^\dagger\right) \ket{\phi},\ \ket{\psi}\right) \tag{distributivity of matrix multiplication}\\
%
	\text{therefore } \left(\sum_ia_i A_i\right)^\dagger &= \sum_ia_i^* A_i^\dagger \tag*{$\square$}
\end{align*}

\Textbf{2.15} Show that $\left(A^\dagger\right)^\dagger = A$.
\Soln We show that $A$ has the defining property of the adjoint of $A^\dagger$. 
\begin{align*}
	\left(\left(A^\dagger\right)^\dagger\ket{\psi},\ \ket{\phi} \right)
	&= \left(\ket{\psi},\ A^\dagger \ket{\phi}\right) \tag*{$\left(\text{definition of } \left(A^\dagger\right)^\dagger\right)$}\\
	&= \left(A^\dagger \ket{\phi},\ \ket{\psi}\right)^* \tag{conjugate symmetry}\\
	&= (\ket{\phi},\ A\ket{\psi})^* \tag{definition of $A^\dagger$}\\
	&= (A\ket{\psi},\ \ket{\phi}) \tag{conjugate symmetry}\\
	\text{therefore } \left(A^\dagger\right)^\dagger &= A \tag*{$\square$}
\end{align*}

\Textbf{2.16} Show that any projector $P$ satisfies the equation $P^2 = P$.
\begin{align*}
	P &= \sum_i \ket{i}\bra{i}. \tag{definition}\\
	P^2 &= \left(\sum_i \ket{i}\bra{i}\right) \left(\sum_j \ket{j}\bra{j}\right) \tag{square definition}\\
	&= \sum_{i,j} \ket{i}\braket{i | j}\bra{j} \tag{distributivity}\\
	&= \sum_{i,j} \ket{i}\bra{j} \delta_{ij} \tag*{$\bigl(\text{evaluate} \braket{i | j}\bigr)$}\\
	&= \sum_i \ket{i}\bra{i}\tag{evaluate sum over $j$}\\
	&= P \tag{definition}
\end{align*}

\Textbf{2.17} Show that a normal matrix is Hermitian if and only if it has real eigenvalues.
\begin{proof}
	($\Rightarrow$) Suppose $A$ is Hermitian. Then $A=A^\dagger$.
	Let $\lambda$ be an eigenvalue of $A$ with unit-eigenvector $\ket{\lambda}$.  We have:
	\begin{align*}
		A \ket{\lambda} &= \lambda \ket{\lambda} \tag{definition} \\
		\braket{\lambda | A | \lambda} &= \lambda \braket{\lambda | \lambda} \tag*{$\bigl(\text{multiply by } \bra{\lambda}\bigr)$}\\
		& = \lambda. \tag{$\lambda$ is a unit-vector}
	\end{align*}
	Now:
	\begin{align*}
		\lambda^*  &= \braket{\lambda | A | \lambda}^* \tag{conjugate}\\
		&= \bigl(\ket{\lambda} , A\ket{\lambda}\bigl)^* \tag{change notation}\\
		&= \bigl(A\ket{\lambda},\lambda\bigr) \tag{conjugate symmetry} \\
		&= \bigl(A^\dagger\ket{\lambda}, \ket{\lambda}\bigr) \tag{hypothesis} \\
		&= \bigl(\ket{\lambda}, A\ket{\lambda}\bigr) \tag{definition of $^\dagger$} \\
		&= \lambda \tag{from above}
	\end{align*}
	So the eigenvalue $\lambda$ is real, since only real numbers are equal to their conjugates.
	
	\noindent($\Leftarrow$) To prove the converse we make use of the spectral decomposition theorem. It's proof does \textit{not} use the fact that a normal matrix is Hermitian if and only if it's eigenvalues are real, so using it here does not make this proof circular.  Suppose the eigenvalues of $A$ are real. From the spectral decomposition theorem there exists a set of eigenvalues $\lambda_i$ and a corresponding orthonormal basis $\ket{\lambda_i}$ such that
	\begin{align}
		A = \sum_i \lambda_i \kb{\lambda_i} \tag{spectral decomposition}
	\end{align}
From this we have:
	\begin{align*}
		A^\dagger &= \left(\sum_i \lambda_i\kb{\lambda_i}\right)^\dagger \tag{apply adjoint}\\
	&= \sum_i\lambda_i^*\bigl(\kb{\lambda_i}\bigr)^\dagger \tag{anti-linearity}\\
	&= \sum_i \lambda_i \kb{\lambda_i} \tag{$\lambda_i$ real, projectors are Hermitian}\\
								&= A \tag{from spectral decomposition}
	\end{align*}
	Thus $A$ is Hermitian.
\end{proof}

\Textbf{2.18} Show that all eigenvalues of a unitary matrix have modulus 1, that is, can be written in the form $e^{i\theta}$ for some real $\theta$.
\Soln
Suppose $\lambda$ is an eigenvalue with corresponding unit-eigenvector $\ket{v}$
\begin{align*}
	1 &= \braket{v | v} \tag{$\ket{v}$ is a unit vector}\\
	&= \braket{v | I | v} \tag{multiply by identity}\\
	&= \braket{v | U^\dagger U  | v} \tag{$U$ is unitary}\\
	&= \bigl(\bra{v}U^\dagger\bigr)\bigl(U\ket{v}\bigr) \tag{associativity of matrix multiplication}\\
	&= \bigl(U\ket{v}\bigr)^\dagger\bigl(U\ket{v}\bigr) \tag{arithmetic properties of $^\dagger$}\\
	&= \bigl(\lambda\ket{v}\bigr)^\dagger\bigl(\lambda\ket{v}\bigr) \tag{$\ket{v}$ is an eigenvector} \\
	&= \lambda^*\lambda\braket{v | v} \tag{re-apply $^\dagger$ and simplify} \\
	&= \norm{\lambda}^2 \tag{definition of $\norm{\cdot}$, $\ket{v}$ is a unit-vector}
\end{align*}
Now $\norm{\lambda} = 1$, and all complex numbers with modulus 1 are located on the unit-circle in $\mathbb{C}$ and can be expressed as $e^{i\theta}$ for some real $\theta \bigl(\in[0,2\pi)\bigr)$

\Textbf{2.19} Show that the Pauli matrices are Hermitian and unitary
\Soln
It is easy to see that the Pauli matrices are Hermitian (self-adjoint) given the conjugate-transpose formula.  We still must show that their squares are the identity:
\begin{align*}
	X^\dagger X = X^2 = \begin{bmatrix}
		0 & 1 \\
		1 & 0
	\end{bmatrix}
	\begin{bmatrix}
		0 & 1 \\
		1 & 0
	\end{bmatrix}
	= \begin{bmatrix}
		1 & 0 \\
		0 & 1
	\end{bmatrix} = I
\end{align*}

\begin{align*}
	Y^\dagger Y = Y^2 = \begin{bmatrix}
		0 & -i \\
		i & 0
	\end{bmatrix}
	\begin{bmatrix}
		0 & -i \\
		i & 0
	\end{bmatrix}
	= \begin{bmatrix}
		-i^2 & 0 \\
		0 & -i^2
	\end{bmatrix} 
	= \begin{bmatrix}
		1 & 0 \\
		0 & 1
	\end{bmatrix} = I
\end{align*}

\begin{align*}
	Z^\dagger Z = Z^2 = \begin{bmatrix}
		1 & 0 \\
		0 & -1
	\end{bmatrix}
	\begin{bmatrix}
		1 & 0 \\
		0 & -1
	\end{bmatrix}
	= \begin{bmatrix}
		1 & 0 \\
		0 & (-1)^2
	\end{bmatrix}
	= \begin{bmatrix}
		1 & 0 \\
		0 & 1
	\end{bmatrix} = I
\end{align*}

\Textbf{2.20} Suppose $A'$ and $A''$ are matrix representations of an operator $A$ on a vector space $V$ with respect to two different orthonormal bases, $\ket{v_i}$ and $\ket{w_i}$.  Then the elements of $A'$ and $A''$ are $A'_{ij} = \braket{v_i | A | v_j}$ and $A''_{ij} = \braket{w_i | A | w_j}$.  Characterize the relationship between $A'$ and $A''$.
\Soln
\begin{align*}
	U \equiv \sum_i \ket{w_i}\bra{v_i}&,\ \  \ U^\dagger=\sum_j \ket{v_j}\bra{w_j} \tag{construct a unitary operator and its adjoint}\\
	\\
	A_{ij}^{'} &= \braket{v_i | A | v_j} \tag{given} \\
	&= \braket{v_i | UU^\dagger A UU^\dagger | v_j} \tag{$U$ is unitary; $UU^\dagger=I$} \\
	&= \sum_{p,q,r,s} \braket{v_i | w_p} \braket{v_p | v_q} \braket{w_q | A | w_r} \braket{v_r | v_s} \braket{w_s | v_j} \tag{expand $U,U^\dagger$, apply linearity}\\
	&= \sum_{p,q,r,s} \braket{v_i | w_p} \delta_{pq} A_{qr}^{''} \delta_{rs}  \braket{w_s | v_j} \tag{$\ket{v_i}$ is orthonormal, apply given for $A''$}\\
	&= \sum_{p,r}  \braket{v_i | w_p}  \braket{w_r | v_j} A_{pr}^{''} \tag{collect non-zero terms and re-index}
\end{align*}

\Textbf{2.21} Repeat the proof of the spectral decomposition in Box 2.2 for the case when $M$ is Hermitian, simplifying the proof wherever possible.

\noindent\textit{Theorem 2.1} \textbf{(Spectral decomposition)} A \textit{Hermitian} operator $M$ on a vector space $V$ is diagonal with respect to some orthonormal basis for $V$. 
\begin{proof} We induct on the dimension of $V$, as in the boxed proof.  Let $\lambda$ be an eigenvalue of $M$, $P$ be the projector onto the $\lambda$ eigenspace, and $Q$ the projector onto the orthogonal complement.
\begin{align*}
	M &= IMI \tag{trivial}\\
		&= (P+Q) M (P+Q)\tag{definition of $Q$}\\
		&= PMP + QMP + PMQ + QMQ\tag{expand}\\
\end{align*}
Now $PMP = \lambda P$ and $QMP = 0$ as before. To show that $PMQ = 0$ is as easy as substituting $M^\dagger$: 
\begin{align*}
PMQ &= PM^\dagger Q \tag{$M$ is Hermitian}\\
&= P(M^{*T}Q) \tag{$^{\dagger}=^{*T}$}\\
&=(QM^*P)^{T} \tag{properties of $^T$}\\
&=((QMP)^*)^T \tag{properties of $^*$}\\
&= 0 \tag{$QMP = 0$}
\end{align*}
Thus $M = PMP + QMQ$.
Next, we prove $QMQ$ is normal.
\begin{align*}
	QMQ (QMQ)^\dagger &= QMQ Q^\dagger M^\dagger Q^\dagger \tag{properties of $^\dagger$, and symmetry}\\
		&= QMQQM^\dagger Q \tag{projectors are Hermitian} \\
		&= QM^\dagger Q QMQ \tag{$M = M^\dagger$}\\
		&=Q^\dagger M^\dagger Q^\dagger QMQ \tag{projectors are Hermitian} \\
		&= (QMQ)^\dagger QMQ  \tag{properties of $^\dagger$, and symmetry}
\end{align*}
Therefore $QMQ$ is normal.
By induction, $QMQ$ is diagonal.  The rest follows Box 2.2 identically.
\end{proof}

\Textbf{2.22} Prove that two eigenvectors of a Hermitian operator with different eigenvalues are necessarily orthogonal
\Soln
Suppose $A$ is a Hermitian operator and $\ket{v_1}, \ket{v_2}$ are eigenvectors of $A$ with eigenvalues $\lambda_1, \lambda_2$, with $\lambda_1\neq\lambda_2$.
Then
\begin{align*}
	\braket{v_1 | A | v_2} = \lambda_2 \braket{v_1 | v_2}. \tag{definition of $v_1$, linearity of $\braket{\cdot |\cdot}$}
\end{align*}

On the other hand,
\begin{align*}
	\braket{v_1 | A | v_2} &= \braket{v_1 | A^\dagger | v_2} \tag{$A$ is Hermitian}\\ 
	&= \braket{v_2 | A | v_1}^* \tag{properties of $^\dagger$, Hermitian $\Rightarrow$ self-transpose} \\
	&= \lambda_1\braket{v_2 | v_1}^* \tag{definition of $v_1$, linearity of $\braket{\cdot |\cdot}$}\\
	&=  \lambda_1\braket{v_1 | v_2} \tag{properties of $^*$}
\end{align*}

Thus
\begin{align*}
	(\lambda_1 - \lambda_2) \braket{v_1 | v_2}  = 0.
\end{align*}
Since $\lambda_1 - \lambda_2 \neq 0$, we must have $\braket{v_1 | v_2}  = 0$, so $v_1$ and $v_2$ are orthogonal.

\Textbf{2.23} Show that the eigenvalues of a projector $P$  are either 0 or 1.
\Soln Suppose $P$ is projector and $\ket{v}$ is an eigenvector of $P$ with eigenvalue $\lambda$.  By exercise 2.16, $P^2 = P$.  We have $P\ket{v} = \lambda \ket{v}$ by hypothesis.  Alternatively,
\begin{align*}
	P \ket{v} &= P^2 \ket{\lambda} \tag{exercise 2.16} \\
 	&= \lambda  P \ket{v} \tag{hypothesis, linearity} \\
	&= \lambda^2 \ket{v} \tag{hypothesis}
\end{align*}
Therefore
\begin{align*}
	\lambda = \lambda^2\\
	\lambda^2-\lambda = 0 \\
	\lambda (\lambda - 1) = 0\\
	\lambda = 0 \text{ or } 1.
\end{align*}

\Textbf{2.24} \textbf{(Hermiticity of positive operators)} Show that a positive operator is necessarily Hermitian.
\Soln Let $A$ be a positive operator, that is, suppose $\braket{v | A | v}$ is real and $\geq 0$ for all $\ket{v}$.  Define $B =\frac{A + A^\dagger}{2}$ and $ C = \frac{A - A^\dagger}{2i}$.  Simple complex arithmetic will show that $A = B+iC$.  $B$ is clearly Hermitian by commutativity of operator addition. $C$ is also Hermitian by linearity of the adjoint, noting that $\left(\frac{1}{2i}\right)^* = -\frac{1}{2i}$.  There are two ways to proceed: one heuristic, and one mathematically rigorous.  We'll start with a heuristic outline of the proof, then provide some mathematically rigorous detail after the fact. 

Let $v$ be a vector and note that it can be proven (below) that $\braket{v | B | v}$ and $\braket{v | C | v}$  are both real numbers.    Now $\braket{v|A|v} = \braket{v|B+iC|v} = \braket{v|B|v} + i\braket{v|C|v}$ by the construction of $B$ and $C$, and the linearity of $\braket{\cdot|\cdot|\cdot}$.  By hypothesis, $\braket{v|A|v}$ is a non-negative real number, so $\braket{v|C|v} = 0$, since both $\braket{v | B | v}$ and $\braket{v | C | v}$  are real.  This will be enough to show that $C=0$ which yields $A=A^\dagger$ by the definition of $C$, that is, $A$ is Hermitian.

Now, to complete the proof, we need to rigorously show that both $\braket{v | B | v}$ and $\braket{v | C | v}$ are real numbers, and that if $\braket{v|C|v}=0$ for all $\ket{v}$, then $C = 0$. 
Let $W$ be Hermitian, thus normal, and note that by exercise 2.17, $W$ has real eigenvalues, say $\omega_i$.  By the spectral decomposition theorem there is an orthonormal basis, say $\ket{w_i}$, such that $W=\sum_i \omega_i\ket{w_i}\bra{w_i}$.  Let $\ket{v}$ be an arbitrary vector, expressed in the orthonormal $\ket{w_i}$-basis as $\sum_i \alpha_i\ket{w_i}$.
\begin{align*}
	\braket{v | W | v}  &= \left\langle\sum_j\alpha_j\ket{w_j} \Big\lvert  W \Big\rvert \sum_i\alpha_i\ket{w_i}\right\rangle \tag{by construction}\\
		&= \sum_i \sum_j \alpha_i\alpha_j^*\braket{w_j|W|w_i} \tag{(conjugate) linearity of $\braket{\cdot|\cdot|\cdot}$} \\
		&= \sum_i\sum_j \alpha_i\alpha_j^*\left\langle w_j\Big\lvert\sum_k \omega_k\ket{w_k}\bra{w_k}\Big\rvert w_i\right\rangle  \tag{spectral decomposition}\\
		&= \sum_i\sum_j\sum_k\alpha_i\alpha_j^*\omega_k\braket{w_j|w_k}\braket{w_k|w_i} \tag{linearity of $\braket{\cdot|\cdot|\cdot}$} \\
		&= \sum_i\sum_j\sum_k\alpha_i\alpha_j^*\omega_k\delta_{jk}\delta_{ki} \tag{orthonormality of the $\ket{w_i}$  basis}  \\
		&= \sum_k\alpha_k\alpha_k^*\omega_k \tag{collecting non-zero terms}\\
		&= \sum_k\norm{\alpha_k}^2\omega_k \tag{definition of $\norm{\cdot}$}
\end{align*}
The $\omega_k$ are real numbers by exercise 2.17, and the $\norm{\alpha_k}^2$ are real by the definition of $\norm{\cdot}$, so $\braket{v|W|v|}$ is a sum of real number,  and hence also real itself.  Applying this to $B$ and $C$ above completes the first missing part.  To finally complete the proof we'll require Theorem \ref{diagonalzero} below, more generally applicable to linear operators on complex vector spaces, without the assumption of Hermiticity.  The proof follows an MIT 8.05 Quantum Physics II lecture note  by Prof. Barton Zwiebach (\url{https://ocw.mit.edu/courses/physics/8-05-quantum-physics-ii-fall-2013/lecture-notes/MIT8_05F13_Chap_03.pdf})

\begin{prop} \label{fullzero}
	Let $T$ be a linear operator on a complex vector space $V$. If $\braket{u | T | v } = 0$ for all $\ket{u}, \ket{v} \in V$, then $T = 0$.
\end{prop}
\begin{proof}
	Let $\ket{u} = T\ket{v}$. Then $\Bigl\langle T\ket{v} \Big\vert T | v \Bigr\rangle = \Bigl\langle T\ket{v} \Big\vert  T\ket{v} \Bigr\rangle = \norm{T\ket{v}}^2 = 0$, which implies $T\ket{v} = 0$ for all $v$ by property 3 of the inner product (page 65).  $T$ is identically 0, so is the zero operator, \text{i.e.} $T = 0$.
\end{proof}
\begin{thm}\label{diagonalzero}
	Let $T$ be a linear operator on a complex vector space $V$. If $\braket{v | T | v } = 0$ for all $\ket{v} \in V$, then $T = 0$.
\end{thm}
\begin{proof}
Note that the weakened hypothesis doesn't directly apply if $\ket{u} \neq \ket{v}$.  We show that the ``off-diagonal'', distinct vector hypothesis of Proposition \ref{fullzero} can be derived from the weakened ``diagonal'' hypothesis' of this theorem, that is, if $\braket{v | T | v } = 0$ for all $\ket{v}$, then $\braket{u | T | v } = 0$ for all $\ket{u},\ket{v}$. Then apply proposition \ref{fullzero}

Suppose $\ket{u}, \ket{v} \in V$. Then note that by ``foiling'' the $\braket{\cdot|\cdot|\cdot}$'s, we can show a ``polarization'' identity, expressing $\braket{u|T|v}$ as follows
	\begin{align*}
		\frac{1}{4} \Bigl( \bigl\langle u+v|T|u+v\bigr\rangle - \bigl\langle u-v|T|u-v\bigr\rangle + \frac{1}{i}\bigl\langle u+iv|T|u+iv\bigr\rangle -\frac{1}{i} \bigl\langle u-iv|T|u-iv\bigr\rangle \Bigr) = \\
		\frac{1}{4}\Bigl( \bigl(\braket{u|T|u} + \braket{u|T|v} + \braket{v|T|u} + \braket{v|T|v}\bigr) - \bigl(\braket{u|T|u} - \braket{u|T|v} - \braket{v|T|u} + \braket{v|T|v}\bigr) + \dots \\
		  \frac{1}{i}\bigl(\braket{u|T|u}+ i\braket{u|T|v}- i\braket{v|T|u}+ \braket{v|T|v}\bigr) -  \frac{1}{i}\bigl(\braket{u|T|u}- i\braket{u|T|v}+ i\braket{v|T|u} + \braket{v|T|v}\bigr)\Bigr) = \\
		\frac{1}{4}\bigr(0\braket{u|T|u} + 4\braket{u|T|v} + 0 \braket{v|T|u} + 0\braket{v|T|v}\bigl) = \\
		\braket{u|T|v} 
	\end{align*}
	Applying the diagonal hypothesis to $\ket{u+v}, \ket{u-v}, \ket{u+iv}$, and $\ket{u-iv}$ in the first expression above gives that $\braket{u|T|v} = 0$ for all $\ket{u},\ket{v}$, hence by Proposition \ref{fullzero}, $T = 0$.
\end{proof}
Applying Theorem \ref{diagonalzero} to $C$ from above finally completes the proof of the Hermiticity of positive operators.

\Textbf{2.25} Show that for any operator $A$, $A^\dagger A$ is positive.
\Soln Its enough to show that $\braket{v | A^\dagger A|v} \geq 0$ for all $v$, but note that $\braket{v| A^\dagger A|v} = \norm{Av}^2$, which is non-negative, so $A^\dagger A$ is a positive operator.

\Textbf{2.26} Let $\ket{\psi}=(\ket{0}+\ket{1})/\sqrt{2}(=\ket{+})$. Write out $\ket{\psi}^{\otimes2}$ and $\ket{\psi}^{\otimes3}$ explicitly, both in terms of tensor products like $\ket{0},\ket{1}$, and using the Kronecker product.
\Soln
\begin{align*}
	\ket{\psi}^{\otimes 2} &= \frac{1}{\sqrt{2}} (\ket{0} + \ket{1}) \otimes \frac{1}{\sqrt{2}} (\ket{0} + \ket{1}\\
		&= \frac{1}{2} (\ket{00}  + \ket{01} + \ket{10} + \ket{11}  )\\
		&= \frac{1}{2} \begin{bmatrix}
			1 \\
			1 \\
			1 \\
			1
		\end{bmatrix}
\end{align*}
\begin{align*}
	\ket{\psi}^{\otimes 3} &= \frac{1}{\sqrt{2}} (\ket{0} + \ket{1}) \otimes \frac{1}{\sqrt{2}} (\ket{0} + \ket{1}  \otimes \frac{1}{\sqrt{2}} (\ket{0} + \ket{1}\\
		&= \frac{1}{2\sqrt{2}} (\ket{000}  + \ket{001} + \ket{010} + \ket{011} +  \ket{100}  + \ket{101} + \ket{110} + \ket{111})\\
		&= \frac{1}{2\sqrt{2}} \begin{bmatrix}
			1 \\
			1 \\
			1 \\
			1 \\
			1 \\
			1 \\
			1 \\
			1
		\end{bmatrix}
\end{align*}

\Textbf{2.27} Calculate the matrix representations of the tensor products of the Pauli operators (a) $X$ and $Z$; (b) $I$ and $X$; (c) $X$ and $I$. Is the tensor product commutative?
\Soln
\begin{align*}
	X \otimes Z &= \begin{bmatrix}
		0 & 1 \\
		1 & 0
	\end{bmatrix}
	\otimes
	\begin{bmatrix}
		1 & 0 \\
		0 & -1
	\end{bmatrix} \\
	&= \begin{bmatrix}
		0\begin{bmatrix}
			1 & 0 \\
			0 & -1
			\end{bmatrix}
		&
		1\begin{bmatrix}
			1 & 0 \\
			0 & -1
			\end{bmatrix}
		\\ \\
		1\begin{bmatrix}
			1 & 0 \\
			0 & -1
			\end{bmatrix}
		&
		0\begin{bmatrix}
			1 & 0 \\
			0 & -1
			\end{bmatrix}
	\end{bmatrix} \\
	&= \begin{bmatrix}
		0 & 0 & 1 & 0 \\
		0 & 0 & 0 & -1 \\
		1 & 0 & 0 & 0 \\
		0 & -1 & 0 & 0
	\end{bmatrix}
\end{align*}
\begin{align*}
	I \otimes X &= \begin{bmatrix}
		1 & 0 \\
		0 & 1
	\end{bmatrix}
	\otimes
	\begin{bmatrix}
		0 & 1 \\
		1 & 0
	\end{bmatrix}\\
	&=\begin{bmatrix}
		1\begin{bmatrix}
		0 & 1 \\
		1 & 0
		\end{bmatrix}
		&
		0\begin{bmatrix}
		0 & 1 \\
		1 & 0
		\end{bmatrix}
		\\ \\
		0\begin{bmatrix}
		0 & 1 \\
		1 & 0
		\end{bmatrix}
		&
		1\begin{bmatrix}
		0 & 1 \\
		1 & 0
		\end{bmatrix}
	\end{bmatrix} \\
	&=
	\begin{bmatrix}
	0 & 1 & 0 & 0 \\
	1 & 0 & 0 & 0 \\
	0 & 0 & 0 & 1 \\
	0 & 0 & 1 & 0
	\end{bmatrix}
\end{align*}
\begin{align*}
	X \otimes I &= \begin{bmatrix}
		0 & 1 \\
		1 & 0
	\end{bmatrix}
	\otimes
	\begin{bmatrix}
		1 & 0 \\
		0 & 1
	\end{bmatrix}\\
	&=\begin{bmatrix}
		0\begin{bmatrix}
			1 & 0 \\
			0 & 1
		\end{bmatrix}
		&
		1\begin{bmatrix}
			1 & 0 \\
			0 & 1
		\end{bmatrix}
		\\ \\
		0\begin{bmatrix}
			1 & 0 \\
			0 & 1
		\end{bmatrix}
		&
		1\begin{bmatrix}
			1 & 0 \\
			0 & 1
		\end{bmatrix}
	\end{bmatrix} \\
	&= \begin{bmatrix}
	0 & 0 & 1 & 0 \\
	0 & 0 & 0 & 1 \\
	1 & 0 & 0 & 0 \\
	0 & 1 & 0 & 0
	\end{bmatrix}
\end{align*}
In general, the tensor product is not commutative.

\Textbf{2.28} Show that the transpose, complex conjugation, and adjoint operations distribute over the tensor prodoct,
\begin{align*}(A\otimes B)^* = A^*\otimes B^*; (A\otimes B)^T = A^T\otimes B^T; (A\otimes B)^\dagger = A^\dagger \otimes B^\dagger\end{align*}
\Soln Let $A$ be $n_1\times m_1$ and $B$ be $n_2\times m_2$, so that $A\otimes B$ is $n\times m$, where $n=n_1n_2$ and $m=m_1m_2$.  The entries in $A\otimes B$ are products of a single entry in $A$ and a single entry in $B$.  Specifically, if $i = i_1n_1+i_2$ and $j = j_1m_1+j_2$, with $0\leq i_2 < n_1$ and $0\leq j_2 <  m_1$, then $(A\otimes B)_{ij}= A_{i_1j_1}B_{i_2j_2}$.

\begin{align*}
(A\otimes B)^* &= \left[ A_{i_1j_1}B_{i_2j_2}\right]^* \tag{from above} \\
&= \left[ A_{i_1j_1}^*B_{i_2j_2}^*\right] \tag{piecewise conjugation} \\
&= A^*\otimes B^*\tag{consistent indexing}
\end{align*}
%
%\begin{align*}
%	(A \otimes B)^*
%
%	\begin{comment}
%	\begin{bmatrix}
%		A_{11} B & \cdots & A_{1n} B \\
%		\vdots & \ddots  & \vdots \\
%		A_{m1}B & \cdots & A_{mn} B
%	\end{bmatrix}^* \\
%	&=
%	\begin{bmatrix}
%		A_{11}^* B^* & \cdots & A_{1n}^* B^* \\
%		\vdots & \ddots  & \vdots \\
%		A_{m1}^* B^* & \cdots & A_{mn}^* B^*
%	\end{bmatrix} \\
%	&= A^* \otimes B^*.
%	\end{comment}
%\end{align*}
%
%
%
%	(A\otimes B)^T &= \left[ A_{i_1j_1}B_{i_2j_2}\right]^T \tag{from above} \\
%	&= \left[ A_{j_1i_1}B_{j_2i_2}\right] \tag{definition of $^T$ exchanges $i$ and $j$}
%\end{align*}
To see that $(A\otimes B)^T = A^T\otimes B^T$, note that $A^T\otimes B^T$ is $m\times n$, and $(A^T\otimes B^T)_{kl}$ is the product of a single entry in $A^T$ and a single entry in $B^T$.  Specifically, if $k = k_1m_1+k_2$ and $\ell=\ell_1n_1+\ell_2$, with $0\leq k_2 < m_2$ and $0\leq \ell_2 < n_2$, then $(A^T\otimes B^T)_{k\ell} = (A^T)_{k_1\ell_1}(B^T)_{k_2\ell_2} = A_{\ell_1k_1}B_{\ell_2k_2}$.  Now, the hypotheses on $k$ match the hypotheses on $j$ above, and similarly for $\ell$ and $i$.  This implies $(A^T\otimes B^T)_{k\ell} = (A\otimes B)_{\ell k} = (A\otimes B)^T_{k\ell}$.  All entries in $A^T\otimes B^T$ and $ (A\otimes B)^T$ are equal, so $(A\otimes B)^T = A^T\otimes B^T$.

\begin{comment}
\begin{align*}
	(A\otimes B)^T &=
	\begin{bmatrix}
		A_{11} B & \cdots & A_{1n} B \\
		\vdots & \ddots  & \vdots \\
		A_{m1}B & \cdots & A_{mn} B
	\end{bmatrix}^T \\
	&=
	\begin{bmatrix}
		A_{11} B^T & \cdots & A_{m1} B^T \\
		\vdots & \ddots  & \vdots \\
		A_{1n} B^T & \cdots & A_{mn} B^T
	\end{bmatrix} \\
	&=
	\begin{bmatrix}
		A_{11} B^T & \cdots & A_{1m} B^T \\
		\vdots & \ddots  & \vdots \\
		A_{n1} B^T & \cdots & A_{nm} B^T
	\end{bmatrix} \\
	&= A^T \otimes B^T.
\end{align*}
\end{comment}
Distributivity of $^\dagger$ follows by applying distributivity of $^*$ and $^T$ in turn:
\begin{align*}
	(A\otimes B)^\dagger&=((A \otimes B)^*)^T	\tag{definition of $\dagger$}\\
		&= (A^* \otimes B^*)^T \tag{distribute $^*$}\\
		&= (A^*)^T \otimes (B^*)^T \tag{distribute $^T$}\\
		&= A^\dagger \otimes B^\dagger \tag{definition of $^\dagger$}.
\end{align*}

\Textbf{2.29} Show that the tensor product of two unitary operators is unitary
\Soln
Suppose $U_1$ and $U_2$ are unitary operators. To avoid implicit assumptions on multiplication of tensor products, let $\ket{v}$ and $\ket{w}$ be vectors in the spaces on which $U_1$ and $U_2$ operate.  Then:
\begin{align*}
	(U_1 \otimes U_2) (U_1 \otimes U_2)^\dagger(\ket{v}\otimes\ket{w}) &= (U_1 \otimes U_2) ( U_1^\dagger \otimes  U_2^\dagger)(\ket{v}\otimes\ket{w})\tag{distributivity of $^\dagger$} \\
	&=(U_1 \otimes U_2)(U_1^\dagger\ket{v}\otimes U_2^\dagger\ket{w}) \tag{definition of tensor product of operators} \\
	&=U_1U_1^\dagger\ket{v}\otimes U_2U_2^\dagger\ket{w} \tag{definition of tensor product of operators} \\
	&= I\ket{v} \otimes I\ket{w} \tag{$U_1$ and $U_2$ are unitary} \\
	&= (I\otimes I)(\ket{v}\otimes\ket{w}) \tag{definition of tensor product of operators} \\
	&= I(\ket{v}\otimes\ket{w}) \tag{$I\otimes I = I$ by construction}
\end{align*}
So, $(U_1\otimes U_2)(U_1\otimes U_2)^\dagger = I$. Similarly, $(U_1 \otimes U_2)^\dagger (U_1 \otimes U_2)  = I \otimes I =  I$, so $U_1\otimes U_2$ is unitary.

\Textbf{2.30} Show that the tensor product of two Hermitian operators is Hermitian.
\Soln
Suppose $A$ and $B$ are Hermitian operators. Then by distributivity of $^\dagger$ and Hermiticity:
\begin{align*}
(A \otimes B)^\dagger = A^\dagger \otimes B^\dagger = A \otimes B.
\end{align*}
Thus $A \otimes B$ is Hermitian.

\Textbf{2.31} Show that the tensor product of two positive operators is positive.
\Soln
Suppose $A$ and $B$ are positive operators.  Then
\begin{align*}
	\Bigl( \ket{\psi} \otimes \ket{\phi} , (A \otimes B) (\ket{\psi} \otimes \ket{\phi})\Bigr) &= \Bigl(\ket{\psi}\otimes\ket{\phi},  A\ket{\psi}\otimes B\ket{\phi}\Bigr) \tag{definition of $A\otimes B$} \\
	&= (\ket{\psi}\otimes\ket{\phi})^\dagger(A\ket{\psi}\otimes B\ket{\phi}) \tag{definition of inner-product}\\
	&=(\bra{\psi}\otimes\bra{\phi})(A\ket{\psi}\otimes B\ket{\phi}) \tag{distributivity of $^\dagger$} \\
	&= \braket{\psi |A| \psi} \braket{\phi | B | \phi}.
\end{align*}
Since $A$ and $B$ are positive operators, $\braket{\psi |A| \psi} \geq 0$ and $\braket{\phi | B | \phi} \geq 0$ for all $\ket{\psi}, \ket{\phi}$, so $\braket{\psi |A| \psi} \braket{\phi | B | \phi} \geq 0$, from which we conclude that $A \otimes B$ is positive.

\Textbf{2.32} Show that the tensor product of two projectors is a projector.
\Soln
Suppose $P_1$ and $P_2$  are projectors. It is tempting to think that by applying exercise 2.16, which yields
\begin{align*}
	(P_1 \otimes P_2) ^2 &= P_1^2 \otimes P_2^2 \tag{tensor product is multiplicative}\\
		&= P_1 \otimes P_2 \tag{exercise 2.16},
\end{align*}
exercise 2.16 would then imply that $ P_1 \otimes P_2$ is also projector. However, this implication is the converse of exercise 2.16, which we have not proven.  Instead, we need to prove that if $P_1 = \sum_{i=0}^k\ket{v_i}\bra{v_i}$ and $P_2=\sum_{j=0}^t\ket{w_j}\bra{w_j}$, where $\ket{v_i}_{i=0}^k$ is a subset of an orthonormal basis $\ket{v_i}_{i=0}^{\kappa}$, and $\ket{w_j}_{j=0}^t$ is a subset of an orthonormal basis $\ket{w_j}_{j=0}^\tau$, then $P_1\otimes P_2 = \sum_{q=0}^s \ket{r_q}\bra{r_q}$, where $\ket{r_q}_{q=0}^{s}$ is a subset of an orthonormal basis $\ket{r_q}_{q=0}^\sigma$.  First, the fact that  $P_1\otimes P_2 = \sum\limits_{\substack{0\leq i\leq k\\0\leq j\leq t}} \bigl(\ket{v_i}\bra{v_i}\bigr)\otimes\bigl(\ket{w_j}\bra{w_j}\bigr)$ follows easily from distributivity of operator tensor products, having illustrated how easily that follows from distributivity of tensor products of vectors in exercise 2.29.  We need to show that $\Bigl\{\bigl(\ket{v_i}\bra{v_i}\bigr)\otimes\bigl(\ket{w_j}\bra{w_j}\bigr)\Bigr\}_{\substack{0\leq i\leq k\\0\leq j\leq t}}$ is a subset of an orthonormal basis.  It is automatically a subset of the set of vector tensor products resulting from loosening the restrictions on $i$ and $j$ to $0\leq i \leq \kappa$ and $0\leq j \leq \tau$, which we may assume is a basis, as stated on page 72.  We need only show that the inner product of tensor products is multiplicative so that orthonormality is preserved.  Let $v_1,v_2,w_1$ and $w_2$ be vectors.
\begin{align*}
\braket{v_1\otimes w_1 | v_2\otimes w_2} &= \ket{v_1\otimes w_1}^\dagger \ket{v_2\otimes w_2} \tag{definition of $\braket{\cdot | \cdot}$}\\
&= \bigl(\ket{v_1}^\dagger\otimes\ket{w_1}^\dagger\bigr)\bigl(\ket{v_2}\otimes\ket{w_2}\bigr) \tag{distributivity of $^\dagger$ over $\otimes$} \\
&= \bigl(\ket{v_1}^\dagger\ket{v_2}\bigr)\otimes\bigl(\ket{w_1}^\dagger\ket{w_2}\bigr) \tag{mixed-product property of Kronecker product} \\
&= \braket{v_1|v_2}\otimes\braket{w_1|w_2} \tag{definition of $\braket{\cdot|\cdot}$}\\
&= \braket{v_1|v_2}\braket{w_1|w_2} \tag{$\braket{\cdot|\cdot}$ is a scalar}
\end{align*} 
So, in the basis $\Bigl\{\bigl(\ket{v_i}\bra{v_i}\bigr)\otimes\bigl(\ket{w_j}\bra{w_j}\bigr)\Bigr\}_{\substack{0\leq i\leq \kappa\\0\leq j\leq \tau}}$, the inner product of two vectors $\ket{v_{i_1}}\otimes\ket{w_{j_1}}$ and $\ket{v_{i_2}}\otimes\ket{w_{j_2}}$ is $\braket{v_{i_1}\otimes w_{j_1} | v_{i_2}\otimes w_{j_2}} = \braket{v_{i_1}|v_{i_2}}\braket{w_{j_1}|w_{j_2}}=\delta_{i_1i_2}\delta_{j_1j_2}$, from which it follows that this basis is orthonormal, completing the proof.

\Textbf{2.33} The Hadamard operator on one qubit may be written as $$H=\frac{1}{\sqrt{2}}\Bigl[\bigl(\ket{0}+\ket{1}\bigr)\bra{0}+\bigl(\ket{0}-\ket{1}\bigr)\bra{1}\Bigr].$$  Show explicitly that the Hadamard transform on $n$ qubits, $H^{\otimes n}$, may be written as $$H^{\otimes n} = \frac{1}{\sqrt{2^n}}\sum_{x,y}(-1)^{x\cdot y}\ket{x}\bra{y}.$$ Write out an explicit matrix representation for $H^{\otimes 2}$.
\Soln It is important to note what is meant by $x\cdot y$ in this formula.  Here $\cdot$ does \textbf{not} mean integer multiplication.  It can be taken to mean popparity of the binary AND of $x$ and $y$. Note that this property is multiplicative across dimensions, when 1 is used for even popparity, and -1 for odd.  We proceed by induction on $n$.  Assume the preceding formula for $n-1$.  We must prove the formula for $n$.
\begin{align*}
H^{\otimes n} &= H\otimes H^{\otimes n-1}\tag{notation} \\
&=\left(\frac{1}{\sqrt{2}}\Bigl[\bigl(\ket{0}+\ket{1}\bigr)\bra{0}+\bigl(\ket{0}-\ket{1}\bigr)\bra{1}\Bigr]\right) \otimes\left(\frac{1}{\sqrt{2^{n-1}}}\sum_{x,y}(-1)^{x\cdot y}\ket{x}\bra{y}\right)\tag{hypothesis} \\
&=\frac{1}{\sqrt{2^n}}\sum_{x,y}(-1)^{x\cdot y}\bigl(\ket{0}\bra{0}+\ket{1}\bra{0}+\ket{0}\bra{1}-\ket{1}\bra{1}\bigr) \otimes \ket{x}\bra{y}\tag{rearrange} \\
&=\frac{1}{\sqrt{2^n}}\sum_{x',y'}(-1)^{x'\cdot y'}\ket{x'}\bra{y'} \tag{multiplicativity of $\cdot$ across dimensions}
\end{align*}
Now, explcitly  
\begin{align*}
	H  = \frac{1}{\sqrt{2}} \begin{bmatrix}
		1 & 1 \\
		1 & -1
	\end{bmatrix}
\end{align*}
and
\begin{align*}
	H^{\otimes 2}
	=
	\frac{1}{\sqrt{2}} \begin{bmatrix}
	1 & 1 \\
	1 & -1
	\end{bmatrix}
	\otimes
	\frac{1}{\sqrt{2}} \begin{bmatrix}
	1 & 1 \\
	1 & -1
	\end{bmatrix}
	=
	\frac{1}{2}\begin{bmatrix}
		1\begin{bmatrix}1 & 1 \\
	1 & -1
	\end{bmatrix} & \quad1\begin{bmatrix}1 & 1 \\
	1 & -1
	\end{bmatrix} \\
	1\begin{bmatrix}1 & 1 \\
	1 & -1
	\end{bmatrix} &
	-1\begin{bmatrix}1 & 1 \\
	1 & -1
	\end{bmatrix}
	\end{bmatrix} =
	\frac{1}{2} \begin{bmatrix}
		1 & 1 & 1 & 1 \\
		1 & -1 & 1 & -1 \\
		1 & 1 & -1 & -1 \\
		1 & -1 & -1 & 1
	\end{bmatrix}
\end{align*}

\Textbf{2.34} Find the square root and logarithm of the matrix $$\begin{bmatrix}4&3\\3&4\end{bmatrix}.$$
\Soln
Suppose $A = \begin{bmatrix}
4 & 3 \\
3 & 4
\end{bmatrix} $.  We will need the ``spectral'' decomposition $A$.  First, to find the eigenvalues of $A$:
\begin{align*}
	0=\det (A - \lambda I ) &= (4-\lambda)^2 - 3^2\\
		&= \lambda^2 -8\lambda + 7\\
		&= (\lambda - 1)(\lambda - 7)
\end{align*}
So, the eigenvalues of $A$ are $\lambda = 1$, and $\lambda = 7$.  The corresponding eigenvectors can easily be seen to be
$\begin{bmatrix}
	1 \\
	-1
	\end{bmatrix}
$, corresponding to $\lambda=1$, and $\begin{bmatrix}
1 \\
1
\end{bmatrix}
$, corresponding to $\lambda=7$.  To construct an orthonormal basis from these eigenvectors, we can scale both by $\frac{1}{\sqrt{2}}$, and denote these scaled vectors by $\ket{\lambda=1}=\frac{1}{\sqrt{2}}\begin{bmatrix}
	1 \\
	-1
	\end{bmatrix}$ and $\ket{\lambda=7}=\frac{1}{2}\begin{bmatrix}
	1 \\
	1
	\end{bmatrix}$. Now, seeing as $A$ is real and self-transpose/adjoint, it is a normal matrix/operator, and as such, $A$ can be ``spectrally decomposed''/diagonalized as:
\begin{align*}
	A &= \kb{\lambda = 1} + 7 \kb{\lambda = 7}  \\
	&= \frac{1}{2}\begin{bmatrix}1&-1\\-1&1\end{bmatrix}+7\left(\frac{1}{2}\begin{bmatrix}1&1\\1&1\end{bmatrix}\right).\end{align*}
The square root of $A$ is ``defined'' as:
\begin{align*}
	\sqrt{A} &= \kb{\lambda = 1} + \sqrt{7} \kb{\lambda = 7}\\
		&= \frac{1}{2} \begin{bmatrix}
		1 & -1 \\
		-1 & 1
		\end{bmatrix}
		+
		\frac{\sqrt{7}}{2} \begin{bmatrix}
		1 & 1 \\
		1 & 1
		\end{bmatrix}\\
		&=
		\frac{1}{2}
		 \begin{bmatrix}
			1+\sqrt{7} & -1+\sqrt{7} \\
			-1 + \sqrt{7} & 1+\sqrt{7}
		\end{bmatrix}
\end{align*}
and $log(A)$ is defined as
 \begin{align*}
 	\log (A) &=  \log (1) \kb{\lambda = 1} + \log (7) \kb{\lambda = 7}\\
 		&= \frac{\log (7)}{2} \begin{bmatrix}
	 		1 & 1 \\
	 		1 & 1
 		\end{bmatrix}
 \end{align*}
Note that one would hope that operator functions would respect various properties of the functions, such as inverses. To formally prove such a thing, let us consider the diagonal representation $A=\sum_a a\kb{\lambda=a}$ and the induced definition of $f(A)=\sum_a f(a)\kb{\lambda = a}$.  Note that the $f(a)$ are eigenvalues of $f(A)$, with corresponding eigenvectors $\ket{\lambda=a}$, since the fact that the $\ket{\lambda=a}$ are orthonormal gives $\bigl(\sum_a f(a) \kb{\lambda=a}\bigr)\ket{\lambda=a'} = f(a')\ket{\lambda=a'}$.  So $\sum_a f(a)\kb{\lambda = a}$ is a diagonal representation of $f(a)$.  Now $f^-1(f(A)) = f^{-1}\bigl(\sum_a f(a)\kb{\lambda=a}\bigr) = \sum_a f^{-1}(f(a))\kb{\lambda=a} = \sum_a a \kb{\lambda=a} = A$.

\Textbf{2.35} \textbf{(Exponentiation of Pauli Matrices)} let $\vec{v}$ be any real three-dimensional unit vector and $\theta$ a real number.  Prove that $$\exp (i\theta\vec{v}\cdot\vec{\sigma}) = \cos(\theta)I+i\sin(\theta)\vec{v}\cdot\vec{\sigma},$$ where $\vec{v}\cdot\vec{\sigma} \equiv \sum_{i=1}^3 v_i\sigma_i$.  This exercise is generalized in problem 2.1 on page 117. 
\Soln To find the eigenvalues of $\vec{v}\cdot{\vec{\sigma}}$, we first express in matrix form:
\begin{align*}
	\vec{v} \cdot \vec{\sigma} &= \sum_{i=1}^3 v_i \sigma_i (= v_1\sigma_x + v_2\sigma_y + v_3\sigma_z = v_1X+v_2Y+v_3Z) \tag{definition}\\
		&= v_1 \begin{bmatrix}
		0 & 1 \\
		1 & 0
		\end{bmatrix}
		+ v_2 \begin{bmatrix}
		0 & -i \\
		i & 0
		\end{bmatrix}
		+ v_3 \begin{bmatrix}
		1 & 0 \\
		0 & -1
		\end{bmatrix} \tag{substitute}\\
		&= \begin{bmatrix}
		v_3 & v_1 - i v_2 \\
		v_1 + iv_2 & -v_3
		\end{bmatrix} \tag{collect terms} \\
	0=\det (\vec{v} \cdot \vec{\sigma}  - \lambda I) &= (v_3 - \lambda) (-v_3 - \lambda) - (v_1 - iv_2) (v_1 + iv_2) \tag{characteristic equation}\\
			&= \lambda^2 - (v_1^2 + v_2^2  + v_3^2) \tag{expand and collect $v$'s}\\
			&= \lambda^2 - 1 \tag{$v$ is a unit vector}
\end{align*}
Solving yields that the eigenvalues of $\vec{v}\cdot\vec{\sigma}$ are $\lambda = \pm 1$.
Let $\ket{\lambda_{ \pm 1 } }$ be eigenvectors with eigenvalues $\pm  1$.  Since $\vec{v}$ is a real valued vector, it is easily seen that $\vec{v} \cdot \vec{\sigma}$ is Hermitian, and so is diagonalizable, and we may take the $\ket{\lambda_{\pm 1}}$ to be orthonormal by Exercise 2.22.   In particular, this gives
\begin{align*}
	\kb{\lambda_1} + \kb{\lambda_{-1}} &= I \tag{completeness} \\
	 \kb{\lambda_1} - \kb{\lambda_{-1}} &= \vec{v} \cdot \vec{\sigma} \tag{diagonalization}
\end{align*}
Now
\begin{align*}
	\exp \left(i \theta \vec{v} \cdot \vec{\sigma} \right) &=
	e^{i \theta} \kb{\lambda_1}  + e^{-i \theta} \kb{\lambda_{-1}} \tag{definition}\\
	&= (\cos \theta + i \sin \theta) \kb{\lambda_1} + (\cos \theta - i \sin \theta) \kb{\lambda_{-1}} \tag{Euler's formula}\\
	&= \cos \theta (\kb{\lambda_1} + \kb{\lambda_{-1}}) + i \sin \theta (\kb{\lambda_1} - \kb{\lambda_{-1}}) \tag{group $\cos$ and $\sin$ terms}\\
	&= \cos( \theta) I + i \sin (\theta) \vec{v} \cdot \vec{\sigma}. \tag{from above}
\end{align*}

\Textbf{2.36} Show that the Pauli matrices except for $I$ have trace zero.
\Soln
\begin{align*}
	\Tr (\sigma_1) &= \Tr \left(
		\begin{bmatrix}
		0 & 1 \\
		1 & 0
		\end{bmatrix}
	\right) = 0\\
%
	\Tr (\sigma_2) &= \Tr \left(
		\begin{bmatrix}
		0 & -i \\
		i & 0
		\end{bmatrix}
	\right) = 0\\
%
	\Tr (\sigma_3) &= \Tr \left(
	\begin{bmatrix}
		1 & 0 \\
		0 & -1
	\end{bmatrix}
	\right) = 1 -1 = 0\\
\end{align*}

\Textbf{2.37} \textbf{(Cyclic property of the trace)} If $A$ and $B$ are two linear operators show that $$\Tr(AB)=\Tr(BA).$$
\begin{align*}
	\Tr (AB) &= \sum_i \braket{i | AB | i} \tag{using matrix representation of $AB$}\\
		&=\sum_i \braket{i | A I B | i} \tag{insert $I$}\\
		&= \sum_{i,j} \braket{i | A | j}\braket{j | B | i} \tag{completeness: $I=\sum_j\kb{j}$}\\
		&= \sum_{i,j} \braket{j | B | i} \braket{i | A | j} \tag{commutativity in $\mathbb{C}$}\\
		&= \sum_j \braket{j | BA | j} \tag{completeness: $I=\sum_i\kb{i}$}\\
		&= \Tr (BA) \tag{using matrix representation of $BA$}
\end{align*}

\Textbf{2.38} \textbf{(Linearity of the trace)} If $A$ and $B$ are two linear operators, show that $$\Tr(A+B)=\Tr(A)+\Tr(B)$$ and if $z$ is an arbitrary complex number show that $$\Tr(zA) = z\Tr(A).$$
\Soln
\begin{align*}
	\Tr (A + B) &= \sum_i \braket{i | A+B | i}  \tag{using matrix representation of $A+B$}\\
		&= \sum_i (\braket{i|A|i}  + \braket{i | B | i}  ) \tag{llinearity of $\braket{\cdot|\cdot|\cdot}$}\\
		&= \sum_i \braket{i|A|i} + \sum_i \braket{i|B|i} \tag{separate terms}\\
		&= \Tr (A) + \Tr (B) \tag{using matrix representation of $A$ and $B$}.
\end{align*}
\begin{align*}
	\Tr (z A) &=  \sum_i \braket{i | z A | i} \tag{matrix representation}\\
		&= \sum_i z \braket{i | A | i} \tag{linearity}\\
		&= z \sum_i \braket{i | A | i}\tag{linearity of sum}\\
		&= z \Tr (A) \tag{matrix representation}.
\end{align*}

\Textbf{2.39} \textbf{(The Hilbert-Schmidt inner product on operators)} The set $L_V$ of linear operators on a Hilbert space $V$ is a vector space.  An important additional result is that the vector space $L_V$ can be given a natural inner product structure, turning it into a Hilbert space.\newline
(1) Show that the function $(\cdot,\cdot)$ on $L_V\times L_V$ defined by $$(A, B) \equiv \Tr(A^\dagger B)$$ is an inner product function.  This inner product is known as the \textit{Hilbert-Schmidt} or \textit{trace} inner product.\newline
(2) If $V$ has $d$ dimensions, show that $L_V$ has dimension $d^2$.\newline
(3) Find an orthonormal basis of Hermitian matrices for the Hilbert space $L_V$.
\Soln
(1) We check the three properties of inner products on page 65:\newline
\indent(i) linearity in the second argument:
\begin{align*}
\left(A, \sum_i \lambda_i B_i \right) &= \Tr \left(A^\dagger \left(\sum_i \lambda_i B_i  \right) \right) \tag{definition of $(\cdot,\cdot)$}\\
&= \Tr\left(\sum_i \lambda_i A^\dagger B_i\right) \tag{linearity in $L_V$} \\
&= \sum_i \lambda_i \Tr (A^\dagger B_i) \tag{linearity of $\Tr$ from Exercise 2.38} \\
&= \sum_i \lambda_i (A,B_i) \qed\tag{definition of $(\cdot, \cdot)$}
\end{align*}
\indent(ii) conjugate symmetry:
\begin{align*}
	(A, B)^* &= \left( \Tr (A^\dagger B) \right)^* \tag{definition}\\
		&= \left(\sum_{i,j} \braket{ i | A^\dagger | j} \braket{j | B | i}  \right)^* \tag{matrix representation, insert $I$, apply completeness}\\
		&= \sum_{i,j}  \braket{j | B | i}^* \braket{ i | A^\dagger | j}^*\tag{distributivity and multiplicativity of $^*$, commutativity in $\mathbb{C}$}\\
		&= \sum_{i,j}  \ket{j}^{\dagger*} B^* \ket{i}^* \ket{i}^{\dagger*} A^{\dagger*} \ket{j}^*\tag{distribute conjugation and make $^\dagger$s explicit}\\
		&= \sum_{i,j}   \ket{j}^{T} B^{\dagger T} \ket{i}^{\dagger T} \ket{i}^{T} A^{T} \ket{j}^{\dagger T} \tag{$^{\dagger*} = ^T$, $^*=^{\dagger T}$}\\
		&= \sum_{i,j}   (\ket{i}^{\dagger}B^{\dagger} \ket{j})^T (\ket{j}^{\dagger} A \ket{i})^T \tag{$^T$ is cyclic} \\
		&= \sum_{i,j}   \braket{i|B^{\dagger} |j} \braket{j | A | i} \tag{definition of $\braket{\cdot|\cdot|\cdot}$, transpose in $\mathbb{C}$} \\
		&= \sum_i \braket{i | B^\dagger A | i} \tag{completeness}\\
		&= \Tr (B^\dagger A) = (B, A) \tag{matrix representation, definition of $(\cdot, \cdot)$}\\
\end{align*}
\indent(iii) positivity:
\begin{align*}
	(A, A) &= \Tr (A^\dagger A) \tag{definition}\\
		&= \sum_i \braket{ i | A^\dagger A | i } \tag{matrix representation} \\
		&\geq 0 \tag{$A^\dagger$ A is positive by Exercise 2.25}
\end{align*}
\begin{adjustwidth}{\parindent}{} We are left only show that if $(A, A) = 0$, then $A = 0$. Note that $(A, A) = 0$ implies that $\braket{i | A^\dagger A | i} = 0$ for all $i$, where here, the $\ket{i}$ are an orthonormal \textit{basis} with respect to which the matrix representation of $A$ is constructed.  Note that $A\ket{i}$ is the $i$-th column of $A$, as constructed on page 64.  Also $\norm{A\ket{i}}^2 = \braket{i | A^\dagger A| i} = 0$, so the $i$-th column of $A$ is $0$, for all $i$.  This gives that $(A, A) = 0$ iff $A=0$, completing the proof of positivity.\end{adjustwidth}

\noindent(2) To show that $L_V$ has dimension $d^2$, note that the elements of $L_V$ are linear operators from $V$ to $V$ and thus have a  matrix representation by a $d\times d$ matrix.    The vector space of $d\times d$ matrices over $\mathbb{C}$ is clearly at most $d^2$-dimensional.   An easily constructed basis is the set of matrices populated with all $0$s except for a single 1, where the position of the 1's ranges over all $d^2$ positions. This set is clearly linearly independent and spans $L_V$, from which dimension $d^2$ follows..  More generally, if $\ket{k}_k=0^{d-1}$ is any orthonormal basis for $V$, then $\left\{\ket{k}\bra{\ell}\right\}_{k,\ell}$ is a basis.  The first example is produced used the standard (computational) basis.

\noindent(3)  The obvious choices of basis matrices discussed above are orthonormal, but not Hermitian.  Note that for $d=2$, the Pauli matrices are orthonormal with respect to the Hilbert-Schmidt inner product, and are Hermitian.  Other bases can be constructed using a ``discrete Weyl system''.  See Example 1.6 on page 11 of the lecture notes titled ``Mathematical Introduction to Quantum Information Processing'', from Professor Michael Wolf of the Technical University of Munich (\url{https://www-m5.ma.tum.de/foswiki/pub/M5/Allgemeines/MA5057_2019S/QIPlecture.pdf}), and a convenient diagram included in ``Holevo Capacity of Discrete Weyl Channels'', by Rehman et. al (\url{https://www.nature.com/articles/s41598-018-35777-7}).  Unfortunately, neither of those constructions are Hermitian.  To construct an orthonormal basis of Hermitian matrices, we use a related construction.  First, let $\ket{k}_{k=0}^{d-1}$ be an orthonormal basis for $V$.  The matrices we construct, $U_{k,\ell}$, will be doubly indexed by elements of the basis. When $\ket{k}_{k=0}^{d-1}$ is the standard basis, each will have two non-zero entries, unless $k=\ell$, in which case there will be a single non-zero entry.  Define $$\alpha_{k,\ell} := \left\{ \arraycolsep=6pt\def\arraystretch{1.3}\begin{array}{cc} \frac{1}{\sqrt{2}} & k < \ell \\ \frac{1}{2} & k = \ell \\ \frac{i}{\sqrt{2}} &  k > \ell\end{array}\right.$$

\noindent Now let $U_{k,\ell} = \alpha_{k,\ell}\ket{k}\bra{\ell}+\alpha_{k,\ell}^*\ket{\ell}\bra{k}$.  When $k=\ell$, $\ket{k}\bra{\ell}$ and $\ket{\ell}\bra{k}$ coincide, placing $ \alpha_{k,\ell}+\alpha_{k,\ell}^* = 1$ somewhere along the diagonal.  More generally, for fixed $k, \ell$
\begin{align*}
U_{k,\ell}^\dagger &= \Bigl( \alpha_{k,\ell}\ket{k}\bra{\ell}+\alpha_{k,\ell}^*\ket{\ell}\bra{k}\Bigr)^\dagger \tag{definition}\\
&=  \alpha_{k,\ell}^*\bra{\ell}^{\dagger}\ket{k}^\dagger + \alpha_{k,\ell}\bra{\ell}^\dagger\ket{k}^\dagger \tag{properties of $^\dagger$} \\
&= \alpha_{k,\ell}^*\ket{\ell}\bra{k} + \alpha_{k,\ell}\ket{k}\bra{\ell} \tag{simplify $^\dagger$}\\
&= U_{k,\ell},
\end{align*}
so each $U_{k,\ell}$ is Hermitian.  To show they are orthonormal, consider the inner-product of two arbitrary matrices $U_{k,\ell}$ and $U_{n.m}$.
\begingroup
\allowdisplaybreaks
\begin{align*}
\left(U_{k,\ell},  U_{n,m}\right) &= \Tr(U_{k,\ell}^\dagger U_{n,m}) \tag{definition of $(\cdot, \cdot)$} \\
&= \Tr(U_{k,\ell}U_{n,m}) \tag{Hermiticity of $U_{k,\ell}$} \\
&=\Tr\biggl(  \Bigl(\alpha_{k,\ell}\ket{k}\bra{\ell}+\alpha_{k,\ell}^*\ket{\ell}\bra{k}\Bigr)\Bigl(\alpha_{n,m}\ket{n}\bra{m}+\alpha_{n,m}^*\ket{m}\bra{n}\Bigr)\biggr) \tag{definition of $U_{i,j}$} \\
&=\Tr\biggl(\begin{array}{cccc}& \alpha_{k,\ell}\alpha_{n,m}\ket{k}\braket{\ell|n}\bra{m} &+& \alpha_{k,\ell}\alpha_{n,m}^*\ket{k}\braket{\ell|m}\bra{n} \\ +& \alpha_{k,\ell}^*\alpha_{n,m}\ket{\ell}\braket{k|n}\bra{m} &+& \alpha_{k,\ell}^*\alpha_{n,m}^*\ket{\ell}\braket{k|m}\bra{n}\end{array}\biggr)\tag{F.O.I.L.} \\
&=\begin{array}{cccc}&\alpha_{k,\ell}\alpha_{n,m}\delta_{\ell,n}\Tr\bigl(\ket{k}\bra{m}\bigr) &+& \alpha_{k,\ell}\alpha_{n,m}^*\delta_{\ell,m}\Tr\bigl(\ket{k}\bra{n}\bigr) \\
                                   +&\alpha_{k,\ell}^*\alpha_{n,m}\delta_{k,n}\Tr\bigl(\ket{\ell}\bra{m}\bigr) &+& \alpha_{k,\ell}^*\alpha_{n,m}^*\delta_{k,m}\Tr\bigl(\ket{\ell}\bra{n}\bigr)\end{array} \tag{linearity, orthonormality} \\
&=\begin{array}{cccc}&\alpha_{k,\ell}\alpha_{n,m}\delta_{\ell,n}\Tr\bigl(\braket{m|k}\bigr) &+& \alpha_{k,\ell}\alpha_{n,m}^*\delta_{\ell,m}\Tr\bigl(\braket{n|k}\bigr) \\
                                   +&\alpha_{k,\ell}^*\alpha_{n,m}\delta_{k,n}\Tr\bigl(\braket{m|\ell}\bigr) &+& \alpha_{k,\ell}^*\alpha_{n,m}^*\delta_{k,m}\Tr\bigl(\braket{n|\ell}\bigr)\end{array} \tag{cyclicity of $\Tr$} \\
&=\begin{array}{cccc}&\alpha_{k,\ell}\alpha_{n,m}\delta_{\ell,n}\delta_{k,m} &+& \alpha_{k,\ell}\alpha_{n,m}^*\delta_{\ell,m}\delta_{k,n} \\
                                   +&\alpha_{k,\ell}^*\alpha_{n,m}\delta_{k,n}\delta_{\ell,m} &+& \alpha_{k,\ell}^*\alpha_{n,m}^*\delta_{k,m}\delta_{\ell,n}\end{array} \tag{orthonormality} \\
&= \begin{array}{cc}&\bigl(\alpha_{k,\ell}\alpha_{n,m}+(\alpha_{k,\ell}\alpha_{n,m})^*\bigr)\delta_{k,m}\delta_{\ell,n} \\ +& \bigl(\alpha_{k,\ell}\alpha_{n,m}^* + (\alpha_{k,\ell}\alpha_{n,m}^*)^*\bigr)\delta_{k,n}\delta_{\ell,m}
\end{array} \tag{group like $\delta$s, property of $^*$} \\
&= \begin{array}{cc}&2\Re(\alpha_{k,\ell}\alpha_{n,m})\delta_{k,m}\delta_{\ell,n} \\+& 2\Re(\alpha_{k,\ell}\alpha_{n,m}^*)\delta_{k,n}\delta_{\ell,m} \end{array} \tag{property of $^*$} \\
\end{align*}
\endgroup
\noindent When $k\neq n$ or $\ell \neq m$ only the first term contributes, so $(U_{k,\ell}, U_{n, m}) = 2\Re(\alpha_{k,\ell}\alpha_{n,m})\delta_{k,m}\delta_{\ell,n}$.  This can only be nonzero if $m=k$ and $n=\ell$, in which case  $(U_{k,\ell}, U_{n, m}) = (U_{k,\ell}, U_{\ell, k}) =  2\Re(\alpha_{k,\ell}\alpha_{\ell,k}) = 2\Re(\frac{i}{2}) = 0$, since $k\neq n = \ell$ implies one of the $\alpha_{k,\ell}$ and $\alpha_{\ell,k}$ is $\frac{1}{\sqrt{2}}$, and the other 
$\frac{i}{\sqrt{2}}$.  Hence, the $U_{k,\ell}$ are orthogonal.  If $k=n$ and $\ell=m$, then $(U_{k,\ell}, U_{k,\ell}) = 2\Re(\alpha_{k,\ell}\alpha_{k,\ell})\delta_{k,\ell}^2 + 2\Re(\alpha_{k,\ell}\alpha_{k,\ell}^*)$.  There are two cases.  If $k=\ell$, then $\alpha_{k,\ell} = \alpha_{k,\ell}^* =\frac{1}{2}$, so $(U_{k,\ell}, U_{k,\ell}) = 2(\frac{1}{4})+2(\frac{1}{4}) = 1$.  When $k\neq\ell$, $(U_{k,\ell}, U_{k,\ell}) = 2\Re(\alpha_{k,\ell}\alpha_{k,\ell}^*) = 2\Re(\norm{\alpha_{k,\ell}}^2) = 2(\frac{1}{2}) = 1$.  So in all cases $(U_{k,\ell}, U_{k,\ell}) = 1$ and thus the $U_{k,\ell}$ are a set of $d^2$ Hermitian matrices which are orthonormal.  Orthonormality implies linear independence, in which case the subspace spanned by the $U_{k,\ell}$ has dimension $d^2$ and so must be $L_V$ itself, making $U_{k,\ell}$ a basis.
%First, let $\{\ket{i}\}_{i=0}^{d-1}$ be a set of orthonormal basis vectors for $V$.  Define $\omega = e^{\frac{2\pi i}{d}}$, and for $ 0\leq n, m < d$, $$U_{n,m} := \sum_{k=0}^{d-1}\omega^{kn}\ket{k-m}\bra{k},$$
%where the subtraction inside the ket is done mod $d$.  Note that $\ket{i}\bra{j}$ is exactly one of the basis matrices discussed above, populated with 0's except for a 1 in positon $i,j$.  To see that these matrices are Hermitian, we must show that for fixed $n,m$, the entry of $U_{n,m}$ in position $i,j$ is the complex conjugate of the entry in position $j,i$.   To be continued.

%\begin{comment}
%To isolate the entry in position $i,j$, note that column $j$ consists of $\omega^{jn}\ket{j-m}$, with $i$-th entry equal to $\omega^{jm}$ iff $j-m\equiv i \Mod{d}$, otherwise $0$.  If $j-m\equiv i \Mod(d}$, then i-ji.if $j-i \not\equiv m\Mod{d}$, then this entry is 0, since the only non-zero entry in column $j$ is  .  Seeing that $i-j\not\equiv m \Mod{d}$ iff $j-i\not\equiv m \Mod{d}$, we see that the entries are complex conjugates in this case
%\begin{align*}
%U_{\ell,k} &= \sum_{r=0}^{d-1}\omega^{rk}\ket{r+k}\bra{r} \tag{definition}
%\end{align*}
%\begin{comment}
%\begin{bmatrix}
%	0 & 1 \\
%	1 & 0
%\end{bmatrix}
%
%\begin{bmatrix}
%	0 & -i \\
%	i & 0
%\end{bmatrix}
%
%\begin{bmatrix}
%	1 & 0 \\
%	0 & -1
%\end{bmatrix}

\Textbf{2.40} \textbf{(Commutation relations for the Pauli matrices)} Verify the commutation relations $$[X, Y] = 2iZ;\ [Y, Z] = 2iX;\ [Z,X] = 2iY.$$ There is an elegant way of writing this using $\epsilon_{jk\ell}$, the antisymmetric tensor on three indices, for which $\epsilon_{jk\ell}=0$ except for $\epsilon_{123} = \epsilon_{231}=\epsilon_{321} = 1$ and $\epsilon_{321}=\epsilon_{213}=\epsilon_{132}=-1$: $$[\sigma_j,\sigma_k] = 2i\sum_{\ell=1}^3\epsilon_{jk\ell}\sigma_\ell$$.
\Soln
\begin{align*}
	[\sigma_1,\sigma_2] = \left[X, Y \right] &=XY - YX\\
		&= \begin{bmatrix}
		0 & 1 \\
		1 & 0
		\end{bmatrix}
		\begin{bmatrix}
		0 & -i \\
		i & 0
		\end{bmatrix}
		-
		\begin{bmatrix}
		0 & -i \\
		i & 0
		\end{bmatrix}
		\begin{bmatrix}
		0 & 1 \\
		1 & 0
		\end{bmatrix} \\
%
		&=
%
		\begin{bmatrix}
			i & 0 \\
			0 & -i
		\end{bmatrix}
		-
		\begin{bmatrix}
			-i & 0 \\
			0 & i
		\end{bmatrix}\\
%
		&=
%
		\begin{bmatrix}
			2i & 0 \\
			0 & -2i
		\end{bmatrix} \\
%
		&=	2i Z = 2i\epsilon_{12}\sigma_3
\end{align*}

\begin{align*}
	[\sigma_2,\sigma_3] = \left[Y, Z \right] &= \begin{bmatrix}
		0 & -i \\
		i & 0
	\end{bmatrix}
	\begin{bmatrix}
		1 & 0 \\
		0 & -1
	\end{bmatrix}
	-
	\begin{bmatrix}
		1 & 0 \\
		0 & -1
	\end{bmatrix}
	\begin{bmatrix}
		0 & -i \\
		i & 0
	\end{bmatrix}\\
	&=
	\begin{bmatrix}
		0 & 2i \\
		2i & 0
	\end{bmatrix}\\
	&= 2iX = 2i\epsilon_{231}\sigma_1
\end{align*}

\begin{align*}
	[\sigma_3,\sigma_1] = \left[Z, X\right] &= \begin{bmatrix}
	1 & 0 \\
	0 & -1
	\end{bmatrix}
	\begin{bmatrix}
	0 & 1 \\
	1 & 0
	\end{bmatrix}
	-
	\begin{bmatrix}
	0 & 1 \\
	1 & 0
	\end{bmatrix}
	\begin{bmatrix}
	1 & 0 \\
	0 & -1
	\end{bmatrix}\\
	&=\begin{bmatrix}
	0 & 2 \\
	-2 & 0
	\end{bmatrix}\\
	&=
	2i \begin{bmatrix}
	0 & -i \\
	i & 0
	\end{bmatrix}\\
	&= 2iY = 2i\epsilon_{312}\sigma_2
\end{align*}

\Textbf{2.41} \textbf{(Anti-commutation relations for the Pauli Matrices)}  Verify that the anticommutation relations $${\sigma_i,\sigma_j}=0$$ where $i\neq j$ are both chosen from the set $1,2,3$.  Also verify that  ($i=0,1,2,3$) $$\sigma_i^2 = I.$$
\Soln
\begin{align*}
\{X, Y\} = \left\{\sigma_1, \sigma_2 \right\} &=\sigma_1 \sigma_2 + \sigma_2 \sigma_1\\
&= \begin{bmatrix}
0 & 1 \\
1 & 0
\end{bmatrix}
\begin{bmatrix}
0 & -i \\
i & 0
\end{bmatrix}
+
\begin{bmatrix}
0 & -i \\
i & 0
\end{bmatrix}
\begin{bmatrix}
0 & 1 \\
1 & 0
\end{bmatrix} \\
%
&=
%
\begin{bmatrix}
i & 0 \\
0 & -i
\end{bmatrix}
+
\begin{bmatrix}
-i & 0 \\
0 & i
\end{bmatrix}\\
%
&= 0
\end{align*}

\begin{align*}
\{Y, Z\}=\left\{\sigma_2, \sigma_3 \right\} &= \begin{bmatrix}
0 & -i \\
i & 0
\end{bmatrix}
\begin{bmatrix}
1 & 0 \\
0 & -1
\end{bmatrix}
+
\begin{bmatrix}
1 & 0 \\
0 & -1
\end{bmatrix}
\begin{bmatrix}
0 & -i \\
i & 0
\end{bmatrix}\\
&=0
\end{align*}

\begin{align*}
\{Z, X\} = \left\{\sigma_3, \sigma_1 \right\} &= \begin{bmatrix}
1 & 0 \\
0 & -1
\end{bmatrix}
\begin{bmatrix}
0 & 1 \\
1 & 0
\end{bmatrix}
+
\begin{bmatrix}
0 & 1 \\
1 & 0
\end{bmatrix}
\begin{bmatrix}
1 & 0 \\
0 & -1
\end{bmatrix}\\
&=0
\end{align*}

\begin{align*}
	\sigma_0^2 &= I^2 = I\\
%
	X^2=\sigma_1^2 &= \begin{bmatrix}
	0 & 1 \\
	1 & 0
	\end{bmatrix} ^2 = I\\
%
	Y^2=\sigma_2^2 &= \begin{bmatrix}
	0 & -i \\
	i & 0
	\end{bmatrix} ^2 = I\\
%
	Z^2=\sigma_3^2 &= \begin{bmatrix}
	1 & 0 \\
	0 & -1
	\end{bmatrix} ^2 = I
\end{align*}

\Textbf{2.42} Verify that $$AB = \frac{[A,B]+\{A,B\}}{2}.$$
\Soln
\begin{align*}
	\frac{\left[A, B \right] + \left\{A, B\right\}}{2} = \frac{AB - BA + AB + BA}{2} = AB \tag{definition of $[\cdot,\cdot]$ and $\{\cdot,\cdot\}$}
\end{align*}

\Textbf{2.43} Show that for $j,k=1,2,3$, $$\sigma_j\sigma_k = \delta_{jk}I+i\sum_{\ell=1}^3\epsilon_{jk\ell}\sigma_\ell.$$

From Exercises 2.41 (eq (2.75), (2.76)), $\left\{\sigma_j,  \sigma_k \right\} = 2 \delta_{jk} I$.
\begin{align*}
	\sigma_j \sigma_k &= \frac{\left[\sigma_j, \sigma_k  \right] + \left\{\sigma_j, \sigma_k \right\}}{2}\tag{Exercise 2.42 (eq 2.77)}\\
		&= \frac{2i \sum_{l=1}^{3} \epsilon_{jkl}\sigma_l +  2 \delta_{jk} I}{2} \tag{Exercise 2.40 (eq 2.74) and above}\\
		&= \delta_{jk} I + i \sum_{\ell=1}^{3} \epsilon_{jk\ell}\sigma_\ell\tag{cancel 2s}
\end{align*}


\Textbf{2.44} Suppose $[A, B] = 0$, $\{A, B\} = 0$, and $A$ is invertible.  Show that $B$ must be 0.
\Soln
By assumption, $\left[A, B\right] = AB-BA=0$ implies $AB=BA$, now $\left\{A, B\right\} = AB+BA=2AB=0$, so $AB = 0$.
Since $A$ is invertible, multiplying by $A^{-1}$ from the left gives
\begin{align*}
	A^{-1} AB = 0 \tag{A is invertible, multiply both sides by $A^{-1}$}\\
	IB = 0 \tag{$A^{-1}A=I$}\\
	B=0.
\end{align*}


\Textbf{2.45} Show that $[A, B]^\dagger = [B^\dagger, A^\dagger]$.
\Soln
\begin{align*}
	\left[A, B\right]^\dagger &= (AB -BA)^\dagger \tag{definition of $[\cdot,\cdot]$}\\
		&= B^\dagger A^\dagger - A^\dagger B^\dagger\tag{properties of $^\dagger$}\\
		&= \left[B^\dagger, A^\dagger \right] \tag{definition of $[\cdot,\cdot]$}
\end{align*}

\Textbf{2.46} Show that $[A,B] = -[B,A]$.
\Soln
\begin{align*}
	\left[A, B\right] &= AB - BA \tag{definition of $[\cdot, \cdot]$}\\
		&= - (BA - AB) \tag{reverse signs}\\
		&= -\left[B, A\right] \tag{definition of $[\cdot, \cdot]$}
\end{align*}

\Textbf{2.47} Suppose $A$ and $B$ are Hermitian.  Show the $i[A,B]$ is Hermitian.
\Soln
\begin{align*}
	\left(i \left[A, B\right] \right)^\dagger &= -i \left[A, B\right]^\dagger \tag{distribute $^\dagger$} \\
		&= -i \left[B^\dagger, A^\dagger \right] \tag{Exercise 2.45}\\
		&= -i \left[B, A \right] \tag{$A$ and $B$ are Hermitian}\\
		&= i \left[A, B\right] \tag{Exercise 2.46}
\end{align*}

\Textbf{2.48} What is the polar decomposition of positive matrix $P$?  Of a unitary matrix $U$? Of a Hermitian matrix $H$?
\Soln

(Positive)  Since $P$ is positive, it is Hermitian by Exercise 2.24, thus normal, so the spectral decomposition theorem gives that it is diagonalizable. Then $P = \sum_i \lambda_i \kb{i}$,  where $\lambda_i \geq 0$ by positivity. Note that this implies that $\sqrt{\lambda_i^2}=\lambda_i$.
\begin{align*}
	J &= \sqrt{P^\dagger P} \tag{uniqueness of $J$}\\
	&= \sqrt{P^2} \tag{Hermiticity}\\
	&= \sum_i \sqrt{\lambda_i^2} \kb{i} \tag{spectral decomposition, definition of $\sqrt{\cdot}$}\\
	&= \sum_i \lambda_i \kb{i} \tag{$\sqrt{\cdot}$ in $\mathbb{R}$}\\
	& = P \tag{spectral decomposition}.
\end{align*}
Therefore, for any positive operator $P$, the polar decomposition of $P$ is $P = UP$.  *If $P$ were positive \textit{definite}*, it would be easy to show that $P$ is invertible.  If $a=\sum_j a_j\ket{j}$, then if $Pa = 0$, $Pa = P\left(\sum_j a_j\ket{j}\right) = \Bigl(\sum_i \lambda_i \kb{i}\Bigr)\left(\sum_j a_j\ket{j}\right) = \sum_i \lambda_i a_i \ket{i} = 0$. Since $\ket{i}$ is a basis, we must have $a_i\lambda_i=0$ for all $i$, but the $\lambda_i$ cannot be 0 since, being a basis vector, $\ket{i}\neq 0$ implies $\braket{i|P|i} =\lambda_i >0$ by positive definiteness.  Now all $a_i = 0$.  Having 0-dimensional null-space, $P$ must be invertible, in which case $U=I$ by uniqueness of $U$, since $I$ satisfies $P = IP$. Then $P = P$ \textit{is} the polar decomposition of $P$.

% Thus $U = I$, then $P = P$ \textit{is} the polar decomposition of $P$.


\vspace{3mm}(Unitary) Suppose unitary $U$ is decomposed by $U = WJ$ where $W$ is unitary and $J$ is positive, $J = \sqrt{U^\dagger U}$.
\begin{align*}
	J = \sqrt{U^\dagger U} = \sqrt{I} = I
\end{align*}
Since unitary operators are invertible, $W = UJ^{-1} = UI^{-1} = UI = U$.
Thus, the polar decomposition of $U$ is $U = U$.

Alternatively, since $J$ is unique, note that $U$ satisfies $U=UJ$, where $J=I$, so $U=U$ is again the polar decomposition of $U$.  This is essentially the same argument, as above, where we use the stated uniqueness of $J$ instead of the unique formula for it.

\vspace{3mm}
(Hermitian) Suppose $H = UJ$.  By Hermiticity
\begin{align*}
	J = \sqrt{H^\dagger H} = \sqrt{HH} = \sqrt{H^2}.
\end{align*}
Thus $H = U\sqrt{H^2}$.

\begin{screen}
	In general, $H \neq \sqrt{H^2}$.

	From spectral decomposition, $H = \sum_i \lambda_i \kb{i}$, $\lambda_i \in \mathds{R}$.
	\begin{align*}
		 \sqrt{H^2} = \sqrt{ \sum_i \lambda_i^2 \kb{i} }
		 =
 		\sum_i
 			\sqrt{
 				\lambda_i^2
			} \kb{i}
		= \sum_i | \lambda_i | \kb{i} \neq H
	\end{align*}
unless $H$ is positive.
\end{screen}

\Textbf{2.49} Express the polar decomposition of a normal matrix in the outer product representation.
\Soln
Let $A$ be a normal matrix, which by the spectral decomposition theorem we may write $A = \sum_i \lambda_i \kb{i}$ for an orthonormal basis $\ket{i}$.  As in the proof of the polar decomposition theorem, define $\ket{e_i} = A\ket{i}$ for those $\ket{i}$  for which $\lambda_i\neq 0$, and extend via Graham-Schmidt to find $\ket{e_i}$ for those $\ket{i}$ for which $\lambda_i=0$.    
\begin{align*}
	J &= \sqrt{A^\dagger A} = \sum_i | \lambda_i | \kb{i}. \tag{uniqueness}\\
	U &= \sum_i \kbt{e_i}{i}.\tag{as constructed in the proof}\\
	A &= UJ \tag{polar decomposition} \\
	  &= \left(\sum_i \ket{e_j}\bra{j}\right)\left(\sum_j |\lambda_j|\kb{j}\right) \tag{defined above} \\ 
	  &= \sum_i |\lambda_i| \kbt{e_i}{i}. \tag{orthonormality}
\end{align*}

\Textbf{2.50} Find the left and right polar decomposition of the matrix $$A=\begin{bmatrix}1& 0\\ 1&1\end{bmatrix}.$$
\Soln We have
$A^\dagger A = \begin{bmatrix}1& 1\\ 0&1\end{bmatrix}\begin{bmatrix}1& 0\\ 1&1\end{bmatrix} = \begin{bmatrix}
2 & 1 \\
1 & 1
\end{bmatrix}$.  To construct $J$, we must find a spectral decomposition of $A^\dagger A$.  The characteristic equation of $A^\dagger A$ is $\det(A^\dagger A - \lambda I) = (2-\lambda)(1-\lambda)-1 = \lambda^2 - 3 \lambda + 1 = 0$.  By the quadratic formula, the eigenvalues of $A^\dagger A$ are $\lambda_\pm = \frac{3 \pm \sqrt{5}}{2}$.  The assosiated orthonormal eigenvectors are $\ket{\lambda_\pm} =
 \frac{5 \pm\sqrt{5}}{10}  \begin{bmatrix} \frac{1\pm\sqrt{5}}{2} \\
 1
 \end{bmatrix} $.  We have $\kb{\lambda_\pm}=\begin{bmatrix}1\pm\frac{2\sqrt{5}}{5} & \frac{1}{2}\pm\frac{3\sqrt{5}}{10} \\ \frac{1}{2}\pm\frac{3\sqrt{5}}{10}  & \frac{1}{2}\pm\frac{\sqrt{5}}{10}\end{bmatrix}$.  By the spectral decomposition theorem, $A^\dagger A = \lambda_+ \kb{\lambda_+} + \lambda_- \kb{\lambda_-}$, and
 \begin{align*}
 	J = \sqrt{A^\dagger A} &= \sqrt{\lambda_+} \kb{\lambda_+} + \sqrt{\lambda_-} \kb{\lambda_-} \tag{definition of $\sqrt{\cdot}$}\\
 		&= \sum_\pm \sqrt{\frac{3 \pm \sqrt{5}}{2}} \begin{bmatrix}1\pm\frac{2\sqrt{5}}{5} & \frac{1}{2}\pm\frac{3\sqrt{5}}{10} \\ \frac{1}{2}\pm\frac{3\sqrt{5}}{10}  & \frac{1}{2}\pm\frac{\sqrt{5}}{10}\end{bmatrix}
 \end{align*}
\begin{align*}
	J^{-1} = \frac{1}{ \sqrt{\lambda_+} } \kb{\lambda_+} + \frac{1}{ \sqrt{\lambda_-} } \kb{\lambda_-}.\tag{Applying the $^-1$ function to $J$}
\end{align*}
\begin{align*}
	U = AJ^{-1} \tag{$A$ is invertible, $U$ is unique}
\end{align*}
It is not worth simplifying $J, J^{-1}$, or $U$, nor is it worth finding the right polar decomposition.

\Textbf{2.51} Verify that the Hadamard gate $H$ is unitary.
\Soln
\begin{align*}
	H^\dagger H = \left(\frac{1}{\sqrt{2}} \begin{bmatrix}
	1 & 1 \\
	1 & -1
	\end{bmatrix}\right)^\dagger
	\frac{1}{\sqrt{2}} \begin{bmatrix}
	1 & 1 \\
	1 & -1
	\end{bmatrix}
	=
	\frac{1}{\sqrt{2}} \begin{bmatrix}
	1 & 1 \\
	1 & -1
	\end{bmatrix}
	\frac{1}{\sqrt{2}} \begin{bmatrix}
	1 & 1 \\
	1 & -1
	\end{bmatrix}
	=
	\frac{1}{2} \begin{bmatrix}
	2 & 0 \\
	0 & 2
	\end{bmatrix}
	=
	I.
\end{align*}

\Textbf{2.52}Very that $H^2=I$.
\Soln It was shown above that $H^\dagger=H$, so $H^2=H^\dagger H= I$.

\Textbf{2.53} What are the eigenvalues and eigenvectors of $H$?
\Soln Using the characteristic equation \begin{align*}
	\det \left(H - \lambda I\right) &= \left(\frac{1}{\sqrt{2}} - \lambda \right) \left(- \frac{1}{\sqrt{2}} - \lambda \right) - \frac{1}{2}\\
		&= \lambda^2 - \frac{1}{2} - \frac{1}{2}\\
		&= \lambda^2 - 1 =0,
\end{align*}
the eigenvalues are $\lambda_\pm = \pm 1$. The associated orthonormal eigenvectors can be calculated to be $\ket{\lambda_\pm} = \frac{1}{\sqrt{4 \mp 2 \sqrt{2}}} \begin{bmatrix}
1 \\
-1 \pm \sqrt{2}
\end{bmatrix} $.

\Textbf{2.54} Suppose $A$ and $B$ are commuting Hermitian operators.  Prove that $\exp(A)\exp(B)=\exp(A+B)$.
\Soln
Since $[A, B] = 0$, $A$ and $B$ are simultaneously diagonalizable. Let $\ket{i}$ be an orthonormal basis such that $A = \sum_i a_i \kb{i}$, $B = \sum_i b_i \kb{i}$.  Note that $A+B$ is also simultaneously diagonalizable, since $A+B = \sum_i (a_i+b_i)\kb{i}$.
\begin{align*}
	\exp (A) \exp (B) &= \left(\sum_i \exp (a_i) \kb{i}\right) \left(\sum_i \exp (b_i) \kb{i}\right) \tag{from above} \\
		&= \sum_{i,j} \exp (a_i + b_j) \ket{i} \braket{i | j} \bra{j} \tag{group sum}\\
		&= \sum_{i,j} \exp (a_i + b_j) \kbt{i}{j} \delta_{i, j} \tag{$\ket{i}$ is orthonormal}\\
		&= \sum_i \exp (a_i +  b_i) \kb{i}\tag{group non-zero terms}\\
		&= \exp (A+B) \tag{definition of $\exp(\cdot)$}.
\end{align*}

\Textbf{2.55} Prove that $U(t_1,t_2)$ defined in Equation (2.91) is unitary.
\Soln Some definitions: $H$ is the Hamiltonian of some closed system.  It is Hermitian, and hence has a spectral decomposition: $H = \sum  E \kb{E}$.  Note that $H^\dagger=\left(\sum E \kb{E}\right)^\dagger = \sum E^* \bigl(\kb{E}\bigr)^\dagger = \sum E^* \kb{E}$ by Exercise 2.13.  $U$ is defined as $U(t_1, t_2) \equiv \exp\left[\frac{-iH(t_2-t_1)}{\hbar}\right]$.  To prove $U$ is unitary, we show $UU^\dagger=I$.  Now
\begin{align*}
 (U(t_1,t_2))^\dagger &= \left(\exp\left[\frac{-iH(t_2-t_1)}{\hbar}\right]\right)^\dagger \tag{definition of $U$}\\
    &= \left(\sum_E \exp\left(\frac{-iE(t_2-t_2)}{\hbar}\right)\kb{E}\right)^\dagger \tag{definition of $\exp(A)$}\\
    &= \sum_E \exp\left(\frac{-iE(t_2-t_1)}{\hbar}\right)^*\kb{E}^\dagger \tag{linearity of $^\dagger$} \\
    &= \sum_E \exp \left(\frac{iE(t_2-t_1)}{\hbar}\right)\kb{E} \tag{complex conjugation, exercise 2.13} \\
    &= \exp\left(\frac{iH(t_2-t_1)}{\hbar}\right) \tag{definition of $\exp(A)$}
    \end{align*}
We have   
\begin{align*}
	U(t_2 - t_1) (U(t_2 - t_1))^\dagger &= \exp \left( - \frac{iH(t_2 - t_1)}{\hbar} \right)  \exp \left(  \frac{iH(t_2 - t_1)}{\hbar} \right) \tag{from above}\\
		&= \sum_{E, E'} \left(\exp \left(\frac{-iE(t_2 - t_1)}{\hbar} \right) \kb{E} \right)
										\left(\exp \left(\frac{iE'(t_2 - t_1)}{\hbar} \right) \kb{E'} \right) \tag{$E'$ used to distinguish from $E$}\\
		&= \sum_{E, E'} \exp \left(- \frac{i(E-E')(t_2 - t_1)}{\hbar} \right) \ket{E}\bra{E'} \delta_{E,E'} \tag{orthonormality}\\
		&= \sum_E \exp(0) \kb{E} \tag{group nonzero terms}\\
		&= \sum_E \kb{E} \tag{$e^0=1$}\\
		&= I \tag{completeness}
\end{align*}

Similarly, $(U(t_2 - t_1))^\dagger U (t_2 - t_1) = I$.  So, $U$ is unitary.

\Textbf{2.56} Use the spectral decomposition to show that $K\equiv-i\log(U)$ is Hermitian for any unitary $U$ and thus $U=\exp(iK)$ for some Hermitian $K$.
\Soln Since $U$ is unitary, it has a spectral decomposition with respect to some orthonormal basis, say $\ket{\lambda}$.  For each eigenvalue $\lambda$, note that exerces 2.18 gives that $\norm{\lambda} = 1$, so express $\lambda=e^{i\theta}$ for some real valued argument $\theta$.  Then $\log(U) = \sum \log(\lambda)\kb{\lambda}=\sum i\theta\kb{\lambda}$.  Now $K \equiv -i\log(U) = \sum \theta \kb{\lambda}$, and $K^\dagger = \left(\sum \theta\kb{\lambda}\right)^\dagger = \sum \theta^*(\kb{\lambda})^\dagger= \sum \theta\kb{\lambda} = K$, since $\theta$ is real and exercise 2.13 gives that $(\kb{\lambda})^\dagger=\kb{\lambda}$.  So, $K$ is Hermitian, and by applying the fact that $\exp$ and $\log$ are inverse complex valued functions, at least for the $\lambda$, we can conclude that $\exp(iK)=\exp(i\cdot(-i\log(U))) = \exp(\log(U)) = U$.

%$U = \sum_i \lambda_i \kb{\lambda_i}$~~~ ($|\lambda_i| = 1$).
%\begin{align*}
%	\log (U) &= \sum_j \log (\lambda_j) \kb{\lambda_j} = \sum_j i \theta_j  \kb{\lambda_j} \text{ where } \theta_j = \arg (\lambda_j)\\
%	K &= - i \log(U) = \sum_j \theta_j \kb{\lambda_j}.
%\end{align*}
%
%\begin{align*}
%	K^\dagger = (-i \log U)^\dagger = \left(\sum_j \theta_j \kb{\lambda_j}\right)^\dagger
%	= \sum_j \theta_j^* \kb{\lambda_j} = \sum_j \theta_j \kb{\lambda_j} = K
%\end{align*}

\Textbf{2.57} \textbf{(Cascaded measurements are  single measurements)} Suppose $\{L_\ell\}$ and $\{M_m\}$ are two sets of measurement operators.  Show that a measurement defined by the measurement operators $\{L_\ell\}$ followed by a measurement defined by the measurement operators $\{M_m\}$ is physically equivalent to a single measurement defined by measurement operators $\{N_{\ell m}\}$ with the representation $N_{\ell m} \equiv M_mL_\ell$.
\Soln  Let the state of a physical system be $\psi$.  Note that by definition the state after applying measurement operators $\{L_\ell\}$ is $\ket{\phi} \equiv \frac{L_\ell\ket{\psi}}{\sqrt{\braket{\psi|L_\ell^\dagger L_\ell|\psi}}}$ for some $\ell$.  The state after applying measurement operators $\{M_m\}$ is $\xi \equiv \frac{M_m\ket{\phi}}{\sqrt{\braket{\phi|M_m^\dagger M_m|\phi}}}$ for some $m$.  Now
\begin{align*}
    \xi &= \frac{M_m\ket{\phi}}{\sqrt{\braket{\phi|M_m^\dagger M_m|\phi}}} \tag{from above} \\
    &= \frac{M_m \frac{L_\ell \ket{\psi}}{\sqrt{{\braket{\psi|L_\ell^\dagger L_\ell|\psi}}}}}{\sqrt{\left\langle\frac{L_\ell\ket{\psi}}{\sqrt{\braket{\psi|L_\ell^\dagger L_\ell|\psi}}}\Big\lvert M_m^\dagger M_m\Big\rvert \frac{L_\ell\ket{\psi}}{\sqrt{\braket{\psi|L_\ell^\dagger L_\ell|\psi}}}\right\rangle}} \tag{substitute for $\phi$} \\
    &= \frac{\frac{M_mL_\ell\ket{\psi}}{\sqrt{\braket{\psi|L_\ell^\dagger L_\ell|\psi}}}}{\sqrt{\frac{\big\langle L_\ell\ket{\psi}\big\lvert M_m^\dagger M_m\big\rvert L_\ell\psi\big\rangle}{\braket{\psi|L_\ell^\dagger L_\ell|\psi}}}} \tag{group internal scalar values} \\
    &=\frac{\frac{\sqrt{\braket{\psi|L_\ell^\dagger L_\ell|\psi}}}{1}\cdot\frac{M_mL_\ell\ket{\psi}}{\sqrt{\braket{\psi|L_\ell^\dagger L|\psi}}}}{\sqrt{\braket{\psi|L_\ell^\dagger M_m^\dagger M_m L_\ell | \psi}}} \tag{move the scalar to numerator}\\
    &=\frac{(M_mL_\ell)\psi}{\sqrt{\braket{\psi| (M_mL_\ell)^\dagger(M_mL_\ell)|\psi}}} \tag{cancel scalars, group operators}\\
    &= \frac{N_{\ell m}\ket{\psi}}{\sqrt{\braket{\psi|N_{\ell m}^\dagger N_{\ell m}|\psi}}} \tag{definition of $N_{\ell m}$}
\end{align*}
This gives that the state of the system after applying measurement operators $\{L_\ell\}$ followed by measurement operators $\{M_m\}$ can be expressed in terms of applying measurement operators $N_{\ell m} = M_mL_\ell$.  Note, though, that the state of the system after applying $N_{\ell m}$ is exactly the state of the system after applying $M_m$, for any $\ell$.  In practice, the intermediate measurement result $\ell$ would be unknown, and to find the probability that the system is in state $m$ after application of the measurement operators $\{N_{\ell m}\}$, one would have to sum over the possible intermediate measurement results.  


%\begin{align*}
%	&\ket{\phi} \equiv \frac{L_l \ket{\psi}}{\sqrt{\braket{\psi |L_l^\dagger L_l | \psi}}}\\
%	&\braket{\phi | M_m^\dagger M_m | \phi} = \frac{\braket{\psi | L_l^\dagger M_m^\dagger M_m L_l | \psi}}{\braket{\psi | L_l^\dagger L_l | \psi}} \\
%%
%	&\frac{M_m \ket{\phi}}{\sqrt{\braket{\phi | M_m^\dagger M_m | \phi}}} =
%		\frac{M_m L_l \ket{\psi}}{\sqrt{\braket{\psi |L_l^\dagger L_l | \psi}}}
%		\cdot
%		\frac{ \sqrt{\braket{\psi | L_l^\dagger L_l | \psi}} }{ \sqrt{\braket{\psi | L_l^\dagger M_m^\dagger M_m L_l | \psi}} }
%		=
%		\frac{M_m L_l \ket{\psi}}{ \sqrt{\braket{\psi | L_l^\dagger M_m^\dagger M_m L_l | \psi}} }
%		=
%		\frac{N_{lm} \ket{\psi}}{\sqrt{\braket{\psi |N^\dagger_{lm}  N_{lm}  | \psi}}}
%\end{align*}



\Textbf{2.58} Suppose we prepare a quantum system in an eigenstate $\ket{\psi}$ of some observable $M$, with corresponding eigenvalue $m$.  What is the average observed value of $M$, and the standard deviation?
\Soln Express $M = \sum_\mu \mu P_\mu$, where the $\ket{\mu}$ are an orthonormal set of eigenvectors each with eigenvalue $\mu$. We may assume $\norm{\ket{\psi}} = 1$, so that $\ket{\psi} = \ket{\mu}$ for some $\ket{\mu}$ (and $m = \mu$ for some $\mu$). 

\begin{align*}
	\mathbf{E}(M) = \braket{M} &= \braket{\psi|M|\psi} \tag{ definition of $\mathbf{E}(M)$}\\
	&= \left\langle \psi \Big\lvert \sum_\mu \mu P_\mu\Big\rvert \psi\right\rangle \tag{ spectral decomposition of $M$}\\
	&=\sum_\mu \mu\braket{\psi|P_\mu|\psi} \tag{linearity}\\
	&=\sum_\mu \mu\braket{\psi|\mu}\braket{\mu|\psi} \tag{$P_\mu$ is a projector}\\
	&=\sum_\mu \mu \delta_{\psi,\mu}^2 \tag{orthonormality}\\
	&=m \tag{collect nonzero terms}
	\end{align*}
Similarly, $\braket{M^2} = \braket{ \psi | M^2 | \psi} = m^2$.  Now $\Delta(M) = \sqrt{\braket{M^2}-\braket{M}^2} = \sqrt{m^2-(m)^2} = 0$.
%\begin{align*}
%	\braket{M^2} &= \braket{ \psi | M^2 | \psi} = \braket{\psi | m^2 | \psi} = m^2 \braket{\psi | \psi} = m^2\\
%	\text{deviation} &= \braket{M^2} - \braket{M}^2 = m^2 - m^2 = 0.
%\end{align*}


\Textbf{2.59} Suppose we have (a) qubit in the state $\ket{0}$, and we  measure the observable $X$.  What is the average value of $X$?  What is the standard deviation?
\Soln There are two ways to proceed.  First, we can apply $X$, noting that $X\ket{0}=\ket{1}$ and $X\ket{1} = \ket{0}$. 
\begin{align*}
	\braket{X} &= \braket{0 | X | 0} = \braket{0 | 1} = 0\\
	\braket{X^2} &= \braket{0 | X^2 | 0} = \braket{0 | X | 1} =\braket{0 | 0} = 1\\
	\Delta(X) &= \sqrt{ \braket{X^2} - \braket{X}^2 } = \sqrt{1-0^2} = 1
\end{align*}
Alternatively, write $X = (\kb{+})-(\kb{-})$, and note that $X^2 = (\kb{+})+(\kb{-})$ Now
\begin{align*}
	\braket{X} &= \braket{0 | X | 0} = \braket{0|+}\braket{+|0} - \braket{0|-}\braket{-|0}=\frac{1}{2}-\frac{1}{2} = 0\\
	\braket{X^2} &= \braket{0 | X^2 | 0} = \braket{0|+}\braket{+|0} + \braket{0|-}\braket{-|0}=\frac{1}{2}+\frac{1}{2} = 1\\
	\Delta(X)  &= \sqrt{ \braket{X^2} - \braket{X}^2 } = \sqrt{1 - 0^2} = 1
\end{align*}



\Textbf{2.60}  Show that $\vec{v}\cdot\vec{\sigma}$ has eigenvalues $\pm1$, and that the projectors onto the corresponding eigenspaces are given by $P_\pm = (I\pm\vec{v}\cdot\vec{\sigma})/2$.
\Soln We calculate eigenvalues by expressing $\vec{v}\cdot\vec{\sigma}$ explicitly in terms of $v_1,v_2$, and $v_3$.
\begin{align*}
    \vec{v} \cdot \vec{\sigma} &= \sum_{i=1}^3 v_i \sigma_i \tag{definition}\\
    &= v_1 \begin{bmatrix}
        0 & 1 \\
        1 & 0
    \end{bmatrix}
    + v_2 \begin{bmatrix}
        0 & -i \\
        i & 0
    \end{bmatrix}
    + v_3 \begin{bmatrix}
        1 & 0 \\
        0 & -1
    \end{bmatrix} \tag{$\sigma_1=X, \sigma_2=Y, \sigma_3=Z$}\\
    &= \begin{bmatrix}
        v_3 & v_1 - i v_2 \\
        v_1 + iv_2 & -v_3
    \end{bmatrix} \tag{add corresponding entries}
\end{align*}

\begin{align*}
    \det (\vec{v} \cdot \vec{\sigma}  - \lambda I) &= (v_3 - \lambda) (-v_3 - \lambda) - (v_1 - iv_2) (v_1 + iv_2) \tag{characteristic equation}\\
    &= \lambda^2 - (v_1^2 + v_2^2  + v_3^2) \tag{simplify}\\
    &= \lambda^2 - 1 \tag{$\vec{v}$ is a unit vector}
\end{align*}
So the eigenvalues of $\vec{v}\cdot\vec{\sigma}$ are $\lambda = \pm 1$.

To calculate the projectors onto the corresponding eigenspaces, note that $\ket{\lambda_1} = \begin{bmatrix}1 \\\frac{1-v_3}{v_1-iv_2}\end{bmatrix}$ can easily be seen to be an eigenvector with eigenvalue $\lambda=1$.  To normalize, we'll use the fact that $1-v_3^2 = v_1^2+v_2^2$, since $\vec{v}$ is a unit vector.  Factoring both sides and dividing yields $\frac{1\pm_1v_3}{v_1\pm_2iv_2} = \frac{v_1\mp_2iv_2}{1\mp_1v_3},$ that is, such rational functions can be flipped by negating both binary operations.

\begin{align*}\norm{\ket{\lambda_1}}^2 &= 1+\norm{\frac{1-v_3}{v_1-iv_2}}^2 \tag{definition of $\norm{\cdot}^2$}\\
    &= 1+\left(\frac{v_1+iv_2}{1+v_3}\right)\left(\frac{v_1-iv_2}{1+v_3}\right) \tag{flip, $\norm{c}^2=c\cdot c^*$} \\
    &= 1+\frac{v_1^2+v_2^2}{(1+v_3)^2} \tag{expand} \\
    &= \frac{1+2v_3+v_1^2+v_2^2+v_3^2}{(1+v_3)^2} \tag{common denominator} \\
    &= \frac{2(1+v_3)}{(1+v_3)^2} \tag{$\vec{v}$ is a unit vector} \\
    &=\frac{2}{1+v_3} \tag{cancel}
\end{align*}
So $\ket{\lambda_1} = \sqrt{ \frac{1+v_3}{2 }} \begin{bmatrix}
1 \\
\frac{1-v_3}{v_1 - iv_2}
\end{bmatrix} $ is a normalized eigenvector with eigenvalue $1$.  Similarly, $\ket{\lambda_{-1}} = \sqrt{ \frac{1-v_3}{2 }} \begin{bmatrix}
    1 \\
    - \frac{1+v_3}{v_1 - iv_2}
\end{bmatrix} $ is a normalized eigenvector with eigenvalue $-1$.  For convenience, we write $\ket{\lambda_{\pm1}} = \sqrt{\frac{1\pm v_3}{2}}\begin{bmatrix}1 \\ \frac{v_3\mp 1}{v_1-iv_2}\end{bmatrix}$.  Calculating the projectors:
\begin{align*}
	\kb{\lambda_{\pm1}} &= \frac{1\pm v_3}{2 } \begin{bmatrix}
		1 \\
		\frac{v_3\mp1}{v_1 - iv_2}
	\end{bmatrix}
	\begin{bmatrix}
   		1 &
   		\left(\frac{v_3\mp1}{v_1 - iv_2}\right)^*
	\end{bmatrix} \tag{definition}\\
	&=  \frac{1\pm v_3}{2 } \begin{bmatrix}
		1 \\
		\frac{v_1 + iv_2}{v_3\pm1}
	\end{bmatrix}
	\begin{bmatrix}
   		1 &
   		\left(\frac{v_1 + iv_2}{v_3\pm1}\right)^*
	\end{bmatrix} \tag{flip}\\
	&=\frac{1\pm v_3}{2 } \begin{bmatrix}
		1 \\
		\frac{v_1 + iv_2}{v_3\pm1}
	\end{bmatrix}
	\begin{bmatrix}
   		1 &
   		\frac{v_1 - iv_2}{v_3\pm1}
	\end{bmatrix} \tag{$v_1,v_2$ are real, conjugate}\\
%
	&=
	 \frac{1\pm v_3}{2 } \begin{bmatrix}
    	 1 & \frac{v_1 - iv_2}{1 \pm v_3} \\
    	 \frac{v_1 + iv_2}{1 \pm v_3} & \frac{v_1^2+v_2^2}{(1 \pm v_3)^2}
	 \end{bmatrix} \tag{multiply}\\
	 &= \frac{1}{2}\begin{bmatrix} 1\pm v_3 & v_1-iv_2 \\ v_1+iv_2 & \frac{1-v_3^2}{1\pm v_3}\end{bmatrix} \tag{$v$ is a unit vector, cancel external numerator}\\
	 &= \frac{1}{2}\begin{bmatrix} 1\pm v_3 & v_1-iv_2 \\ v_1+iv_2 & 1\mp v_3^2 \end{bmatrix} \tag{cancel internal denominator}\\
	 &= \frac{1}{2}\left(I\pm \begin{bmatrix}v_3 & v_1-iv_2 \\ v_1+iv_2 & -v_3\end{bmatrix}\right) \tag{separate}\\
	 &= \frac{1}{2}(I\pm\vec{v}\cdot\vec{\sigma}) \tag{definition}
\end{align*}
\begin{comment}
(ii) If $\lambda = -1$.
\begin{align*}
	\vec{v} \cdot \vec{\sigma}  - \lambda I &= \vec{v} \cdot \vec{\sigma}  + I\\
	&= \begin{bmatrix}
		v_3 + 1 & v_1 - i v_2 \\
		v_1 + i v_2 & - v_3 + 1
	\end{bmatrix}
\end{align*}

Normalized eigenvalue is 


\begin{align*}
	\kb{\lambda_{-1}} &= \frac{1 - v_3}{2} \begin{bmatrix}
	1 \\
	- \frac{1+v_3}{v_1 - iv_2}
	\end{bmatrix}
	\begin{bmatrix}
		1 & - \frac{1+v_3}{v_1 + iv_2}
	\end{bmatrix}\\
	&=
	\frac{1 - v_3}{2} \begin{bmatrix}
		1 & - \frac{v_1 - iv_2}{1 - v_3} \\
		- \frac{v_1 + iv_2}{1 - v_3} & \frac{1+v_3}{1 - v_3}
	\end{bmatrix} \\
	&=
	\frac{1}{2} \begin{bmatrix}
		1 - v_3 & -(v_1 - iv_2) \\
		- (v_1 + iv_2) & 1 + v_3
	\end{bmatrix} \\
	&=
	\frac{1}{2} \left( I - \begin{bmatrix}
		v_3 & v_1 - iv_2 \\
		(v_1 + iv_2 & - v_3
	\end{bmatrix} \right)\\
	&= \frac{1}{2} (I - \vec{v} \cdot \vec{\sigma} ).
\end{align*}
\end{comment}
The first author points out that when $v_1-iv_2 = 0$, or equivalently when $v_3\pm1 = 0$, various expressions above are indeterminant.  The first author attempts to circumvent this by working more generally below, however, the second author is suspicious that the argument is circular.  Some of the details are instructive however, so it is left below.  To deal with the degenerate cases, note that, since $\vec{v}$ is a real-valued vector, $v_1-iv_2=0$ implies $v_1=0, v_2=0$, and $v_3=\pm 1$, in which case $\vec{v}\cdot\vec{\sigma} = \pm Z$.  The normalized eigenvectors are $\ket{\lambda_1} = \pm\ket{0}$ and $\ket{\lambda_{-1}}=\pm\ket{1}$, where here the $\pm$ indicate the sign of $v_3$ instead of the sign of the eigenvalue.  Now
\begin{align*}
\kb{\lambda_1} = (\pm1)^2\kb{0} &= \begin{bmatrix}1&0\\0&0\end{bmatrix} = \frac{1}{2}(I+Z)=\frac{1}{2}(I+\vec{v}\cdot{\vec{\sigma}}) \\
\kb{\lambda_{-1}}=(\pm1)^2\kb{1} &=  \begin{bmatrix}0&0\\0&1\end{bmatrix} = \frac{1}{2}(I-Z)=\frac{1}{2}(I-\vec{v}\cdot{\vec{\sigma}}),
\end{align*}
completing the proof.  The first author's attempted resolution follows:
\vspace{5pt}
\hrule
\vspace{5pt}
	While I review my proof, I notice that my proof has a defect.
	The case $(v_1,v_2,v_3) = (0,0,1)$, second component of eigenstate, $\frac{1-v_3}{v_1 - iv_2}$, diverges.
	So I implicitly assume $v_1 - iv_2 \neq 0$. Hence my proof is incomplete.

	Since the exercise doesn't require explicit form of projector, we should prove the problem more abstractly.
	In order to prove, we use the following properties of $\vec{v} \cdot \vec{\sigma}$
	\begin{itemize}
		\item $\vec{v} \cdot \vec{\sigma}$ is Hermitian
		\item $(\vec{v} \cdot \vec{\sigma})^2 = I$ where $\vec{v}$ is a real unit vector.
	\end{itemize}

	We can easily check above conditions.
	\begin{align*}
	(\vec{v} \cdot \vec{\sigma})^\dagger &= (v_1 \sigma_1 + v_2 \sigma_2 + v_3 \sigma_3)^\dagger\\
	&= v_1 \sigma_1^\dagger + v_2 \sigma_2^\dagger + v_3 \sigma_3^\dagger\\
	&= v_1 \sigma_1 + v_2 \sigma_2 + v_3 \sigma_3~~~(\because \text{Pauli matrices are Hermitian.})\\
	&= \vec{v} \cdot \vec{\sigma}
	\end{align*}

	\begin{align*}
	(\vec{v} \cdot \vec{\sigma})^2 &= \sum_{j,k=1}^3 (v_j \sigma_j)  (v_k \sigma_k)\\
	&= \sum_{j,k=1}^3 v_j v_k \sigma_j \sigma_k\\
	&= \sum_{j,k=1}^3 v_j v_k \left(\delta_{jk}I + i \sum_{l=1}^3 \epsilon_{jkl}\sigma_l \right) ~~~(\because \text{Exercise 2.43, eqn (2.78) page 78})\\
	&= \sum_{j,k=1}^3 v_j v_k \delta_{jk}I  + i \sum_{j,k,l=1}^3 \epsilon_{jkl} v_j v_k \sigma_l\\
	&= \sum_{j=1}^3 v_j^2 I\\
	&= I ~~~\left(\because \sum_j v_j^2 = 1 \right)
	\end{align*}


	\begin{proof}
		Suppose $\ket{\lambda}$ is an eigenstate of $\vec{v} \cdot \vec{\sigma}$ with eigenvalue $\lambda$. Then
		\begin{align*}
		\vec{v} \cdot \vec{\sigma} \ket{\lambda} = \lambda \ket{\lambda}\\
		(\vec{v} \cdot \vec{\sigma})^2 \ket{\lambda} = \lambda^2 \ket{\lambda}
		\end{align*}
		On the other hand $(\vec{v} \cdot \vec{\sigma})^2 = I$,
		\begin{align*}
		(\vec{v} \cdot \vec{\sigma})^2 \ket{\lambda} = I \ket{\lambda} = \ket{\lambda}\\
		\therefore \lambda^2\ket{\lambda} = \ket{\lambda}.
		\end{align*}
		Thus $\lambda^2 = 1 \Rightarrow \lambda = \pm 1$. Therefore $\vec{v} \cdot \vec{\sigma}$ has eigenvalues $\pm 1$.

		Let $\ket{\lambda_1}$ and $\ket{\lambda_{-1}}$ are eigenvectors with eigenvalues $1$ and $-1$, respectively.
		I will prove that $P_\pm = \kb{\lambda_{\pm 1}}$.

		In order to prove above equation, all we have to do is prove following condition. (see Theorem \ref{diagonalzero})
		\begin{screen}
			\begin{align}
				\braket{\psi | (P_\pm - \kb{\lambda_{\pm 1}})| \psi} = 0 \text{ for all } \ket{\psi} \in \mathds{C}^2.
			\end{align}
		\end{screen}

		Since $\vec{v} \cdot \vec{\sigma}$ is Hermitian, $\ket{\lambda_1}$ and $\ket{\lambda_{-1}}$ are orthonormal vector ($\because $ Exercise 2.22).
		Let $\ket{\psi} \in \mathds{C}^2$ be an arbitrary state. $\ket{\psi}$ can be written as
		\begin{align*}
		\ket{\psi} = \alpha \ket{\lambda_1} + \beta \ket{\lambda_{\pm 1}} ~~(|\alpha|^2 + |\beta|^2 = 1, \alpha, \beta \in \mathds{C}).
		\end{align*}

		\begin{align*}
		\braket{\psi | (P_{\pm} - \kb{\lambda_\pm})| \psi}
		%		&= \braket{\psi | \left(\frac{1}{2} (I \pm \vec{v} \cdot \vec{\sigma}) - \kb{\lambda_\pm}\right)| \psi}\\
		&= \braket{\psi | P_\pm | \psi} - \braket{\psi | \lambda_\pm} \braket{\lambda_\pm | \psi}.   \\
		\braket{\psi | P_\pm | \psi} &= \braket{\psi | \frac{1}{2}(I \pm \vec{v} \cdot \vec{\sigma}) | \psi}\tag*{$\Bigl(\Bigr. \begin{array}{r}\text{implicit assumption the above equals 0}\\ \text{and that} \kb{\lambda_{\pm1}}=\frac{1}{2}(I\pm\vec{v}\cdot{\sigma})\end{array} \Bigl.\Bigr)$ }\\
		&= \frac{1}{2} \pm \frac{1}{2} \braket{\psi | \vec{v} \cdot \vec{\sigma})| \psi}\\
		&= \frac{1}{2} \pm \frac{1}{2} (|\alpha|^2 - |\beta|^2)\\
		&= \frac{1}{2} \pm \frac{1}{2} (2|\alpha|^2 - 1) ~~~(\because |\alpha|^2 + |\beta|^2 = 1)\\
		\braket{\psi | \lambda_1} \braket{\lambda_1 | \psi} &= |\alpha|^2\\
		\braket{\psi | \lambda_{-1}} \braket{\lambda_{-1}| \psi} &= |\beta|^2 = 1  - |\alpha|^2
		\end{align*}

		Therefore $\braket{\psi | (P_\pm - \kb{\lambda_{\pm 1}})| \psi} = 0$ for all $\ket{\psi} \in \mathds{C}^2$.
		Thus $P_\pm = \kb{\lambda_{\pm 1}}$.
	\end{proof}

\vspace{5pt}
\hrule
\vspace{5pt}
\Textbf{2.61} Calculate the probability of obtaining the result $+1$ for a measurement of $\vec{v}\cdot\vec{\sigma}$, given that the state prior to measurement is $\ket{0}$.  What is the state of the system after the measurement if $+1$ is obtained?
\Soln 
\begin{align*}
	p(1) = \braket{0|P_1|0} &= \left\langle 0 \Big\lvert \frac{1}{2} ( I + \vec{v} \cdot \vec{\sigma} ) \Big\rvert 0\right\rangle \tag{definition}\\
	           &= \braket{0|\lambda_1}\braket{\lambda_1|0} \tag{use eigenvector directly}\\
	           &=\frac{1+v_3}{2}\begin{bmatrix}1&0\end{bmatrix}\begin{bmatrix}1 \\ \frac{1-v_3}{v_1-iv_2}\end{bmatrix}\begin{bmatrix}1 & \frac{1-v_3}{v_1+iv_2}\end{bmatrix}\begin{bmatrix}1\\0\end{bmatrix} \tag{substitute}\\
	           &= \frac{1+v_3}{2}\begin{bmatrix}1&0\end{bmatrix} \begin{bmatrix}
    	 1 & \frac{v_1 - iv_2}{1 + v_3} \\
    	 \frac{v_1 + iv_2}{1 + v_3} & \frac{1-v_3}{1+v_3}
	 \end{bmatrix}\begin{bmatrix}1\\0\end{bmatrix} \tag{multiply}\\
		&= \frac{1}{2} (1 + v_3) \tag{extract 0,0 entry}
\end{align*}
The post-measurement state is
\begin{align*} \frac{\ket{\lambda_1}\braket{\lambda_1|0}}{\sqrt{p(1)}} &=\left(\frac{\sqrt{\frac{1+v_3}{2}}}{\sqrt{\frac{1+v_3}{2}}}\begin{bmatrix}1 & \left(\frac{1-v_3}{v_1-iv_2}\right)^*\end{bmatrix}\begin{bmatrix}1\\0\end{bmatrix}\right) \ket{\lambda_1} \tag*{$\Bigl(\Bigr. \begin{array}{r}\text{numerator from normalization}\\ \text{denominator from above}\end{array} \Bigl.\Bigr)$ } \\
&= \ket{\lambda_1} \tag{simplify, multiply}
\end{align*}
%\begin{align*}
%	\frac{\ket{\lambda_1} \braket{\lambda_1 | 0}}{ \sqrt{\braket{0 | \lambda_1} \braket{\lambda_1 | 0}} } &= \frac{1}{\sqrt{\frac{1}{2} (1 + v_3)}}
%	\cdot \frac{1}{2}
%	\begin{bmatrix}
%		1 + v_3 \\
%		v_1 + iv_2
%	\end{bmatrix} \\
%		&= \sqrt{ \frac{1}{2}  (1 + v_3) } \begin{bmatrix}
%		1 \\
%		\frac{v_1 + iv_2}{1+v_3}
%		\end{bmatrix} \\
%		&=  \sqrt{ \frac{1 + v_3}{2} } \begin{bmatrix}
%		1 \\
%		\frac{1 - v_3}{v_1 - iv_2}
%		\end{bmatrix} \\
%		&= \ket{\lambda_1}.
%\end{align*}
\Textbf{2.62} Show that any measurement where the measurement operators and the POVM elements coincide is a projective measurement. 
\Soln We can no longer avoid the converse of Exercise 2.16.  Before we prove it, we quibble about semantics.  The assumption in the exercise is that the POVM elements ``coincide'' with the measurement operators.  This can be taken to mean one of two things.  Let $\{M_m\}$ be the set of measurement operators.  One interpretation is that $M_m = E_m = M_m^\dagger M_m$ for all $m$.  We'll call this direct coincidence.  Another interpretation is that they coincide as sets, i.e., that $\{M_m\} = \{E_m\}$, without requiring $M_m = E_m$ for each $m$.  A mathematician may argue that what is to be assumed is set-wise coincidence, but we'll show that this is not the case.  We'll argue for the assumption of direct coincidence by contradiction.  If it were the case that POVM measurements which satisfied the setwise coincidence assumption but {\em not} the direct coincidence assumption were projective measurements, then there would exist $M_m=E_\mu = M_\mu^\dagger M_\mu$, where $M_m \neq M_\mu$.  However, being a projective measurement, $M_\mu$ must be Hermitian, in which case $M_m = M_\mu^2 =M_\mu$, by Exercise 2.16 (not the converse).  This is a contradiction, but it is important to note what we can conclude from it.  Our assumption was that setwise coincidence of measurement operators and POVM measurements was enough to prove the measurement projective.  Having contradicted this, we conclude that the exercise must assume direct coincidence instead.  We have {\em not} proven that setwise coincidence without direct coincidence is impossible.
\begin{screen}
\begin{thm}\label{idempotent}
	Let $P$ be a {\em normal} linear operator.  If $P^2 = P$, then $P$ is a projector, that is $P=\sum_i \kb{i}$ for some orthonormal basis $\ket{i}$.
\end{thm}
\begin{proof} Having assumed normality (the statement is not true in general if we do not), we may apply the Structural Decomposition Theorem to write $P=\sum_i\lambda_i \kb{i}$.  Note, we may assume $\lambda_i \neq 0$.  Being idempotent, $\sum_i\lambda_i \kb{i} = P = P^2 = \left(\sum_i\lambda_i \kb{i}\right)^2 = \sum_i \lambda_i^2 \kb{i}$ by orthonormality.  The $\kb{i}$ are linearly independent (see Exercise 2.10), from which we conclude that all $\lambda_i$ satisfy $\lambda_i = \lambda_i^2$, from which $\lambda_i(\lambda_i-1) = 0$, or that $\lambda_i = 1$, since $\lambda_i\neq 0$.  Now $P = \sum_i \kb{i}$, as required.

Note: Some definitions of projectors define them in terms of relations between their kernels and images as opposed to the formulaic definition given in Equation 2.35.  They are equivalent (at least for normal matrices).  With the set theoretic definition, Exercise 2.16 and it's converse Theorem \ref{idempotent} follow by definition.
\end{proof}
\end{screen}
On to the exercise:  Suppose $M_m$ is a measurement operator such that $E_m = M_m^\dagger M_m = M_m$.  Note that $M_m = M_m^\dagger M_m$ is positive by exercise 2.25, thus Hermitian, so  $M_m =  E_m = M_m^\dagger M_m = M_m^2$.  Being Hermitian, $M_m$ is normal, in which case Theorem \ref{idempotent} above applies, giving that $M_m$ is a projector.  Thus the measurement is a projective measurement.

\Textbf{2.63} Suppose a measurement is described by measurement operators $M_m$.  Show that there exist unitary operators $U_m$ such that $M_m=U_m\sqrt{E_m}$, where $E_m$ is the POVM associated to the measurement.
\Soln By the singular value decomposition (Corrolary 2.4, eq 2.80, p 79), there exists unitary $U, V$, and real-valued diagonal $D$  such that $M_m = UDV$.  Now
\begin{align*} \sqrt{E_m} &= \sqrt{M_m^\dagger M_m} \tag{definition}\\
                                          &= \sqrt{V^\dagger D^\dagger U^\dagger UDV} \tag{properties of $^\dagger$} \\
                                          &= \sqrt{V^\dagger D^2 V} \tag{$U$ unitary, $D$ real-valued diagonal} \\
                                          &= V^\dagger D V \tag{$V$ unitary$\rightarrow (V^\dagger DV)^2=V^\dagger D^2 V$} \\
                                          &= V^\dagger(U^\dagger U)DV \tag{$U$ unitary, $U^\dagger U=I$} \\
                    UV\sqrt{E_m}&= M_m \tag{$U,V$ unitary, solve for $M_m$}
\end{align*}
Define $U_m \equiv UV$ so that $M_m = U_m\sqrt{E_m}$, completing the solution.
\begin{comment}
\begin{align*}
    M_m^\dagger M_m &= \sqrt{E_m} U_m^\dagger U_m \sqrt{E_m}\\
        &= \sqrt{E_m} I \sqrt{E_m}\\
        &= E_m.
\end{align*}
Since $E_m$ is POVM,  for arbitrary  unitary $U$, $M_m^\dagger M_m$ is POVM.
\end{comment}

\Textbf{2.64} Suppose Bob is given a quantum state chosen from a set $\ket{\psi_1}, \ldots, \ket{\psi_m}$ of linearly independent states.  Construct a POVM $\{E_1, \ldots, E_{m+1}\}$ such that if outcome $E_i$ occurs, $1 \leq i \leq m$, then Bob knows with certainty that he was given the state $\ket{\psi_i}$.
\Soln Being linearly independent, $S$ forms a basis for the subspace it spans. Consider the subspaces spanned by $S_i' = \{\ket{\psi_j}\}_{i\neq j}$.  Let $\ket{\psi_i'}$ be a non-zero unit vector in the orthogonal complement.  Note that if $i\neq j$, then $\braket{\psi_i|\psi_j'} = 0$, since $\psi_j'$ is in the orthogonal complement of a subspace containing $\psi_i$.  Also note that $\braket{\psi_i | \psi_i'} \neq 0$, since if it were, then $\psi_i$ would be in the subspace spanned by $S_i'$, violating linear independence.   Combining, we write $\braket{\psi_i|\psi_j'} = \delta_{i,j}\cdot p_i$, for some non-zero $p_i$.  We may scale the $\psi_i'$ so that $ p_i = 1$.  For, $1\leq i\leq m$, define $E_i = \kb{\psi_i'}$, then, to cover the  define $E_{m+1} = I-\sum_m E_i$.  Now, for $1\leq i,j\leq m$,
\begin{align*}
p_i(j) &\equiv \braket{\psi_i|E_j^\dagger E_j|\psi_i} \tag{definition} \\
    &= \braket{\psi_i\bigl\lvert (\kb{\psi_j'})^\dagger \kb{\psi_j'}\bigr\rvert \psi_i} \tag{definition of $E_j$}\\
    &= \braket{\psi_i|\psi_j'}\braket{\psi_j'|\psi_j'}\braket{\psi_j'|\psi_i} \tag{Exercise 2.13: $(\kb{\phi})^\dagger=\kb{\phi}$} \\
    &= \delta_{i,j}\cdot 1\cdot \delta_{i,j} \tag{construction}\\
    &= \delta_{i,j} \tag{simplify}
\end{align*}
That is, $E_1,\ldots , E_{m+1}$ is a  POVM such that state $\ket{\psi_i}$ and outcome $E_i$ correspond exactly, for $1\leq i\leq m$.  Outcome $E_{m+1}$ will result from measuring any state outside of the span of $S$, that is, in their orthogonal complement, with probability 1 as well
The first author included links to several relevant journal articles.  The second author has not evaluated them, but they are include below.
\begin{itemize}
    \item Lu-Ming Duan, Guang-Can Guo.  Probabilistic cloning and identification of linearly independent quantum states. Phys. Rev. Lett.,80:4999-5002, 1998. arXiv:quant-ph/9804064\\
    \url{https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.80.4999}\\
    \url{https://arxiv.org/abs/quant-ph/9804064}
%
    \item Stephen M. Barnett, Sarah Croke, Quantum state discrimination, arXiv:0810.1970 [quant-ph]\\
    \url{https://arxiv.org/abs/0810.1970}\\
    \url{https://www.osapublishing.org/DirectPDFAccess/67EF4200-CBD2-8E68-1979E37886263936_176580/aop-1-2-238.pdf}
\end{itemize}
\Textbf{2.65} Express the states $(\ket{0}+\ket{1})/\sqrt{2}$ and $(\ket{0}-\ket{1})/\sqrt{2}$ in a basis in which there are \textit{not} the same up to a relative phase shift.
\Soln Note that $(\ket{0} + \ket{1})/\sqrt{2} = \ket{+}$ and $(\ket{0}-\ket{1})/\sqrt{2}=\ket{-}$ are an orthonormal basis. Since the amplitude of $\ket{+}$ in the expression $(\ket{0} + \ket{1})/\sqrt{2} = \ket{+}$ is 1, and that in $(\ket{0} - \ket{1})/\sqrt{2} = \ket{-}$ is 0, there is no relative phase $\theta$ such that such that $e^{i\theta}\cdot 1 = 0$, so $\ket{+}$ and $\ket{-}$ do not differ by a relative phase in the basis they comprise.
%\begin{align*}
%    \ket{+} \equiv \frac{\ket{0} + \ket{1}}{\sqrt{2}}, ~~~ \ket{-} \equiv \frac{\ket{0} - \ket{1}}{\sqrt{2}}
%\end{align*}
\Textbf{2.66}Show that the average value of the observable $X_1Z_2$ for a two qubit system measured in the state $(\ket{00}+\ket{11})/\sqrt{2}$ is zero.
\Soln Let $\ket{\Phi^+} = (\ket{00}+\ket{11})/\sqrt{2}$.
\begin{align*}
    \mathbf{E}(X_1 Z_2) = \braket{X_1 Z_2} &= \braket{ \Phi^+|X_1 Z_2|\Phi^+} \\
    &=  \frac{\bra{00}+\bra{11}}{\sqrt{2}} \cdot \frac{X_1Z_2(\ket{00}+\ket{11})}{\sqrt{2}} \\
    &=  \frac{\bra{00}+\bra{11}}{\sqrt{2}}\cdot \frac{\ket{10}-\ket{01}}{\sqrt{2}} \\
    &= \frac{\braket{00|10} - \braket{00|01} + \braket{11|10} - \braket{11|01}}{2} \\
    &= \frac{0-0+0-0}{2} \\
    &= 0
\end{align*}
\Textbf{2.67} Suppose $V$ is a Hilbert space with a subspace $W$.  Suppose $U : W\rightarrow V$ is a linear operator which preserves inner products, that is, for any $\ket{w_1}$ and $\ket{w_2}$ in $W$, $$\braket{w_1|U^\dagger U|w_2} = \braket{w_1|w_2}.$$  Prove that there exists a unitary operator $U': V\rightarrow V$ which \textit{extends} $U$.  That is, $U'\ket{w} = U\ket{w}$ for all $\ket{w}$ in $W$, but $U'$ is defined on the entire space $V$.  Usually we omit the prime symbol $'$ and just write $U$ to denote the extension.
\Soln
 Let $\ket{w_i}$ be an orthonormal basis for $W$.  Since $U$ preserves inner products in $W$, $\ket{u_i}\equiv U\ket{w_i}$ is an orthonormal basis for $\text{image}(U)$, hence $\braket{u_i|u_j} = \delta_{i,j}$.  Consider the orthogonal complement, $W^\perp$.  By definition $V = W \oplus W^\perp$.  Let $\ket{w_j'}$ and $\ket{u_j'}$ be orthonormal bases for $W^\perp$ and $\left( \mathrm{image}(U) \right)^\perp$.  Note that, as provided, $U$ is not defined on the $\ket{w_j'}$, so we may not state that \sout{$\ket{u_j'} = U\ket{w_j'}$}, where here we  strike the statement not because it is necessarily false, but because it is not assumed.  We can, however, state that $\braket{u_i'|u_j'} = \delta{i,j}$, since the $\ket{u_j}$ are orthonormal, and that $\braket{u_i|u_j'} = 0$, since the $\ket{u_j'}$ are in a space orthogonal to the $\ket{u_i}$.
% Define $U'$ as $U'|_W\ket{w_i} = U\ket{w_i} (= \ket{u_i})$ and $U'|_{W^\perp}\ket{w_j'} = \ket{u_j'}$
Define $U': V \rightarrow V$ as $U' = \sum_i \kbt{u_i}{w_i} + \sum_j \kbt{u_j'}{w_j'}$.  First, we prove unitarity:
\begin{align*}
    (U')^\dagger U' &= \Biggl( \sum_{i} \kbt{w_i}{u_i} + \sum_{j} \kbt{w_j'}{u_j'} \Biggr)  \Biggl( \sum_k \kbt{u_k}{w_k} + \sum_\ell \kbt{u_\ell'}{w_\ell'} \Biggr)\tag{definition, linearity, Exercise 2.13}\\
    &=\sum_{i,k}\ket{w_i}\braket{u_i|u_k}\bra{w_k} + \sum_{i,\ell} \ket{w_i}\braket{u_i|u_\ell'}\bra{w_\ell'}+  \sum_{j,k} \ket{w_j'}\braket{u_j'|u_k}\bra{w_k'} + \sum_{j,\ell}  \ket{w_j'}\braket{u_j'|u_\ell'}\bra{w_\ell'} \tag{F.O.I.L., linearity} \\
    &= \sum_{i,k}\delta_{i,k}\kbt{w_i}{w_k} + \sum_{j,\ell}\delta_{j,\ell}\kbt{w_j'}{w_\ell'} \tag{orthonormality, orthogonality} \\
                    &= \sum_i \kb{w_i} + \sum_j \kb{w_j'} \tag{collect non-zero term} \\
                    &= I \tag{completeness}
\end{align*}
where the last equality holds because $\{\ket{w_i}\}\bigcup\{\ket{w_j'}\}$ forms a basis for the entire space $V$.  Similarly $U'(U')^\dagger = I$,  so $U'$ is unitary. It is left only to show that $U'$ is an extension of $U$, that is $U'\ket{w} = U\ket{w}$ for all $w$ in $W$.
\begin{align*}
    U' \ket{w} &= \left( \sum_i \kbt{u_i}{w_i} + \sum_j \kbt{u_j'}{w_j'} \right) \ket{w} \tag{definition of $U'$} \\
               &= \left(\sum_i \ket{u_i} \bra{w_i}\right) \ket{w} + \sum_j \ket{u_j'} \braket{w_j' | w} \tag{linearity}\\
			   &=  \left(\sum_i \ket{u_i} \bra{w_i}\right) \ket{w}  \tag{$\ket{w}\in W$, $\ket{w_j'} \in W^\perp$}\\
			   &= \left(\sum_i U \ket{w_i} \bra{w_i}\right) \ket{w}\tag{definition of $u_i$}\\
			   &= U\left(\sum_i \kb{w_i}\right)\ket{w} \tag{linearity}\\
			   &= U \ket{w}. \tag{completeness}
\end{align*}
where the last inequality holds only when applied to vectors in the subspace $W$, as is being done.

\Textbf{2.68}  Prove that $(\ket{\Psi^+} =)\ket{\psi}\equiv (\ket{00}+\ket{11})/\sqrt{2} \neq \ket{a}\ket{b}$ for all single qubit states $\ket{a}$ and $\ket{b}$.
\Soln
Suppose $\ket{a} = a_0 \ket{0}  + a_1\ket{1}$ and $\ket{b} = b_0 \ket{0}  + b_1\ket{1}$.  Then
\begin{align*}
    \ket{a} \ket{b} =\ket{a}\otimes\ket{b} = a_0 b_0 \ket{00} + a_0 b_1 \ket{01} + a_1 b_0 \ket{10} + a_1 b_1 \ket{11}.
\end{align*}
If $\ket{\psi} = \ket{a} \ket{b}$, then $a_0 b_0 = 1,~ a_0 b_1=0,~ a_1 b_0 = 0,~ a_1 b_1 = 1$ since $\{\ket{ij}\}$ is an orthonormal basis. Since $a_0 b_1 = 0$, either $a_0 = 0$ or $b_1 = 0$, however, $a_0 = 0$ contradicts $a_0 b_0 = 1$, and $b_1 = 0$ contradicts $a_1 b_1 = 1$. Thus $\ket{\psi} \neq \ket{a} \ket{b}$.


\Textbf{2.69} Verify that the Bell basis forms an orthonormal basis for the two qubit state space.
Define Bell states and the Bell Matrix as follows.
\begin{align*}
    \ket{\Phi^+}=\ket{\psi_0} \equiv \frac{\ket{00} + \ket{11}}{\sqrt{2}} = \frac{1}{\sqrt{2}} \begin{bmatrix}
    1 \\
    0 \\
    0 \\
    1
    \end{bmatrix} &~~~& 
    \ket{\Phi^-}=\ket{\psi_1} \equiv \frac{\ket{00} - \ket{11}}{\sqrt{2}} = \frac{1}{\sqrt{2}} \begin{bmatrix}
    1 \\
    0 \\
    0 \\
    -1
    \end{bmatrix} \\
    \ket{\Psi^+}=\ket{\psi_2} \equiv \frac{\ket{01} + \ket{10}}{\sqrt{2}} = \frac{1}{\sqrt{2}} \begin{bmatrix}
    0 \\
    1 \\
    1 \\
    0
    \end{bmatrix} &~~~& 
    \ket{\Psi^-}=\ket{\psi_3} \equiv \frac{\ket{01} - \ket{10}}{\sqrt{2}} = \frac{1}{\sqrt{2}} \begin{bmatrix}
    0 \\
    1 \\
    -1 \\
    0
    \end{bmatrix}
    \end{align*}
    \begin{align*}
    B = \begin{bmatrix}\ket{\Phi^+} & \ket{\Phi^-} &  \ket{\Psi^+} &  \ket{\Psi^-}\end{bmatrix} &&
\end{align*}
Note that $\braket{\psi_i|\psi_j}$ is the $i,j$-entry in $B^\dagger B$, so orthonormality will follow if $B^\dagger B = I$.
\begin{align*}
 B^\dagger B &= \frac{1}{2}\begin{bmatrix}1 & 0 & 0 & 1 \\ 1 & 0 & 0& -1 \\ 0 & 1 & 1 & 0 \\ 0 & 1 & -1 & 0\end{bmatrix}\begin{bmatrix} 1 & 1 & 0 & 0 \\ 0 & 0 & 1 & 1 \\ 0 & 0 & 1 & -1 \\ 1 & -1 & 0 & 0\end{bmatrix} = \frac{1}{2}\begin{bmatrix}2 & 0 & 0 & 0\\ 0 & 2 & 0 & 0 \\ 0 & 0 & 2 & 0 \\ 0 & 0 & 0 &2\end{bmatrix} = I
 \end{align*}
 Linear independence follows from orthogonality.  Being 4 linearly independent vectors in a 4-dimensional vector space, the Bell states form a basis.
\begin{comment}
First, we prove $\{\ket{\psi_i} \}$ is a linearly independent basis.
\begin{align*}
    &a_1 \ket{\psi_1} + a_2 \ket{\psi_2} + a_3 \ket{\psi_3} + a_4 \ket{\psi_4} = 0\\
    &\therefore \frac{1}{\sqrt{2}} \begin{bmatrix}
        a_1 + a_2 \\
        a_3 + a_4 \\
        a_3 - a_4 \\
        a_1 - a_2
    \end{bmatrix} = 0
\end{align*}
\begin{subnumcases}
 \therefore {}
a_1 + a_2 = 0& \nonumber \\
a_3+ a_4 = 0& \nonumber \\
a_3 - a_4 = 0& \nonumber \\
a_1 - a_2 = 0& \nonumber
\end{subnumcases}
\begin{align*}
    \therefore a_1 = a_2 = a_3 = a_4 = 0
\end{align*}
Thus $\{\ket{\psi_i}\}$ is a linearly independent basis.

Moreover $\norm{\ket{\psi_i}} = 1$ and $\braket{\psi_i | \psi_j} = \delta_{ij}$ for $i,j = 1, 2, 3, 4$.
Therefore $\{\ket{\psi_i}\}$ forms an orthonormal basis.
\end{comment}
\\
\Textbf{2.70} Suppose $E$ is any positive operator acting on Alice's qubit.  Show that $\braket{\psi|E\otimes I|\psi}$ \textit{takes the same value} when $\ket{\psi}$ is any of the four Bell states.  Suppose some malevolent third party (`Eve') intercepts Alice's qubit on the way to Bob in the superdense coding protocol.  Can Eve infer anything about which of the four possible bit strings $00, 01, 10, 11$ Alice is trying to send?  If so, how, or if not, why not?
\Soln
For any of the Bell states we get $\braket{\psi_i | E \otimes I | \psi_i} = \frac{1}{2} (\braket{0|E|0} + \braket{1|E|1})$.  We exhibit the calculation for $\ket{\psi_0}$.  Others are similar.
\begin{align*}
\braket{\psi_0 | E \otimes I | \psi_0} &= \frac{1}{2}\Bigl(\bigl(\bra{00}+\bra{11}\bigr)\cdot (E\otimes I)\bigl(\ket{00}+\ket{11}\bigr)\Bigr) \tag{definition} \\
&=  \frac{1}{2}\biggl(\bigl(\bra{00}+\bra{11}\bigr)\cdot  \Bigl(\bigl((E\ket{0}\bigr)\otimes\ket{0} + \bigl(E\ket{1}\bigr)\otimes\ket{1}\Bigr)\biggr) \tag{$E$ and $I$ act independently} \\
&= \frac{1}{2}\Bigl(\braket{0|E|0}\cdot\braket{0|0} + \braket{0|E|1}\cdot\braket{0|1} + \braket{1|E|0}\cdot\braket{1|0}+\braket{1|E|1}\cdot\braket{1|1}\Bigr) \tag{F.O.I.L.} \\
&=\frac{1}{2}\bigl(\braket{0|E|0} + \braket{1|E|1}\bigr) \tag{collect non-zero terms}
\end{align*}
Suppose Eve measures the qubit Alice sent by measurement operators $M_m$.
The probability that Eve gets result $m$ is $p_i(m) = \braket{\psi_i | M_m^\dagger M_m \otimes I | \psi_i}$.
Since $M ^\dagger_m M_m$ is positive, the result above applies, in which case the $p_i(m)$ take on the same values for all $\ket{\psi_i}$, that is, for all $i$ and $m$, $p_i(m) = 1/4$.  In other words, no matter the outcome $m$, the probability that $\psi$ was in any of the Bell states is uniform.   So Eve can't distinguish Bell states given only access to a single qubit.  [Note, the exercise above only proves Eve can't distinguish Bell states given only access to the first qubit, but the only difference from Bob's perspective is that Eve's Bell basis uses $-\ket{\psi_3}=-\ket{\Psi^-}$.  This negative does not change the result of any of the calculations, so Eve can't distinguish the Bell states given access to Bob's qubit either.]  

\Textbf{2.71} \textbf{(Criterion to decide if a state is mixed or pure)} Let $\rho$ be a density operator.  Show that $\Tr(\rho^2) \leq 1$, with equality if and only if $\rho$ is a pure state.
\Soln
By definition, there exists an ensemble of pure states $\{p_i,\ket{\psi_i}\}$, where $0\leq p_i\leq 1$ and $\sum_i p_i = 1$ such that $\rho = \sum_i p_i \kb{\psi_i}$.  Note that on this range  $0\leq p_i^2 \leq p_i$. Also, $\rho = \sum_i \lambda_i \kb{i}$ for some orthonormal basis $\kb{i}$, by the spectral decomposition theorem.  Express $\ket{\psi_i} = \sum_j \alpha_j \ket{i}$ ... I believe the first author's proof below assumes that $\ket{\psi_i} = \ket{i}$, i.e. that the $\ket{\psi_i}$ are orthonormal, but this is not guaranteed, and in fact, very much not assumed.
\hrule
\begin{align*}
    \rho^2 &= \sum_{i,j} p_i p_j \ket{i}\braket{i|j}\bra{j}\\
        &= \sum_{i,j} p_i p_j \kbt{i}{j}\delta_{ij}\\
        &= \sum_i p_i^2 \kb{i}
\end{align*}

\begin{align*}
    \Tr (\rho^2) &= \Tr \left(\sum_i p_i^2 \kb{i}\right)
        = \sum_i p_i^2 \Tr(\kb{i})
        = \sum_i p_i^2 \braket{i|i}
        = \sum_i p_i^2
        \leq \sum_i p_i = 1~~~ (\because p_i^2 \leq p_i)
\end{align*}

Suppose $\Tr (\rho^2) = 1$. Then $\sum_i p_i^2 = 1$.
Since $p_i^2 < p_i$ for $0 < p_i < 1$,
only single $p_i$ should be 1 and otherwise have to  vanish.
Therefore $\rho = \kb{\psi_i}$. It is a pure state.

Conversely if $\rho$ is pure, then $\rho = \kb{\psi}$.
\begin{align*}
    \Tr (\rho^2) = \Tr (\ket{\psi}\braket{\psi | \psi} \bra{\psi}) = \Tr (\kb{\psi}) = \braket{\psi | \psi} = 1.
\end{align*}
\hrule

\Textbf{2.72} \textbf{(Bloch sphere for mixed states)}  The Bloch sphere picture for pure states of a single qubit was introduced in Section 1.2.  This description has an important generalization to mixed states as follows.\newline
(1) Show that an arbitrary density matrix for a mixed state qubit may be written as $$\rho=\frac{I+\vec{r}\cdot\vec{\sigma}}{2},$$ where $\vec{r}$ is a real three-dimensional vector such that $\norm{\vec{r}}\leq 1$.  This vector is known as the \textit{Bloch vector} for the state $\rho$. \newline
(2) What is the Bloch vector representation for the state $\rho = I/2$?\newline
(3) Show that a state $\rho$ is pure if and only if $\norm{\vec{r}}= 1$.\newline
(4) Show that for pure states the description of the Bloch vector we have given coincides with that in Section 1.2.
\Soln Note, even though the topic of this problem includes mixed states, the representation we are constructing is a representation of a single qubit.  That qubit could be entangled with others, but these other qubits are not explicitly represented in the Bloch sphere representation of the qubit of interest.  The level of entanglement of the qubit of interest with other qubits is represented, though.\newline
(1) Let $\rho$ be an arbitrary density matrix for a single complex-dimensional state.  $\rho$ is Hermitian, so we may set $\rho = \begin{bmatrix}
    a & b \\ b^* & d
\end{bmatrix}$, where $a, d \in \mathds{R}$ and $b \in \mathds{C}$.
Because $\rho$ is density matrix, $\Tr (\rho) = a+d = 1$.   Define $r_1 = \Re(b)/2$, $r_2=-\Im(b)/2$, $r_3 = a-d$, and finally $\vec{r}=(r_1,r_2,r_3)$.  Expressing $a,b,$ and $d$ in terms of $\vec{r}$, we have $a=\frac{1+r_3}{2}$, $b = \frac{r_1-ir_2}{2}$, and $d=\frac{1-r_3}{2}$.  Now
\begin{align*}
    \rho = \begin{bmatrix}
        a & b \\ b^* & d
    \end{bmatrix}
    =
    \frac{1}{2} \begin{bmatrix}
        1+r_3 & r_1 - ir_2 \\
        r_1 + ir_2 & 1 - r_3
    \end{bmatrix}
    =
    \frac{1}{2} (I + \vec{r} \cdot \vec{\sigma}).
\end{align*}
Thus an arbitrary density matrix $\rho$ can be written as $\rho = \frac{1}{2} (I + \vec{r} \cdot \vec{\sigma})$ for some real-valued three-dimensional vector $\vec{r}$.  It remains to show that $\norm{\vec{r}} \leq 1$. To do so, note that Theorem 2.5 gives that $\rho$ is positive. Being positive, the eigenvalues of $\rho$ must be non-negaive.  So, let's find the eigenvalues:
%\begin{align*}
%\norm{r}^2 &= \norm{r_1}^2+\norm{r_2|}^2+\norm{r_3}^2 \tag{definition of $\norm{\cdot}$ in $\mathbb{R}^3$} \\
%    &= \frac{\Re(b)^2 + \Im(b)^2}{4} +  (a-d)^2  \tag{definitions of $a,b,d$} \\
%    &= \frac{\norm{b}^2}{4} + (a+d)^2-4ad \tag{simplify} \\
%    &= \frac{\norm{b}^2}{4} + 1 - 4ad \tag{$\Tr(\rho)=1$}
%\end{align*}
\begin{align*}
    \det (\rho - \lambda I) &= (a-  \lambda) (d - \lambda) - \norm{b}^2 \tag{characteristic equation} \\
    &= \lambda^2 - (a+d)\lambda + ad - \norm{b}^2 = 0 \tag{simplify}\\
    &= \lambda^2 -\lambda + \left(\frac{1-r_3^2}{4}-\frac{r_1^2+r_2^2}{4}\right) \tag{$\Tr(\rho)=1$, express in terms of $\vec{r}$}\\
    &= \lambda^2 - \lambda +\left(\frac{1-\norm{r}^2}{4}\right) \tag{definition of $\norm{\cdot}$ in $\mathbb{R}^3$}\\
        \lambda  &= \frac{1 \pm \sqrt{1 - (1 -\norm{r}^2)}}{2}\tag{quadratic formula}\\
        &= \frac{1 \pm \norm{\vec{r}}^2}{2} \tag{simplify}
\end{align*}
Now $\frac{1 - \norm{\vec{r}}^2}{2} \geq 0 \rightarrow \norm{\vec{r}} \leq 1$.\newline

\noindent(2) If $\rho = I / 2$, then $a=d=1/2$ and $b=0$.  So $\vec{v} = (0,0,0)$, and $\rho = I / 2$ corresponds to the origin of Bloch sphere.

\noindent(3)  By exercise 2.71, $\rho$ is a pure state if and only if $\Tr(\rho^2)=1$. Let's calculate $\Tr(\rho^2)$.

\begin{align*}
    \rho^2 &= \frac{1}{2} (I + \vec{r} \cdot \vec{\sigma})~ \frac{1}{2} (I + \vec{r} \cdot \vec{\sigma}) \tag{part (1) above}\\
        &= \frac{1}{4} \left[ I + 2 \vec{r}\cdot \vec{\sigma} + \norm{r}^2\left(\frac{\vec{r}\cdot\vec{\sigma}}{\norm{r}}\right)^2\right] \tag{F.O.I.L., normalize}\\
        &= \frac{1}{4} \left(I + 2 \vec{r}\cdot \vec{\sigma} + \norm{\vec{r}}^2 I \right) \tag*{$\Bigl(\Bigr. \begin{array}{r}\text{see the first author's attempted resolution}\\ \text{ of the special case of Exercise 2.60}\end{array} \Bigl.\Bigr)$ }\\
    \Tr(\rho^2) &= \frac{1}{4}\left[\Tr(I)+2\Tr(\vec{r}\cdot\vec{\sigma}) + \norm{\vec{r}}^2\Tr(I)\right] \tag{linearity of $\Tr(\cdot)$} \\
    &=\frac{2+2\norm{\vec{r}}^2}{4} \tag{$\Tr(I)=2$ in $\mathbb{C}^2$, $\Tr(\vec{r}\cdot\vec{\sigma})=r_3-r_3 = 0$}.
\end{align*}
Now $\rho$ is a pure state if and only if $\Tr(\rho^2) = \frac{2+2\norm{\vec{r}}^2}{4} = 1$, which occurs if and only if $\norm{\vec{r}} = 1$.\newline
\noindent(4) TODO

\Textbf{2.73}
\begin{screen}
    \Textbf{Theorem 2.6}
%
    \begin{align*}
        \rho = \sum_i p_i \kb{\psi_i}
            = \sum_i \kb{\tilde{\psi_i}}
            = \sum_j \kb{\tilde{\varphi}_j}
            = \sum_j q_j \kb{\varphi_j}
                ~~ \Leftrightarrow ~~
            \ket{\tilde{\psi}_i} = \sum_j u_{ij} \ket{\tilde{\varphi}_j}
    \end{align*}
    where $u$ is unitary.

	The-transformation in theorem 2.6, $\ket{\tilde{\psi}_i} = \sum_j u_{ij} \ket{\tilde{\varphi}_j}$, corresponds to
	\begin{align*}
	    \left[ \ket{\tilde{\psi}_1} \cdots \ket{\tilde{\psi}_k} \right] = \Big[ \ket{\tilde{\varphi}_1} \cdots \ket{\tilde{\varphi}_k} \Big] U^T
	\end{align*}
	where $k = \mathrm{rank} (\mathcal{\rho})$.
    \begin{align}
        \sum_i \kb{\tilde{\psi_i}} &= \left[ \ket{\tilde{\psi}_1} \cdots \ket{\tilde{\psi}_k} \right]
            \begin{bmatrix}
                \bra{\tilde{\psi_1}}\\
                \vdots\\
                \bra{\tilde{\psi_k}}
            \end{bmatrix}\\
        &= \Big[ \ket{\tilde{\varphi}_1} \cdots \ket{\tilde{\varphi}_k} \Big] U^T
            U^* \begin{bmatrix}
                    \bra{\tilde{\varphi}_1}\\
                    \vdots\\
                    \bra{\tilde{\varphi}_k}
            \end{bmatrix}\\
        &= \Big[ \ket{\tilde{\varphi}_1} \cdots \ket{\tilde{\varphi}_k} \Big]
             \begin{bmatrix}
                \bra{\tilde{\varphi}_1}\\
                \vdots\\
                \bra{\tilde{\varphi}_k}
            \end{bmatrix}\\
        &= \sum_j \kb{\tilde{\varphi}_j}.
    \end{align}
\end{screen}

From spectral theorem, density matrix $\rho$ is decomposed as $\rho = \sum_{k=1}^{d} \lambda_k \kb{k}$ where $d = \dim \mathcal{H}$.
Without loss of generality, we can assume $p_k > 0$ for $k = 1 \cdots , l$ where $l = \mathrm{rank} (\rho)$ and $p_k = 0$ for $k = l+1, \cdots, d$.
Thus $\rho = \sum_{k=1}^{l} p_k \kb{k} = \sum_{k=1}^{l} \kb{\tilde{k}}$, where $\ket{\tilde{k}} = \sqrt{\lambda_k} \ket{k}$.

Suppose $\ket{\psi_i}$ is a state in support $\rho$. Then
\begin{align*}
	\ket{\psi_i} = \sum_{k=1}^l c_{ik} \ket{k}, ~~ \sum_k |c_{ik}|^2 = 1.
\end{align*}

Define $\displaystyle p_i = \frac{1}{\sum_k \frac{|c_{ik}|^2}{\lambda_k} }$ and $\displaystyle u_{ik} = \frac{\sqrt{p_i} c_{ik}}{\sqrt{\lambda_k}}$.

Now
\begin{align*}
	\sum_k |u_{ik}|^2 = \sum_k \frac{p_i | c_{ik} |^2 }{\lambda_k} = p_i \sum_k \frac{| c_{ik} |^2 }{\lambda_k} = 1.
\end{align*}

Next prepare an unitary operator
\footnote{By Gram-Schmidt procedure construct an orthonormal basis $\{\boldsymbol{u}_j\}$ (row vector) with $\boldsymbol{u}_i = [u_{i1} \cdots u_{ik} \cdots u_{il}]$. Then define unitary $U = \begin{bmatrix}
    \boldsymbol{u}_1 \\
    \vdots \\
    \boldsymbol{u}_i \\
    \vdots \\
    \boldsymbol{u}_l
    \end{bmatrix}$.}
such that $i$th row of $U$ is $[u_{i1} \cdots u_{ik} \cdots u_{il}]$.
Then we can define another ensemble such that
\begin{align*}
	\Big[  \ket{\tilde{\psi}_1} \cdots  \ket{\tilde{\psi}_i} \cdots \ket{\tilde{\psi}_l}\Big] = \Big[ \ket{\tilde{k}_1} \cdots \ket{\tilde{k}_l} \Big] U^T
\end{align*}
where $\ket{\tilde{\psi_i}} = \sqrt{p_i} \ket{\psi_i}$.
From theorem 2.6,
\begin{align*}
	\rho = \sum_k \kb{\tilde{k}} = \sum_k \kb{\tilde{\psi}_k}.
\end{align*}

Therefore we can obtain a minimal ensemble for $\rho$ that contains $\ket{\psi_i}$.

Moreover since $\rho^{-1} = \sum_k \frac{1}{\lambda_k} \kb{k}$,
\begin{align*}
	\braket{\psi_i | \rho^{-1} | \psi_i} = \sum_k \frac{1}{\lambda_k} \braket{\psi_i | k} \hspace{-1mm} \braket{k | \psi_i} = \sum_k \frac{|c_{ik}|^2}{\lambda_k} = \frac{1}{p_i}.
\end{align*}

Hence, $ \frac{1}{\braket{\psi_i | \rho^{-1} | \psi_i}} = p_i $.


\Textbf{2.74}
\begin{align*}
	\rho_{AB} &= \kb{a}_A \otimes \kb{b}_B\\
	\rho_A &= \Tr_{B} \rho_{AB} = \kb{a} \Tr (\kb{b}) = \kb{a}\\
	\Tr (\rho_A^2) &= 1
\end{align*}
Thus $\rho_A$ is pure.


\Textbf{2.75}
Define $\ket{\Phi_\pm} = \frac{1}{\sqrt{2}} (\ket{00} \pm \ket{11})$ and $\ket{\Psi_\pm} = \frac{1}{\sqrt{2}} (\ket{01} \pm \ket{10})$.
\begin{align*}
	\kb{\Phi_\pm}_{AB} &= \frac{1}{2} (\kb{00} \pm \kbt{00}{11} \pm \kbt{11}{00} + \kb{11})\\
	\Tr_B (\kb{\Phi_\pm}_{AB}) &= \frac{1}{2} (\kb{0} + \kb{1}) = \frac{I}{2}\\
%
	\kb{\Psi_\pm} &= \frac{1}{2} (\kb{01} \pm \kbt{01}{10} \pm \kbt{10}{01} + \kb{10})\\
	\Tr_B (\kb{\Psi_\pm}) &= \frac{1}{2} (\kb{0} + \kb{1}) = \frac{I}{2}
\end{align*}


\Textbf{2.76}

Unsolved. \sout{I think the polar decomposition can only apply to square matrix $A$, not arbitrary linear operators.
Suppose $A$ is $m \times n$ matrix. Then size of $A^\dagger A$ is $n \times n$. Thus the size of $U$ should be $m \times n$.
Maybe $U$ is isometry, but I think it is not unitary.}

I misunderstand linear operator.
\begin{quote}
	Quoted from "Advanced Liner Algebra" by Steven Roman, ISBN 0387247661.

	A linear transformation $\tau : V \rightarrow V$ is called a \textbf{linear operator} on $V$.\footnote{According to Roman, some authors use the term linear operator for any linear transformation from $V$ to $W$.}
\end{quote}
Thus coordinate matrices of linear operator are square matrices. And Nielsen and Chaung say at Theorem 2.3, "Let $A$ be a linear operator on a vector space $V$." Therefore $A$ is a linear transformation such that $A : V \rightarrow V$.

\Textbf{2.77}
\begin{align*}
	\ket{\psi}  &=  \ket{0}  \ket{\Phi_+}\\
		&= \ket{0} \left[\frac{1}{\sqrt{2}}(\ket{00} + \ket{11})\right]\\
		&= (\alpha \ket{\phi_0} + \beta \ket{\phi_1})  \left[\frac{1}{\sqrt{2}}(\ket{\phi_0 \phi_0} + \ket{\phi_1 \phi_1})\right]\\
\end{align*}
where $\ket{\phi_i}$ are arbitrary orthonormal states and $\alpha, \beta \in \mathds{C}$.
We cannot vanish cross term. Therefore $\ket{\psi}$ cannot be written as $\ket{\psi} = \sum_i \lambda_i \ket{i}_A \ket{i}_B \ket{i}_C$.


\Textbf{2.78}
\begin{proof}
	Former part.

	If $\ket{\psi}$ is product, then there exist a state $\ket{\phi_A}$ for system $A$, and a state $\ket{\phi_B}$ for system $B$ such that
	$\ket{\psi} = \ket{\phi_A} \ket{\phi_B}$.

	Obviously, this Schmidt number is  1.

	Conversely, if Schmidt number is 1, the state is written as $\ket{\psi} = \ket{\phi_A} \ket{\phi_B}$.
	Hence this is a product state.
\end{proof}


\begin{proof}
	Later part.

	($\Rightarrow$) Proved by exercise 2.74.

	($\Leftarrow$) Let a pure state be  $\ket{\psi} = \sum_i \lambda_i \ket{i_A} \ket{i_B}$. Then $\rho_A = \Tr_B (\kb{\psi}) = \sum_i \lambda_i^2 \kb{i}$.
	If $\rho_A$ is a pure state, then $\lambda_j = 1$ and otherwise 0 for some $j$.
	It follows that  $\ket{\psi_j} = \ket{j_A} \ket{j_B}$. Thus $\ket{\psi}$ is a product state.
\end{proof}


\Textbf{2.79}
\begin{screen}
	Procedure of Schmidt decomposition.

	Goal: $\ket{\psi} = \sum_{i} \sqrt{\lambda_i} \ket{i_A} \ket{i_B}$

	\begin{itemize}
		\item Diagonalize reduced density matrix $\rho_A = \sum_i \lambda_i \kb{i_A}$.
		\item Derive $\ket{i_B}$, $\displaystyle  \ket{i_B} = \frac{(I \otimes \bra{i_A}) \ket{\psi}}{\sqrt{\lambda_i}}$
		\item Construct $\ket{\psi}$.
	\end{itemize}

\end{screen}


(i)
\begin{align*}
	\frac{1}{\sqrt{2}} (\ket{00} + \ket{11}) \text{ This is already decomposed.}
\end{align*}


(ii)
\begin{align*}
	\frac{\ket{00}+ \ket{01} + \ket{10} + \ket{11}}{2} = \left( \frac{\ket{0} + \ket{1}}{\sqrt{2}}  \right) \otimes \left( \frac{\ket{0} + \ket{1}}{\sqrt{2}}  \right) = \ket{\psi} \ket{\psi} \text{ where } \ket{\psi} = \frac{\ket{0} + \ket{1}}{\sqrt{2}}
\end{align*}



(iii)
\begin{align*}
	\ket{\psi}_{AB} &= \frac{1}{\sqrt{3}} (\ket{00} + \ket{01} + \ket{10})\\
	\rho_{AB} &= \kb{\psi}_{AB}
\end{align*}
%
%
%
\begin{align*}
	\rho_A &= \Tr_B (\rho_{AB}) = \frac{1}{3} \left( 2\kb{0} + \kbt{0}{1} + \kbt{1}{0} + \kb{1} \right)\\
	\det (\rho_A - \lambda I) &= \left( \frac{2}{3} - \lambda \right) \left( \frac{1}{3} - \lambda \right) - \frac{1}{9} = 0\\
	\lambda^2 &- \lambda + \frac{1}{9} = 0\\
	\lambda &= \frac{1 \pm \sqrt{5} / 3}{2} = \frac{3 \pm \sqrt{5}}{6}
\end{align*}



Eigenvector with eigenvalue $\displaystyle \lambda_0 \equiv \frac{3 + \sqrt{5}}{6}$ is $\displaystyle \ket{\lambda_0} \equiv \frac{1}{\sqrt{\frac{5 + \sqrt{5}}{2}}} \begin{bmatrix}
    \frac{1 + \sqrt{5}}{2} \\
    1
\end{bmatrix}$ .

Eigenvector with eigenvalue $\displaystyle \lambda_1 \equiv \frac{3 - \sqrt{5}}{6}$ is $\displaystyle \ket{\lambda_1} \equiv \frac{1}{\sqrt{\frac{5 - \sqrt{5}}{2}}} \begin{bmatrix}
    \frac{1 - \sqrt{5}}{2} \\
    1
\end{bmatrix} $.

\begin{align*}
	\rho_A = \lambda_0 \kb{\lambda_0} + \lambda_1 \kb{\lambda_1}.
\end{align*}


\begin{align*}
	\ket{a_0} \equiv \frac{(I \otimes \bra{\lambda_0}) \ket{\psi} }{\sqrt{\lambda_0}}\\
	\ket{a_1} \equiv \frac{(I \otimes \bra{\lambda_1}) \ket{\psi} }{\sqrt{\lambda_1}}
\end{align*}

Then
\begin{align*}
	\ket{\psi} = \sum_{i=0}^1 \sqrt{\lambda_i} \ket{a_i} \ket{\lambda_i}.
\end{align*}


(It's too tiresome to calculate $\ket{a_i}$)



\Textbf{2.80}

Let $\ket{\psi} = \sum_i \lambda_i \ket{\psi_i}_A \ket{\psi_i}_B$ and $\ket{\varphi} = \sum_i \lambda_i \ket{\varphi_i}_A \ket{\varphi_i}_B$.

Define $U = \sum_i \kbt{\psi_j}{\varphi_j}_A$ and $V = \sum_j \kbt{\psi_j}{\varphi_j}_B$.

Then
\begin{align*}
	(U \otimes V) \ket{\varphi} &= \sum_i \lambda_i U \ket{\varphi_i}_A  V \ket{\varphi_i}_B\\
		&= \sum_i \lambda_i \ket{\psi_i}_A \ket{\psi_i}_B\\
		&= \ket{\psi}.
\end{align*}


\Textbf{2.81}

Let the Schmidt decomposition of $\ket{AR_1}$ be $\ket{AR_1} = \sum_i \sqrt{p_i} \ket{\psi_i^A} \ket{\psi_i^R}$ and
let $\ket{AR_2} = \sum_i \sqrt{q_i} \ket{\phi_i^A} \ket{\phi_i^R}$.

Suppose $\rho^A$ has orthonormal decomposition $\rho^A = \sum_i p_i \kb{i}$.

Since $\ket{AR_1}$ and $\ket{AR_2}$ are purifications of the $\rho^A$, we have
% \begin{align*}
%     \ket{AR_1} &= \sum_i \sqrt{p_i} \ket{i} \ket{\psi_i}\\
%     \ket{AR_2} &= \sum_i \sqrt{p_i} \ket{i} \ket{\phi_i}
% \end{align*}
% where $\ket{\psi_i}$ and $\ket{\phi_i}$ are orthonormal bases on system $R$.
%
\begin{align*}
    \Tr_R (\kb{AR_1}) = \Tr_R (\kb{AR_2}) = \rho^A\\
    \therefore \sum_i p_i \kb{\psi_i^A} = \sum_i q_i \kb{\phi_i^A} = \sum_i \lambda_i \kb{i}.
\end{align*}

The $\ket{i}$, $\ket{\psi_i^A}$, and $\ket{\psi_i^A}$ are orthonormal bases and they are eigenvectors of $\rho^A$.
Hence without loss of generality, we can consider
\begin{align*}
    \lambda_i = p_i = q_i \text{ and } \ket{i} = \ket{\psi_i^A} = \ket{\phi_i^A}.
\end{align*}
%
Then
\begin{align*}
    \ket{AR_1} = \sum_i \lambda_i \ket{i} \ket{\psi_i^R}\\
    \ket{AR_2} = \sum_i \lambda_i \ket{i} \ket{\phi_i^R}
\end{align*}
Since $\ket{AR_1}$ and $\ket{AR_2}$ have same Schmidt numbers, there are two unitary operators $U$ and $V$ such that
$\ket{AR_1} = (U \otimes V) \ket{AR_2}$ from exercise 2.80.

Suppose $U = I$ and $V = \sum_i \kbt{\psi_i^R}{\phi_i^R}$.
Then
\begin{align*}
    \left(I \otimes \sum_j \kbt{\psi_j^R}{\phi_j^R} \right) \ket{AR_2} &= \sum_i \lambda_i \ket{i} \left( \sum_j \ket{\psi_j^R} \braket{\phi_j^R | \phi_i^R} \right)\\
                                                           &= \sum_i \lambda_i \ket{i} \ket{\psi_i^R}\\
                                                           &= \ket{AR_1}.
\end{align*}
%
Therefore there exists a unitary transformation $U_R$ acting on system $R$ such that $\ket{AR_1} = (I \otimes U_R) \ket{AR_2}$.


\Textbf{2.82}

(1)

Let $\ket{\psi} = \sum_i \sqrt{p_i} \ket{\psi_i} \ket{i}$.
\begin{align*}
    \Tr_R (\kb{\psi})
        &= \sum_{i,j} \sqrt{p_i} \sqrt{p_j} \kbt{\psi_i}{\psi_j} \Tr_R (\kbt{i}{j})\\
        &= \sum_{i,j} \sqrt{p_i} \sqrt{p_j} \kbt{\psi_i}{\psi_j} \delta_{ij}\\
        &= \sum_i p_i \kb{\psi_i} = \rho.
\end{align*}
Thus $\ket{\psi}$ is a purification of $\rho$.

\vspace{5mm}
(2)

Define the projector $P$ by $P = I \otimes \kb{i}$.
The probability we get the result $i$ is
\begin{align*}
    \Tr \left[ P \kb{\psi}\right] = \braket{\psi | P | \psi} = \braket{\psi | (I \otimes \kb{i}) | \psi} = p_i \braket{\psi_i | \psi_i} = p_i.
\end{align*}

The post-measurement state is
\begin{align*}
    \frac{P \ket{\psi}}{\sqrt{p_i}}
    = \frac{(I \otimes \kb{i}) \ket{\psi}}{\sqrt{p_i}}
        = \frac{\sqrt{p_i} \ket{\psi_i}\ket{i}}{\sqrt{p_i}} = \ket{\psi_i}\ket{i}.
\end{align*}

If we only focus on the state on system $A$,
\begin{align*}
    \Tr_R (\ket{\psi_i} \ket{i}) = \ket{\psi_i}.
\end{align*}

\vspace{5mm}
(3)

($\{ \ket{\psi_i} \}$ is not necessary an orthonormal basis.)


Suppose $\ket{AR}$ is a purification of $\rho$ and its Schmidt decomposition is $\ket{AR} = \sum_i \sqrt{\lambda_i} \ket{\phi_i^A} \ket{\phi_i^R}$.

From assumption
\begin{align*}
    \Tr_R \left( \kb{AR} \right) = \sum_i \lambda_i \kb{\phi_i^A} = \sum_i p_i \kb{\psi_i}.
\end{align*}

By theorem 2.6, there exits an unitary matrix $u_{ij}$ such that $\sqrt{\lambda_i}\ket{\phi_i^A} = \sum_j u_{ij} \sqrt{p_j} \ket{\psi_j}$.
Then
\begin{align*}
    \ket{AR} &= \sum_i \left( \sum_j u_{ij} \sqrt{p_j} \ket{\psi_j} \right) \ket{\phi_i^R}\\
             &= \sum_j \sqrt{p_j} \ket{\psi_j} \otimes\left( \sum_i u_{ij} \ket{\phi_i^R} \right)\\
             &= \sum_j \sqrt{p_j} \ket{\psi_j} \ket{j}\\
             &= \sum_i \sqrt{p_i} \ket{\psi_i} \ket{i}
\end{align*}
where $\ket{i} = \sum_k u_{ki} \ket{\phi_k^R}$.

About $\ket{i}$,
\begin{align*}
    \braket{k | l} &= \sum_{m,n} u_{mk}^* u_{nl} \braket{\phi_m^R | \phi_n^R }\\
        &= \sum_{m,n} u_{mk}^* u_{nl} \delta_{mn}\\
        &= \sum_m u_{mk}^* u_{ml}\\
        &= \delta_{kl}, ~~~(\because u_{ij} \text{ is unitary.})
\end{align*}
which implies $\ket{j}$ is an orthonormal basis for system $R$.

Therefore if we measure system $R$ w.r.t $\ket{j}$, we obtain $j$ with probability $p_j$ and post-measurement state for $A$ is $\ket{\psi_j}$ from (2).
Thus for any purification $\ket{AR}$, there exists an orthonormal basis $\ket{i}$ which satisfies the assertion.


\Textbf{Problem 2.1}

From Exercise 2.35, $\vec{n} \cdot \vec{\sigma}$ is decomposed as
\begin{align*}
	\vec{n} \cdot \vec{\sigma} &= \kb{\lambda_1} - \kb{\lambda_{-1}}
\end{align*}
where $\ket{\lambda_{\pm 1}}$ are eigenvector of $\vec{n} \cdot \vec{\sigma}$ with eigenvalues $\pm 1$.

Thus
\begin{align*}
	f(\theta \vec{n} \cdot \vec{\sigma}) &= f(\theta) \kb{\lambda_1} + f(- \theta) \kb{\lambda_{-1}}\\
		&= \left( \frac{f(\theta) + f(-\theta)}{2} + \frac{f(\theta) - f(-\theta)}{2}  \right) \kb{\lambda_1} + \left( \frac{f(\theta) + f(-\theta)}{2} - \frac{f(\theta) - f(-\theta)}{2}  \right)\kb{\lambda_{-1}}\\
		&= \frac{f(\theta) + f(-\theta)}{2} \left( \kb{\lambda_1} + \kb{\lambda_{-1}} \right) +  \frac{f(\theta) - f(-\theta)}{2} \left(\kb{\lambda_1} - \kb{\lambda_{-1}} \right)\\
		&= \frac{f(\theta) + f(-\theta)}{2} I + \frac{f(\theta) - f(-\theta)}{2} \vec{n} \cdot \vec{\sigma}
\end{align*}

\Textbf{Problem 2.2}
Unsolved

\Textbf{Problem 2.3}
Unsolved
