%!TeX root=../solnQCQI.tex

\chapter{Introduction to computer science}
\Textbf{3.1} \textbf{(Non-computable processes in Nature)} How might we recognize that a process in Nature computes a function non computable by a Turing machine?
\Soln There are several well known non-Turing-computable functions which if identified to be computable by a process in nature would provide examples.  For instance, the Halting problem: \url{https://en.wikipedia.org/wiki/Halting_problem}.   More specifically, since Turing machines map non-negative integers to non-negative integers, their input and output spaces are countable (\url{https://en.wikipedia.org/wiki/Countable_set}).  If any process in nature was found to compute a function taking input or providing output from an uncountable space, this could not be computed using a Turing machine.  Note, Turing machines could compute the function within any desired level of approximation, but could not compute the function exactly.

\Textbf{3.2} \textbf{(Turing numbers)}  Show that single-tape Turing machines can each be given a number from a list 1,2,3,\ldots in such a way that the number uniquely specifies the corresponding machine.  We call this number the \textit{Turing number} of the corresponding machine.  (\textit{Hint}:  Every positiive integer has a unique prime factorizatization $p_1^{a_1}p_2^{a_2}\ldots p_k^{a_k}$, where $p_i$ are distinct prime numbers and $a_1,\ldots, a_k$ are non-negative integers.)
\Soln Per the hint, we show that a Turing machine can be encoded uniquely be a finite ordered list of integer values $[a_1, a_2, \ldots, a_k]$.  Unique prime factorization can be used to encode the Turing machine as the non-negative integer $\prod_i p_i^{a_i}$, where $p_i$ is the $i$-th prime, starting with $p_1 = 2, p_2 = 3,\ldots$. A non-negative integer corresponding to a Turing machine can then be decoded to reproduce the unique Turing machine whose encoding gives rise to it via the exponents in it's unique prime factorization. What follows is likely overly detailed for some.  The basic idea is that each part of the the Turing machine can be encoded in a finite sequence of non-negative integers and decoded from that sequence.  Concatenating those sequences (carefully) is then enough to specify the Turing machine.  Note, it will not be the case that all non-negative integers correspond to valid Turing machines, but this is not required.  We'll extend this encoding to encode Turing machines in operation, and explain how operation of a Turing machine can be simulated by multiplication of it's Turing number by a rational number determined conditionally by the Turing number itself.  [Note: it is unclear how useful this extension will be. The idea is relatively simple, but its formal specification is intricate and very much not necessary to understand.  Feel free to skip it.] 

To produce an encoding of a Turing machine, we encode each of it's elements separately.  We start with the finite state control.  The finite state control consists of a finite set of $m+2$ states $Q = \{q_s, q_1, \ldots, q_m, q_h\}$.  Individually, it doesn't matter what form the $q_i$ take, only that they are distinguishable.  The integers $0, 1,\ldots, m, m+1$ are distinguishable, so all that is required to encode a finite state machine is a single integer.  So, setting $a_1=m$  allows $a_1$ to track the size of the finite state machine and is enough to encode it.

To encode the tape, we let $a_2$ be the size of the alpthabet $\Gamma$: $a_2 = |\Gamma|$, where here $\Gamma$  includes the starting character $\triangleright$, corresponding to tape value 0, and blank character $b$ corresponding to tape value $a_2-1$.  The other states may be assumed to be non-negative integers $1, \ldots, a_2-2$.  To encode the entirety of the tape, note that only a finite number of squares are non-blank.  Let $\beta$ be the largest index of a non-blank tape square.  Then set $a_3 = \beta$, and for each tape square with index $i$, for $1\leq i\leq \beta$, set $a_{3+i}$ equal to the non-negative integer value assigned to the alphabet character occupying tape square $i$.  All tape squares with index more than $\beta$ are blank and need not be encoded.   Note that by construction $a_4=0$ for all Turing machines, since tape square 1 always contains $\triangleright$, which was assigned value 0.  

Next, we encode the program.    The program contains a finite ordered list of program lines, say $\pi$ of them.  Set $a_{3+\beta+1} = \pi$.  For $1\leq i \leq \pi$, we encode program line $i$ with a second prime factorization. Program line $i$ consists of 5 elements: $\braket{q_i, x_i, q_i', x_i', s_i}$.  Here, $q_i$ and $q_i'$ are states in $Q$ which can be indexed with non-negative integers, say $\ell_{i,1}$ and $\ell_{i,3}$, with $0\leq \ell_{i,1},\ell_{i,3}\leq m+1$.  $x_i$ and $x_i'$ are characters in the alphabet $\Gamma$ which can be indexed with non-negative integers, say $\ell_{i,2}$ and $\ell_{i,4}$, with $0\leq \ell_{i,2}, \ell_{i,4} < |\Gamma| (= a_2)$.  $s_i$ is an integer value that is either -1, 0, or 1.  Setting $\ell_{i,5} = s_i$ directly leaves open the possibility that $\ell_{i,5} = -1$, which in turn will yield non-integer encodings of the program line.  There are several ways to circumvent this, the likely easiest of which is to set $\ell_{i,5} = s_i+1$. However, the author prefers setting $\ell_{i,5} = s_i~\%~3$, the remainder of $s_i$ when divided by 3 (its residue modulo 3).   This allows $s_i=0$ and $s_i=1$ to be encoded as $\ell_{i,5}=0$ and $\ell_{i,5}=1$, which are natural Boolean indicators that the tape-head should advance to the right, but requires $s_i=-1$ be encoded as $\ell_{i,5}=2$, indicating that the tape-head should move to the left.  Now, to encode program line $i$, for $1\leq i\leq \pi$, set $a_{3+\beta+1+i} = 2^{\ell_{i,1}}\cdot3^{\ell_{i,2}}\cdot5^{\ell_{i,3}}\cdot7^{\ell_{i,4}}\cdot11^{\ell_{i,5}}$.  

Now, for a Turing machine $M$, assigning Turing number $\tau(M) = \prod\limits_{i=1}^{3+\beta+1+\pi} p_i^{a_i}$ produces an integer encoding.  To show that it is unique, we reverse the encoding process and argue that all pieces of the Turing machine can be recovered uniquely from this integer value.  Let an encoding of a Turing machine, $\tau(M)$, be given and begin with its unique prime factorization $\tau(M) = \prod\limits_{i=1}^{\omega(\tau(M))} p_i^{a_i}$, where here $\omega$ is a function that returns the largest index of a prime that divides input integer.  [Note, $a_4=0$ will mean that this isn't the number of distinct prime factors].  Immediately, we recover the size of the finite state machine, \textit{i.e.} $m$.  It contains $a_1$ states indexed by integers, along with the special starting and halting state $q_s$ and $q_h$.  Next, the size of the alphabet $\Gamma$ is given by $a_2$, where here $\Gamma$ includes the starting character $\triangleright$ and the blank character $b$.  Next, the encoding of the tape starts with $a_3=\beta$ which indicates the maximum index of a non-blank tape square.  $\beta$ encodings of tape squares follow, starting with $a_4=0$, indicating that tape square 1 contains the starting character $\triangleright$, which was assigned character value 0.  If $a_4\neq0$, the integer provided could not be an encoding of a Turing machine, violating the assumption that $\tau(M)$ was such an integer.  $a_{3+i}$ encodes the value stored on the tape at index $i$, for $i\leq i \leq \beta$, where $a_{3+i}=a_2-1$ indicates the tape square $i$ is blank.  All tape squares with index $i$, for $i > \beta$, are assumed to be blank.  It is left only to decode the program.  We start with its length $\pi = a_{3+\beta+1}$.  $\pi$ encodings of individual program lines should follow, each of which should be of the form $a_{3+\beta+1+i} = 2^{\ell_{i,1}}\cdot3^{\ell_{i,2}}\cdot5^{\ell_{i,3}}\cdot7^{\ell_{i,4}}\cdot11^{\ell_{i,5}}$, from which we can recover $q_i = \ell_{i,1}$, $x_i = \ell_{i,2}$, $q_i' = \ell_{i,3}$, $x_i'=\ell_{i,4}$, and $s_i=\ell_{i,5}~\widetilde{\%}~3$, where here $\widetilde{\%}~3$ is modular reduction on to the set of residues $-1,0$, and $1$, instead of the standard set of residues $0,1,2$.  Note that $r~\widetilde{\%}~3 = ((r+1)~ \% ~3) - 1$.  It is easy to see that the program line encoded by $a_{3+\beta_1+i}$ is uniquely determined, as is the initial state of the tape from $a_3,\ldots, a_{3+\beta}$.  The alphabet $\Gamma$ is uniquely determined by $a_2$, and the finite state machine is uniquely determined by its size, given by $a_1$.  So, the entirety of the Turing machine $M$ can be uniquely recovered from it's Turing number $\tau(M)$, so $\tau(M)$ is unique. 

\noindent \textbf{(Extension):} Note that, as defined, our encoding uniquely encodes Turing machines in their initial state $q_s$, with read-write tape-head positioned on tape square 1 holding the starting character $\triangleright$.  The encoding scheme could be extended to encode Turing machines in operation by adding an encoding of the current state in the finite state control and current position of the read-write tape-head which will require only two additional prime factors and exponents.  For compatibility, to encode the current state of the finite state machine, we use $a_{3+\beta+1+\pi+1}$, where $a_{3+\beta+1+\pi+1} = 0$ indicates the state machine is in starting state $q_s$, and $a_{3+\beta+1+\pi+1} = m+1$ indicates the state machine is in state $q_h$ and has halted.  To encode the position of the read-write tape-head we require one more additional prime factor and exponent. For compatibility, we use $a_{3+\beta+1+\pi+2}$.  Full compatibility of encoding will require $a_{3+\beta+1+\pi+2}=0$ to indicate that the machine is not yet operating and that the read-write tape-head has not yet been positioned on a tape square, neither tape square 1 holding $\triangleright$, as encoded by $a_4=0$, or another subsequent tape square holding any other value.  Having $a_{3+\beta+1+\pi+2}>0$ indicates that the Turing machine $M$ is in operation in state specified by $a_{3+\beta+1+\pi+1}$, which we'll call $\sigma$, and read-write tape-head on the tape square specified by $a_ {3+\beta+1+\pi+2}$, which we'll call $\sigma$.  Note then that the tape-square pointed to by the read-write tape-head would contain the value specified by $a_{3+a_ {\sigma}}$, which we'll call $\nu$.

Now, to simulate execution of the Turing machine, note that in each step the program list is searched for a pattern matching it's current state and the character in the tape square being read by the read-write tape-head, that is, for $\braket{\sigma,\nu,\cdot,\cdot,\cdot}$.  This is equivalent to searching $a_{4+\beta+1},\ldots,a_{4+\beta+\pi}$ for an integer divisible by $2^{\sigma}\cdot 3^{\nu}$, but no more powers of 2 or 3. Once a matching $a_{4+\beta+i}$ is found, the multiplicities of $5, 7$, and $11$ in its factorization will give values for $\ell_{i,3}, \ell_{i,4}$, and $\ell_{i,5}$. The finite state machine can then be updated by multiplying by $p_{3+\beta+1+\pi+1}^{\ell_{i,3}-\sigma}$.  The contents of the tape can be updated by multiplying by $p_{3+a_\sigma}^{\ell_{i,4}-\nu}$.  The read-write tape-head can be moved by multiplying by $p_{3+\beta+1+\pi+2}^{\ell_{i,5}~\widetilde{\%}~3 }$.  Doing so will change the Turing number of the machine in operation $M$ to the number encoding $M$ after a single step.

\Textbf{3.3} \textbf{(Turing machine to reverse a bit string)}  Describe a Turing machine which takes a binary number $x$ as input, and outputs the bits of x in reverse order. (\textit{Hint}: In this and the next exercise it may help to use a mutli-tape Turing machine and/or symbols other than $\triangleright$, 0, 1, and the blanks.)
\Soln By ``takes a binary number $x$ as input'', what is meant is the non-blank portion of the tape contains the value $x$, encoded somehow.  In general, the tap can hold more than just function input, but for this problem that won't be necessary.  We'll use a two-tape machine, with both tapes containing symbols from the alphabet $\triangleright, 0, 1, b$.  Tape 1 will contain $\triangleright$, followed by the input value $x$ in binary, followed by blanks indicated with $b$s.  The second tape will contain $\triangleright$ and blanks.  The Turing machine will populate the second tape with the reversed binary value of $x$, followed by blanks.  It will not clear the first tape (although that could be done without too much trouble).  Before we define the program, we specify that the finite state machine will contain 4 states, the starting state $q_s$, the halted state $q_h$, a search state $s$, and a write state $w$.  Now, consider the program
\begin{align*}
P=\left\{
\begin{array}{ccccccccrrr}     1: &\langle&q_s,&\triangleright,&\triangleright,&s,&\triangleright,&\triangleright,&+1,&0&\rangle\\
       2: &\langle&s,& 0,&\triangleright,&s,&0,&\triangleright,&+1,&0&\rangle \\
       3: &\langle&s,& 1,& \triangleright,& s,& 1,& \triangleright,& +1,&0&\rangle \\
       4: &\langle&s,& b,& \triangleright,& w,& b,& \triangleright,& -1,& +1&\rangle \\
       5: &\langle&w,& 0,& b,& w,& 0,& 0,& -1,& +1&\rangle \\
       6: &\langle&w,& 1,& b,& w,& 1,& 1,& -1,& +1&\rangle \\ 
       7: &\langle&w,& \triangleright,& b,& q_h,& \triangleright,& b,& 0,& 0&\rangle
       \end{array}\right.
\end{align*}
Execution of the Turing machine begins by executing line 1 of P, which moves tape-head 1 forward, leaves tape-head 2 in place, and sets the state of the finite state machine to the search state. While in the search state, the program operates by executing lines 2 and 3, advancing tape-head 1 leaving the content of tape 1 unchanged, until it reaches a blank indicating that the end of the input has been reached, finally matching line 4.  Once the blank on tape 1 is reached, line 4 changes the finite state machine to the write state, shifts tape-head 1 to the last bit of input, and advancing tape-head 2 to the first position in tape 2.  Until the start of tape 1 is encountered, the program operates by executing lines 5 and 6, each of which copies the character on tape 1 pointed to be tape-head 1 onto tape 2 in the position pointed to by tape-head 2.  It then moves the tape-heads in opposite directions so that tape-head 1 points to the preceeded bit of input and tape-head 2 points to the next bit of output.  When the start of tape 1 is encountered, line 7 explicitly halts the program.   Explicitly halting is not necessary in this case.  Note, tape 1 could be cleared by replacing $x_1'$ in lines 5 and 6 with $b$s.  Then, the output could be moved to tape 1 while simultaneously clearing tape 2 by replacing line 7 with:
\begin{align*}
\begin{array}{rccccccrrrr}     7: \langle&w,& \triangleright,& b,& w,& \triangleright,& b,& 0,& -1&\rangle \\
        8: \langle&w,& \triangleright,& 0,& w,& \triangleright,& 0,& 0,& -1&\rangle \\
        9: \langle&w,& \triangleright,& 1,& w,& \triangleright,& 1,& 0,& -1&\rangle \\
        10: \langle&w,& \triangleright,& \triangleright,& w,& \triangleright,& \triangleright,& +1,& +1&\rangle \\
        11: \langle&w,&  b,& 1,& w,& 1,& b,& +1,& +1&\rangle \\
        12: \langle&w,&  b,& 0,& w,& 0,& b,& +1,& +1&\rangle \\
        13: \langle&w,&  b,& b,& q_h,& b,& b,& 0,& 0&\rangle \\
\end{array}
\end{align*}
Here, line 7 reverses the direction of tape-head 2. Lines 8 and 9 allow it to retreat to the start of tape 2 in line 10, at which point tape-head 1 and 2 are advanced in tandem and the blank in tape 1 is swapped with the bit in tape 2, one bit at a time, by executing lines 11 and 12.  Once tape-head 2 is at the end of the reversed bit-string, line 13 is reached, explicitly halting the program.
        
\Textbf{3.4} \textbf{(Turing machine to add modulo 2)} Describe a Turing machine to add two binary numbers $x$ and $y$ modulo 2.  The numbers are input on the Turing machine tape in binary, in the form $x$, followed by a single blank, followed by $y$.  If one number is not as long as the other then you may assume that it has been padded with leading 0's to make the two numbers the same length.
\Soln The specification that $x$ and $y$ can be padded so that they have the same length clouds the interpretation of the exercise.   If adding $x$ and $y$ modulo 2 means finding the parity of $x+y$, the more natural interpretation to a mathematician, then a rather natural machine achieves this without padding.  Alternatively, adding $x$ and $y$ modulo 2 could mean $x\wedge y$.  Here, padding would be convenient.  We'll start with the first interpretation, the parity of $x+y$, that is $x+y\pmod{2}$. Here, only the last bits of $x$ and $y$ matter.

We define a single-tape machine, using the standard alphabet $\Gamma=\{\triangleright, 0, 1, b\}$.  For convenience, define a set of states $S=\{s, 0, 1, h\}$ and let the finite state control consist of (a subset of) $S\bigoplus S$, the Cartesian product of $S$ with itself, with $q_s\equiv (s,s)$ and $q_h\equiv (h,h)$.  This will allow the state to represent a bit from $x$ and a bit from $y$ simultaneously.  For convenience, let a $*$ in a state contained within a program line be a wildcard, where a $*$ will only occur in an output state if one also occurred in the input state in the same coordinate, in which case the coordinate state is left unchanged in the output state.  For added convenience, let the $\#$ wildcard represent a 0 or 1 character read from the tape, and a corresponding 0 or 1 state in a coordinate of the output state.  The $\#$ character will never be used as an output character to be written to the tape.  The program below could be specified without wildcards by replicating the program lines containing them, producing a fully specified program line for each value the wildcards could represent.  If multiple wildcards occur in a line, the result would be distinct fully specified program lines, one for each pair of state, character pair in $S\bigoplus\{0,1\}$.
\begin{align*}
P=\left\{
\begin{array}{cccccrrr}     1: &\langle&(s,s),&\triangleright,&(s,s),&\triangleright,&+1&\rangle\\
       2: &\langle&(*,s),&\#,&(\#,s),&b,&+1&\rangle \\
       3: &\langle&(*,s),&b,&(*,b),&b,&+1&\rangle \\
       4: &\langle&(*,b),&\#,&(*,\#),&b,&+1&\rangle \\
       5: &\langle&(*,0),&\#,&(*,\#),&b,&+1&\rangle \\
       6: &\langle&(*,1),&\#,&(*,\#),&b,&+1&\rangle \\
       7: &\langle&(*,0),&b,&(*,0),&b,&-1&\rangle \\
       8: &\langle&(*,1),&b,&(*,1),&b,&-1&\rangle \\
       9: &\langle&(0,0),&\triangleright,&(0,h),&\triangleright,&+1&\rangle \\
       10: &\langle&(0,1),&\triangleright,&(1,h),&\triangleright,&+1&\rangle \\
       11: &\langle&(1,0),&\triangleright,&(1,h),&\triangleright,&+1&\rangle \\
       12: &\langle&(1,1),&\triangleright,&(0,h),&\triangleright,&+1&\rangle \\
       13: &\langle&(0,h),&b,&(h,h),&0,&0&\rangle \\
       14: &\langle&(1,h),&b,&(h,h),&1,&0&\rangle \\
       \end{array}\right.
\end{align*}
Line 1 initializes the program, advancing the tape head to the first bit of $x$.  Line 2 iteratively stores the last bit of $x$ read from the tape in the first coordinate of the state and erases the bit from the tape, advancing the tape head to the next bit of $x$.  Line 3 identifies the end of $x$ and advances to the first bit of $y$.  Lines 4-6 iteratively store the last bit of $y$ read from the tape in the second coordinate of the state and erases the bit from the tape, advancing the tape head to the next bit of $y$.  Lines 7 and 8 identify the end of $y$, then iteratively returns the tape head to the start of the tape, since the tape has been erased entirely except for the starting $\triangleright$ character, which triggers one of lines 9-12 when read.  While the tape head is returning to the start of the tape, and in particular, when it reaches it, the current state contains the last bit of $x$ and the last bit of $y$ by construction.  In lines 9-12, these bits are XORed, the tape head is advanced to the first position (which holds the blank character), and the output state is set equal to the XOR paired with $h$ in the second coordinate indicating halting is in progress.  The XOR is the parity of $x+y$ that needs to be output onto the tape.  Once the second state-coordinate indicates halting, the XOR is written to the tape in the current/first character, replacing the blank that was there, and the output state is explicitly set to $(h, h)\equiv q_h$, the halting state of the Cartesian product finite state control. 

To compute $x\wedge y$ we use a two-tape machine.  The finite state control will consist of the states $\{q_s, s, m, r, w, q_h\}$, which we'll refer to as begin, search, move, return, write, and halt.  The standard alphabet $\Gamma=\{\triangleright, 0, 1, b\}$ will be sufficient.  Once again, we use $\#$ wildcards, where now a subscript will indicate which tape the character was read from, and $\#_1\wedge\#_2$ is the XOR of the binary integers represented by the characters $\#_1$ and $\#_2$ read from the respective tapes.  Consider the program:
\begin{align*}
P=\left\{
\begin{array}{ccccccccrrr}     1: &\langle&q_s,&\triangleright,&\triangleright,&s,&\triangleright,&\triangleright,&+1,&0&\rangle\\
       2: &\langle&s,&\#_1,&\triangleright,&s,&\#_1,&\triangleright,&+1,&0&\rangle\\
       3: &\langle&s,&b,&\triangleright,&m,&b,&\triangleright,&+1,&+1&\rangle\\
       4: &\langle&m,&\#_1,&b,&m,&b,&\#_1,&+1,&+1&\rangle\\
       5: &\langle&m,&b,&b,&r,&b,&b,&-1,&-1&\rangle\\
       6: &\langle&r,&b,&\#_2,&r,&b,&\#2,&-1,&-1&\rangle\\
       7: &\langle&r,&b,&\triangleright,&r,&b,&\triangleright,&-1,&0&\rangle\\
       8: &\langle&r,&\#_1,&\triangleright,&r,&\#_1,&\triangleright,&-1,&0&\rangle\\
       9: &\langle&r,&\triangleright,&\triangleright,&w,&\triangleright,&\triangleright,&+1,&+1&\rangle\\
       10: &\langle&w,&\#_1,&\#_2,&w,&\#_1\wedge\#_2,&b,&+1,&+1&\rangle\\
       11: &\langle&w,&b,&b,&q_h,&b,&b,&0,&0&\rangle\\
       \end{array}\right.
\end{align*}
Line 1 starts the program, transitioning the machine to the search state, advancing the first tape to the first bit of $x$.  Here, ``search'' is searching for the start of $y$.  While in the search state, line 2 causes tape head 2 to stay in place, advances tape head 1, and leaves the contents of $x$ unchanged on tape 1.  Once tape head 1 reaches the end of $x$, line 3 transitions the finite state control to the move state, and advances both tape heads. In the move state, the bits of $y$ are iteratively moved from tape 1 to tape 2, erasing tape 1, by executing line 4.  Since $x$ and $y$ have the same length, this aligns $x$ and $y$ in their respective tapes, with corresponding bits in tape squares with equal indices.  When tape head 1 reaches the end of $y$, line 5 sets the state to return, and retreats both tapes.  Line 6 is then executed iteratively, retreating tape head 1 across the erased portion of tape 1 which previously held $y$, and tape head 2 across the portion of tape 2 that \textit{now}  holds $y$, leaving $y$ in place.  Since $x$ and $y$ have the same length, we reach the blank which previously separated $x$ and $y$ on tape 1 and the $\triangleright$ on tape 2 at the same time.  This executes line 7, which transitions the machine to only retreating tape head 1, while leaving tape head 2 in place on the starting $\triangleright$ character.  Then line 8 is executed iteratively, continuing to retreat tape head 1 to its start.  When both tape heads have returned to their starts, we execute line 9, transitioning into the write state and move both tape heads forward: tape head 1 to the first bit of $x$ and tape head 2 to the first bit of $y$.  While in the write state,executing  line 10 writes the bitwise XOR of $x$ and $y$ onto tape 1 and erases tape 2, advancing both tape heads in turn.  Since $x$ and $y$ have the same length, we are guaranteed to reach the blank characters at the ends of $x$ and $y$ at the same time, causing line 11 to be executed, explicitly halting the program, leaving tape 1 containing $x\wedge y$ and tape 2 blank.

\begin{comment}
\Textbf{3.5} \textbf{(Halting problem with no inputs)} Show that given a Turing machine $M$ there is no algorithm to determine whether $M$ halts when the input to the machine is a blank tape.
\Soln There is a minor sticking point in the way this question is asked.  It does not ask us to show that there is no \textit{Turing machine} with this capability.  It asks us to show that there is no \textit{algorithm}.  Let us assume for a contradiction that such an algorithm did exist.  To answer the question as asked, we must think of the algorithm in a general sense and not assume it is Turing computable.  Let $f$ be a function that takes a Turing machine with no input, $M$, and returns $1$ if $M$ halts and $0$ if not.  This function is well-defined by hypothesis, and for any fixed Turing machine can be determined.  Now, consider a Turing machine (which will tak If allowed to assume by contradiction that the algorithm takes the form of a Turing machine, say $\mathcal{M}$, which takes $M$ as input like in a universal Turing machine,  then define the function  , then 

To that end, let us assume such an algorithm to determine whether a Turing machine $M$ halted when given no input existed.  By exercise 3.2, each Turing machine can be assigned a unique Turing number, $\tau(M)$.  For $n\in\mathbb{N}$, define $\mathcal{M}_n$ to be the set of all Turing machines $M$ such that $\log_2(\tau(M)) \leq n$, and $M$ halts when given no input.  The existence of the hypothesized algorithm guarantees that $\mathcal{M}_n$ is well-defined, and nothing more.  When $M\in\mathcal{M}_n$ halts (when given no input), it will have calculated a number as output, say $\nu(M)$.  Let $f:\mathbb{N}\rightarrow\mathbb{N}$ be a function that calculates the smallest integer not computed by any Turing machine in $\mathcal{M}_n$.  That is, $f(n) \equiv \min(\mathbb{N}\setminus\{\nu(M)|M\in\mathcal{M}_n\})$.  $f$ is Turing computable, for instance by a universal Turing machine. 


\end{comment}

\Textbf{3.8} \textbf{(Universality of} \NAND\textbf{)} Show that the \NAND{} gate can be used to simulate \AND, \XOR, and \NOT{} gates, provided wires, ancilla bits and \FANOUT{} are available

NOT:
\begin{circuitikz}
    \draw
    (1.5,0) node[nand port] (mynand){}
    (0,0) |- (mynand.in 1)
    (0,0) |- (mynand.in 2)
    (-0.25,0) to[short, -*] (0,0);
\end{circuitikz}

AND:
\begin{circuitikz}
\draw
    (1,0) node[nand port] (mynand1){}

    (2.60,0) node[nand port] (mynand2){}
    (1.20,0) |- (mynand2.in 1)
    (1.20,0) |- (mynand2.in 2)
    (1.25,0) to[short, -*] (1.20,0);
    
%    \draw
%    (1,0) node[nand port] (mynand1){}
%    (3,0) node[nand port] (mynand2){}
%    (mynand2.in 1) -- ++ (-2,0) |- (mynand.in 2) coordinate (a) to[short, *-] (0.5,0.5|-a);
%    (2,0) node[circ port] (fanout){}
%    (mynand1.out 1) |- (fanout.in)
%    (fanout.out 1) |- (mynand2.in 1)
%    (fanout.out 2) |- (mynand2.in 2);
%    (mynand2.in 1) |- (mynand1.out)
%    (mynand2.in 2) |- (mynand1.out);
%%     (mynand2.in 1) |- coordinate[pos=1] (a)
%     (mynand2.in 2) |- coordinate[pos=1](a)
%     (mynand1.out) to[short, *-] (a);
%    (0,1) to[short, -*] (2,0);
    \end{circuitikz}
    
%XOR:
%\begin{circuitikz}
%\draw


\Textbf{(Landau Big-O notation} A brief note about wording:  Section 3.2.1 says that $f(n)$ \textit{is} $O(g(n))$, but this leads to a temptation to write equations like $f(n) = O(g(n))$.  Such equations aren't reflexive though; writing$O(g(n)) = f(n)$ doesn't make sense.  We'll say that$f(n)$ is \textit{in} $O(g(n))$, where $O(g(n))$ is taken to be a \textit{class} of functions, as originally presented. So, \textit{in} refers to class membership and may be written $f(n)\in O(g(n))$.  Also, when a parameter is used inside the $O, \Omega,$ or $\Theta$, it is unnecessary to include the parameter in statements about function's membership in such a class.  For example, if $f(n)\equiv n$, then $f\in\Theta(n)$ is unambiguous.

\Textbf{3.9} Prove that $f(n)$ is $O(g(n))$ if and only if $g(n)$ is $\Omega(f(n))$. Deduce that $f(n)$ is $\Theta(g(n))$ if and only if $g(n)$ is $\Theta(f(n))$.
\Soln Let us assume that $f(n)\in O(g(n))$.  By definition there exists $c$ and $n_0$ such that for all $n>n_0$, $f(n) \leq c\cdot g(n)$.  Dividing by $c$ and setting $c'=\frac{1}{c}$ yields that there exists $c'$ and (the same) $n_0$ such that for all $n > n_0$, $c'\cdot f(n) \leq g(n)$.  This is exactly the defining property of the statement: $g(n)\in\Omega(f(n))$, so $f(n)\in O(g(n))$ implies $g(n)\in\Omega(f(n))$.  to prove the converse, assume that $g(n)\in\Omega(f(n))$, divide by $c'$ to re-recover $c$, and recognize the definition of $f(n)\in O(g(n))$.

Now, to show that $f(n)\in\Theta(g(n))$ if and only if $g(n)\in\Theta(f(n))$, note that by definition$f(n)\in\Theta(g(n))$ means that $f(n)\in O(g(n))$ and $f(n)\in\Omega(g(n))$, which by the first part of the exercise means that $g(n)\in \Omega(f(n))$ and $g(n)\in O(f(n))$, which is exactly the definition of $g(n)\in\Omega(f(n))$.

\Textbf{3.10} Suppose $g(n)$ is a polynomial of degree $k$.  Show that $g(n)$ is $O(n^\ell)$ for any $\ell\geq k$.
\Soln Let $k$ be fixed and $\ell\geq k$ be given.  Let $g(n) \equiv \sum_{i=0}^k c_i n^i$, define $c \equiv 2\cdot\sum_{j=0}^k \abs{c_j}$ and let $n > 2 \equiv n_0$.
\begin{align*}
g(n) &= \sum_{i=0}^k c_i n^i \tag{definition} \\
 & \leq \sum_{i=0}^k \abs{c_i} n^i  \tag{$c_i \leq \abs{c_i}$}\\
 & \leq \sum_{i=0}^k \left(\sum_{j=0}^k \abs{c_j}\right) n^i \tag{$\abs{c_i} \leq \abs{c_0} + \ldots + \abs{c_i} +\ldots +\abs{c_k}$}\\
 & = \left(\frac{c}{2}\right) \sum_{i=0}^k n^i \tag{definition of $c$}\\
 &= \left(\frac{c}{2}\right)\left(n^k + \sum_{i=0}^{k-1}n^i\right) \tag{separate leading term}\\
 &= \left(\frac{c}{2}\right)\left(n^k + \frac{n^k-1}{n-1}\right) \tag{finitie geometric series}\\
 &< \left(\frac{c}{2}\right)\left(n^k + \frac{n^k}{1}\right) \tag{denominator > 1, numerator smaller}\\
 & = c\cdot n^k \tag{simplify} \\
 & \leq c \cdot n^\ell \tag{ $n > 1$}
\end{align*}
By definition, $g\in O(n^\ell)$.

\Textbf{3.11} Show that $\log n$ is $O(n^k)$ for any $k > 0$.
\Soln We prove the exercise using the \textit{natural} log.  Logarithms in other bases differ from the natural log by constant multiplicative factors which can be absorbed into the constant $c$, so the result will hold for logarithms in any base. Let $k > 0$ be given and define $\kappa = \ceil{\frac{1}{k}}$.  Note that $\kappa$ is an integer with value at least 1.  Define $c = (\kappa!)^{1/\kappa}$, $n_0 = 1$, and let $n > n_0$.  Now, consider the Taylor series expansion of $\displaystyle e^x=\sum_{i=0}^{\infty} \frac{x^i}{i!}$, which converges and is valid for all $x\in \mathbb{R}$.  In particular, this series is valid for $x = (\kappa! n)^{1/\kappa}$.

\begin{align*}
  e^{(\kappa! n) ^{1/\kappa}} &= \sum_{i=0}^{\infty}\frac{(\kappa! n)^{i/\kappa}}{i!} \tag{Taylor series} \\
  &> \frac{(\kappa!n)^{\kappa/\kappa}}{\kappa!} \tag{all terms positive, take only the $\kappa$ term} \\
  &= n \tag{simplify} \\
  \log n &< (\kappa!)^{1/\kappa}n^{1/\kappa} \tag*{$\Bigl(\Bigr. \begin{array}{r}\text{take logarithm, increasing functions preserve}\\ \text{inequalities, but we've switch sides}\end{array} \Bigl.\Bigr)$ }\\
    &= c n^{1/\kappa} \tag{definition of $c$} \\
    &< c n^k \tag{$n>1$, $\kappa = \ceil{1/k}\geq 1/k \implies k \geq 1/\kappa$  }
\end{align*}
So $\log \in O(n^k)$.

\Textbf{3.12} \textbf{(}$n^{\log n}$ \textbf{is super-polynomial)}  Show that $n^k$ is $O(n^{\log n})$ for any $k$, but that $n^{\log n}$ is never $O(n^k)$.
\Soln To avoid assumptions about behavior of constant exponents and asymptotic functions (as opposed to constant multiples), in this problem we allow the logarithm base, say $a$, be arbitrary, but we must assume $a > 1$.  For $a \leq 1$, the result is not true.  Let $k$ be given, define $c\equiv1$, $n_0\equiv \max(1,a^k)$, and let $n>n_0$.
\begin{align*}
  c\cdot n^{\log_a n} &= n^{\log_a n} \tag{definition of $c$} \\
  & \geq n^{\log_a a^k} \tag*{$\Bigl(\Bigr. \begin{array}{r}n>1, a>1 \Rightarrow n^t \text{ and } \log_a t \text{ increasing} \\ n>a^k \text{ by construction} \end{array} \Bigl.\Bigr)$ }\\
  &= n^k \tag{simplify}
\end{align*}
So $n^k\in O(n^{\log_a n})$.  To show that $n^{\log_a n} \not\in O(n_k)$, we again let $k$ be given, $c>0$ be any fixed constant, and let $n_0$ be a fixed positive integer.  We show there exists $n > n_0$ such that $n^{\log_a n} > c\cdot n^k$.  Consider, for example, $n=log_{n_0} a + k$ ... to be continued
%\begin{align*}We do so by arguing that $\displaystyle \lim_{n\rightarrow \infty} n^{log_a n} - c\cdot n^k = \infty$.  Note that $\log_n$ is a continuous increasing function such that $\displaystyle \lim_{m\rightarrow\infty}\log_n m = \infty$, so we may apply $\log_n$ to the expression inside the limit, that is, we may show $\displaystyle \lim_{n\rightarrow\infty} \log_n\left(n^{\log_a n}- c\cdot n^k\right) = \infty$.  

\Textbf{3.13}
\Textbf{3.14}

\Textbf{3.15} \textbf{(Lower bound for compare-and-swap based sorts)} Suppose an $n$ element list is sorted by applying some sequence of compare-and-swap operations to the list.  There are $n!$ possible initial orderings of the list.  Show that after $k$ of the compare-and-swap operations have been applied, at most $2^k$ of the possible initial orderings will have been sorted into the correct order.  Conclude that $\Omega(n\log n)$ compare-and-swap operations are required to sort all posible initial orderings into the correct order.
\Soln In each compare-and-swap operation there are two choices, either swap or do not swap according to the result of the comparison.  After each comparison-based decision, no matter how the next pair of entries to compare is decided, there are still only two options, swap or do not swap.  The resulting logical control flow can be modeled with a binary tree with depth at most $k$.  Each leaf of this binary tree corresponds to a sequence of compare-and-swap operations, where we've decided to swap after some comparisons and not after others.  Swaps apply transpositions to the original permuted list.  At a leaf, the original list will be sorted if and only if the product of transpositions applied is equal to the inverse of the permutation representing the original order of the list.  Importantly, this can only be true for a single ordering of the original list. So, each leaf corresponds to having sorted a single ordering of the list, so after $k$ compare-and-swap operations at most $2^k$ initial orderings can be sorted.

In order for a compare-and-swap based sorting algorithm to sort all possible orderings of an $n$-long list, we need there to be at least $n!$ leaves in the binary tree.  As there are at most $2^k$ leaves, we need $k$ to be such that $2^k \geq n!$ .  It can easily be show that $n! > \left(\frac{n}{2}\right)^\frac{n}{2}$, so we need $2^k \geq  \left(\frac{n}{2}\right)^\frac{n}{2}$.  Applying $\log_2$ to both sides yields $k \geq \left(\frac{n}{2}\right)(\log_2 n-1)$.  This is easily seen to imply that $k\in\Omega(n\log_2 n)$.



 
















